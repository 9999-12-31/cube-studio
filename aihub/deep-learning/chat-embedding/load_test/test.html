<!DOCTYPE html>
<!-- saved from url=(0040)https://www.zhihu.com/question/592844544 -->
<html lang="zh" data-hairline="true" class="itcauecng" data-theme="light" data-rh="data-theme"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="renderer" content="webkit"><meta name="force-rendering" content="webkit"><meta http-equiv="X-UA-Compatible" content="IE=10,chrome=1"><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"><meta data-rh="true" name="description" property="og:description" content="即使是按使用量付费的api也同样有这个限制具体到不同模型可能限制还不一样，少的只有几千"><meta data-rh="true" name="keywords" content="互联网,人工智能,AI技术,AIGC,ChatGPT"><link data-rh="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.81060cab.png"><link data-rh="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.81060cab.png" sizes="152x152"><link data-rh="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.d5793cac.png" sizes="120x120"><link data-rh="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7abf3393.png" sizes="76x76"><link data-rh="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.362a8eac.png" sizes="60x60"><link crossorigin="" rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/heifetz/favicon.ico"><link crossorigin="" rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/heifetz/search.xml" title="知乎"><link rel="dns-prefetch" href="https://static.zhimg.com/"><link rel="dns-prefetch" href="https://pica.zhimg.com/"><link rel="dns-prefetch" href="https://picx.zhimg.com/"><link rel="dns-prefetch" href="https://pic1.zhimg.com/"><link rel="dns-prefetch" href="https://pic2.zhimg.com/"><link rel="dns-prefetch" href="https://pic3.zhimg.com/"><link rel="dns-prefetch" href="https://pic4.zhimg.com/"><link rel="dns-prefetch" href="https://static.zhihu.com/"><script nonce="" data-web-reporter-config="{&quot;platform&quot;:&quot;web&quot;,&quot;project&quot;:&quot;heifetz&quot;}">!function(e,t){"object"==typeof exports&&"undefined"!=typeof module?t(exports):"function"==typeof define&&define.amd?define(["exports"],t):t((e=e||self).webReporter={})}(this,function(e){"use strict";var t={},n=!1,o=function(){var e,o,r,a,i;return n||(e=document.querySelector("script[data-web-reporter-config]"),o=e&&e.dataset.webReporterConfig||"{}",r=JSON.parse(o),a=r.platform,i=r.project,t={platform:a,project:i},n=!0),t};function r(e){return a(function(){return localStorage.getItem(e)})()}function a(e){return function(){try{return e.apply(void 0,arguments)}catch(e){}}}var i=a(function(e,t){var n={platform:"web",project:o().project,clientTimestamp:+new Date};!function(e,t,n){"1"===r("weber:logenabled")&&console.log("[web-reporter]%o",{type:e,base:t,data:n})}(e,n,t),function(e,t){var n=btoa(JSON.stringify(t));if("undefined"!=typeof Blob&&window.navigator&&window.navigator.sendBeacon){var o=new Blob([n],{type:"text/plain"});navigator.sendBeacon(e,o)}else{var r=new XMLHttpRequest;r.open("POST",e),r.withCredentials=!1,r.setRequestHeader("Content-Type","text/plain;charset=UTF-8"),r.send(n)}}(r("weber:api")||"https://apm.zhihu.com/collector/web_json",{type:e,base:n,data:t})});e.report=i,Object.defineProperty(e,"__esModule",{value:!0})});
</script><link href="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/6116.216a26f4.7e059bd26c25b9e701c1.css" crossorigin="" rel="stylesheet"><link href="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/main.216a26f4.a33d15462b94f00e8222.css" crossorigin="" rel="stylesheet"><link href="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/7359.216a26f4.0a883c98b859b51420cd.css" crossorigin="" rel="stylesheet"><link href="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/4621.216a26f4.07d0cc6f5ff1c2e1a3e1.css" crossorigin="" rel="stylesheet"><link href="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/1250.216a26f4.6c9e8f052424c017b76d.css" crossorigin="" rel="stylesheet"><link href="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/main-question-routes.216a26f4.20c56022d638215f09c2.css" crossorigin="" rel="stylesheet"><link rel="stylesheet" type="text/css" href="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/ECommerceAd.216a26f4.274a8f67b7e51e30037f.css" crossorigin="anonymous"><link rel="stylesheet" type="text/css" href="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/2540.216a26f4.8e45a92938411bd5cdb6.css" crossorigin="anonymous"><link rel="stylesheet" type="text/css" href="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/5930.216a26f4.8c3c95c8cdb970b064a3.css" crossorigin="anonymous"><link rel="stylesheet" type="text/css" href="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/richinputV2.216a26f4.5623ffb4cccac1e9b92a.css" crossorigin="anonymous"><link rel="stylesheet" type="text/css" href="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/navbar-notifications.216a26f4.aca8c4d05000000cb86d.css" crossorigin="anonymous"><link rel="stylesheet" type="text/css" href="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/9744.216a26f4.480978294e267b62de9a.css" crossorigin="anonymous"><link rel="stylesheet" type="text/css" href="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/navbar-messages.216a26f4.eedcb908d97722490483.css" crossorigin="anonymous"><link rel="stylesheet" type="text/css" href="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/Labels.216a26f4.81c9ce8725560c5bcc6a.css" crossorigin="anonymous"><link rel="stylesheet" type="text/css" href="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/user-hover-card.216a26f4.906410e32db3bcbf1a3a.css" crossorigin="anonymous"><link rel="stylesheet" type="text/css" href="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/GoodsRecommendGoodsCardList.216a26f4.d95ce79191cdf8d7ac28.css" crossorigin="anonymous"><link rel="stylesheet" type="text/css" href="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/5622.216a26f4.dc923f5795e5fd88dda4.css" crossorigin="anonymous"><link rel="stylesheet" type="text/css" href="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/3280.216a26f4.8bfc371d6d7cfdc6aeec.css" crossorigin="anonymous"><script nonce="">!function(){"use strict";!function(e,n){var r=[];function t(e){return function(){r.push([e,arguments])}}n.Raven={captureException:t("captureException"),captureMessage:t("captureMessage"),captureBreadcrumb:t("captureBreadcrumb")};var a,o,c,i,s,u="undefined"!=typeof DOMError;function d(e){var n=e instanceof Error||e instanceof ErrorEvent||u&&e instanceof DOMError||e instanceof DOMException;Raven.captureException(n?e:new Error(e.message||e.reason))}n.addEventListener("unhandledrejection",d),n.addEventListener("error",d,!0),a=e.src,o=e,c=function(){r.forEach(function(e){var n;(n=Raven)[e[0]].apply(n,e[1])}),n.removeEventListener("unhandledrejection",d),n.removeEventListener("error",d,!0)},i=document.head||document.getElementsByTagName("head")[0],(s=document.createElement("script")).crossOrigin=o.crossOrigin,s.dataset.sentryConfig=o["data-sentry-config"],s.onload=c,s.src=a,i.appendChild(s)}({"defer":true,"crossOrigin":"anonymous","src":"https://unpkg.zhimg.com/@cfe/sentry-script@1.3.1/dist/init.js","data-sentry-config":"{\"dsn\":\"https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224\",\"sampleRate\":0.1,\"release\":\"840-a63ba4db\",\"ignoreErrorNames\":[\"NetworkError\",\"SecurityError\"],\"ignoreErrorsPreset\":\"ReactApp\",\"tags\":{\"app_name\":\"heifetz\"}}"},window)}();
</script><script crossorigin="anonymous" data-sentry-config="{&quot;dsn&quot;:&quot;https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;840-a63ba4db&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrorsPreset&quot;:&quot;ReactApp&quot;,&quot;tags&quot;:{&quot;app_name&quot;:&quot;heifetz&quot;}}" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/init.js"></script><style data-emotion-css="uzm3ri">.css-uzm3ri{position:fixed;top:0;right:0;left:0;z-index:101;display:none;height:2px;pointer-events:none;background:#056DE8;-webkit-transform:translateX(-100%);-ms-transform:translateX(-100%);transform:translateX(-100%);}</style><style data-emotion-css="1x8hcdw">.css-1x8hcdw{background-color:#FFFFFF;-webkit-transition-property:background-color,box-shadow;transition-property:background-color,box-shadow;-webkit-transition-duration:0.25s;transition-duration:0.25s;-webkit-transition-timing-function:ease-in;transition-timing-function:ease-in;}</style><style data-emotion-css="l2ygoj">.css-l2ygoj{width:auto;max-width:1156px;min-width:1000px;padding-left:16px;padding-right:55px;}.css-l2ygoj .AppHeader-userInfo{margin-left:30px;width:auto;}.css-l2ygoj .AppHeader-TabsLink.is-active,.css-l2ygoj .AppHeader-TabsLink:hover{color:#121212;}</style><style data-emotion-css="1hlrcxk">.css-1hlrcxk{-webkit-transition-property:fill;transition-property:fill;-webkit-transition-duration:0.25s;transition-duration:0.25s;-webkit-transition-timing-function:ease-in;transition-timing-function:ease-in;}</style><style data-emotion-css="g0ay3v">.css-g0ay3v{margin-left:25px;margin-right:15px;}.css-g0ay3v .AppHeader-Tab{padding-left:15px;padding-right:15px;}.css-g0ay3v .Tabs-link.is-active::after{height:4px;}</style><style data-emotion-css="12n8klz">.css-12n8klz{color:undefined !important;-webkit-transition-property:color;transition-property:color;-webkit-transition-duration:0.25s;transition-duration:0.25s;-webkit-transition-timing-function:ease-in;transition-timing-function:ease-in;}.css-12n8klz.is-active,.css-12n8klz:hover{opacity:1;}.css-12n8klz.is-active::after{background-color:#056DE8 !important;}</style><style data-emotion-css="1acwmmj">.css-1acwmmj{box-sizing:border-box;margin:0;min-width:0;-webkit-flex:1;-ms-flex:1;flex:1;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}</style><style data-emotion-css="10fy1q8">.css-10fy1q8{max-width:482px;}.css-10fy1q8 .SearchBar-input{border-radius:999px;padding-left:16px;}.css-10fy1q8 .SearchBar-askButton{border-radius:999px;width:70px;margin-left:12px;}.css-10fy1q8 .SearchBar-searchButton{border-bottom-right-radius:999px;border-top-right-radius:999px;}</style><style data-emotion-css="11bw1mm">.css-11bw1mm{border-color:undefined !important;-webkit-transition-property:background-color,border,color;transition-property:background-color,border,color;-webkit-transition-duration:0.25s;transition-duration:0.25s;-webkit-transition-timing-function:ease-in;transition-timing-function:ease-in;}.css-11bw1mm{border-color:undefined !important;-webkit-transition-property:background-color,border,color;transition-property:background-color,border,color;-webkit-transition-duration:0.25s;transition-duration:0.25s;-webkit-transition-timing-function:ease-in;transition-timing-function:ease-in;}.css-11bw1mm{border-color:undefined !important;-webkit-transition-property:background-color,border,color;transition-property:background-color,border,color;-webkit-transition-duration:0.25s;transition-duration:0.25s;-webkit-transition-timing-function:ease-in;transition-timing-function:ease-in;}.css-11bw1mm{border-color:undefined !important;-webkit-transition-property:background-color,border,color;transition-property:background-color,border,color;-webkit-transition-duration:0.25s;transition-duration:0.25s;-webkit-transition-timing-function:ease-in;transition-timing-function:ease-in;}</style><style data-emotion-css="1dlt5yv">.css-1dlt5yv{-webkit-transition-property:color;transition-property:color;-webkit-transition-duration:0.25s;transition-duration:0.25s;-webkit-transition-timing-function:ease-in;transition-timing-function:ease-in;}.css-1dlt5yv:hover{opacity:1;}</style><style data-emotion-css="3q84jd">.css-3q84jd{background-color:undefined !important;color:undefined !important;border:undefined !important;-webkit-transition-property:background-color,color;transition-property:background-color,color;-webkit-transition-duration:0.25s;transition-duration:0.25s;-webkit-transition-timing-function:ease-in;transition-timing-function:ease-in;}.css-3q84jd:hover{opacity:1;}</style><style data-emotion-css="79elbk">.css-79elbk{position:relative;}</style><style data-emotion-css="7dgah8">.css-7dgah8{-webkit-transition-property:color,opacity;transition-property:color,opacity;-webkit-transition-duration:0.25s;transition-duration:0.25s;-webkit-transition-timing-function:ease-in;transition-timing-function:ease-in;}.css-7dgah8:hover{opacity:1;}</style><style data-emotion-css="1m60na">.css-1m60na{box-sizing:border-box;margin:0;min-width:0;font-size:12px;line-height:14px;color:undefined !important;-webkit-transition-property:color;transition-property:color;-webkit-transition-duration:0.25s;transition-duration:0.25s;-webkit-transition-timing-function:ease-in;transition-timing-function:ease-in;}.css-1m60na.is-active,.css-1m60na:hover{opacity:1;}.css-1m60na.is-active::after{background-color:#056DE8 !important;}</style><style data-emotion-css="icip60">.css-icip60{border-radius:2px;}</style><style data-emotion-css="1tzrnvf">.css-1tzrnvf{box-sizing:border-box;margin:0;min-width:0;max-width:100%;height:auto;background-color:#FFFFFF;width:30px;height:30px;border-radius:2px;}</style><style data-emotion-css="1s3a4zw">.css-1s3a4zw{position:relative;display:inline-block;height:30px;padding:0 12px;font-size:14px;line-height:30px;color:#056DE8;vertical-align:top;border-radius:100px;background:rgba(5,109,232,0.1);}.css-1s3a4zw:hover{background-color:rgba(5,109,232,0.15);}</style><style data-emotion-css="1xlfegr">.css-1xlfegr{background:transparent;box-shadow:none;}</style><style data-emotion-css="1gomreu">.css-1gomreu{position:relative;display:inline-block;}</style><style data-emotion-css="4cffwv">.css-4cffwv{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}</style><style data-emotion-css="eew49z">.css-eew49z{min-height:10px;}</style><style data-emotion="css-global"></style><style data-emotion-css="9hu0cu">.css-9hu0cu{width:1000px;}</style><style data-emotion-css="124jo1g">.css-124jo1g{box-sizing:border-box;margin:0;min-width:0;background-color:#F6F6F6;margin-top:10px;margin-left:auto;margin-right:auto;width:1000px;}</style><style data-emotion-css="yvuz2o">.css-yvuz2o .Select-arrow{width:1.2em;height:1.2em;}</style><style data-emotion-css="1oqflzh">.css-1oqflzh{box-sizing:border-box;margin:0;min-width:0;max-width:100%;height:auto;background-color:#FFFFFF;width:38px;height:38px;border-radius:2px;}</style><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style><style data-emotion-css="2dtzk2">.css-2dtzk2{cursor:pointer;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin-left:.3em;}</style><style data-emotion-css="wu55un">.css-wu55un{height:1em;width:1em;}</style><style data-emotion-css="14ur8a8">.css-14ur8a8.AuthorInfo-badgeText{display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;width:490px;white-space:unset;}</style><style data-emotion-css="124ezq8">.css-124ezq8{position:absolute;bottom:0;right:0;width:1px;height:1px;}</style><style data-emotion-css="1k5dpte">.css-1k5dpte{box-sizing:border-box;margin:0;min-width:0;color:#8590A6;font-size:14px;margin-top:16px;}</style><style data-emotion-css="376mun">.css-376mun{position:relative;display:inline;}</style><style data-emotion-css="1hhle02">.css-1hhle02 .FileLinkCard{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:rgba(246,246,246,0.88);border-radius:12px;box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;margin:1em auto;max-width:100%;overflow:hidden;padding:12px;position:relative;width:390px;}.css-1hhle02 .FileLinkCard-icon{-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;height:30px;width:30px;}.css-1hhle02 .FileLinkCard-info{margin-left:12px;}.css-1hhle02 .FileLinkCard-name{color:#121212;font-size:15px;font-weight:500;line-height:21px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-1hhle02 .FileLinkCard-meta{color:#999999;font-size:12px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;line-height:14px;margin-top:5px;}.css-1hhle02 .FileLinkCard-source{white-space:pre;}.css-1hhle02 img[data-uncomfortable]{content:url(data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20viewBox%3D%220%200%20344.88888888888886%20194%22%3E%3CforeignObject%20width%3D%22344.88888888888886%22%20height%3D%22194%22%3E%0A%20%20%20%20%20%20%3Cdiv%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxhtml%22%20style%3D%22font-size%3A%2013px%3B%20font-family%3A%20-apple-system%2C%20BlinkMacSystemFont%2C%20Microsoft%20YaHei%2C%20sans-serif%3B%20color%3A%20%23fff%3B%20width%3A100%25%3B%20height%3A194px%3B%22%3E%0A%20%20%20%20%20%20%20%20%3Cdiv%20style%3D%22display%3A%20flex%3B%20flex-direction%3A%20column%3B%20align-items%3A%20center%3B%20justify-content%3A%20center%3B%20height%3A%20100%25%3B%22%3E%0A%20%20%20%20%20%20%20%20%20%20%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20width%3D%2218%22%20height%3D%2218%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22currentColor%22%3E%3Cpath%20d%3D%22M8%203.65a7%207%200%2000-1.353.128.65.65%200%2011-.25-1.275A8.3%208.3%200%20018%202.35c2.387%200%204.172.954%205.357%202.125C14.511%205.615%2015.15%207.022%2015.15%208c0%20.621-.257%201.391-.699%202.134a7.076%207.076%200%2001-1.403%201.68l.495.46a.65.65%200%2011-.886.951l-.998-.929a.645.645%200%2001-.104-.097L9.73%2010.501a.647.647%200%2001-.29.301%203.15%203.15%200%2001-4.313-4.094.647.647%200%2001.234-.275L3.908%205.08a5.774%205.774%200%2000-1.283%201.522C2.282%207.198%202.15%207.707%202.15%208c0%20.522.41%201.616%201.407%202.6.965.954%202.43%201.75%204.443%201.75.468%200%20.905-.043%201.311-.12a.65.65%200%2001.243%201.277A8.322%208.322%200%20018%2013.65c-2.387%200-4.172-.954-5.357-2.125C1.49%2010.385.85%208.978.85%208c0-.598.238-1.333.648-2.046A7.054%207.054%200%20012.95%204.188l-.547-.509a.65.65%200%2011.886-.951l8.8%208.194a5.793%205.793%200%20001.244-1.453c.372-.624.516-1.163.516-1.469%200-.522-.41-1.616-1.407-2.6-.965-.954-2.43-1.75-4.443-1.75zM6.29%207.296a1.85%201.85%200%20002.534%202.36l-2.535-2.36zM8%204.85a.65.65%200%20100%201.3%201.85%201.85%200%20011.843%201.694.65.65%200%20101.296-.11A3.15%203.15%200%20008%204.85z%22%20fill-rule%3D%22evenodd%22%20clip-rule%3D%22evenodd%22%3E%3C%2Fpath%3E%3C%2Fsvg%3E%0A%20%20%20%20%20%20%20%20%20%20%3Cdiv%20style%3D%22margin%3A%20.6em%200%201.2em%22%3E%E8%AF%A5%E5%9B%BE%E7%89%87%E6%9C%89%E5%8F%AF%E8%83%BD%E4%BC%9A%E5%BC%95%E8%B5%B7%E4%B8%8D%E9%80%82%3C%2Fdiv%3E%0A%20%20%20%20%20%20%20%20%20%20%3Cbutton%20style%3D%22padding%3A%204px%201em%3B%20font-size%3A%201.1em%3B%20color%3A%20inherit%3B%20background%3A%20none%3B%20border%3A%201px%20solid%20rgba%28255%2C255%2C255%2C.5%29%3B%20border-radius%3A%209999px%3B%22%3E%E7%BB%A7%E7%BB%AD%E6%9F%A5%E7%9C%8B%3C%2Fbutton%3E%0A%20%20%20%20%20%20%20%20%3C%2Fdiv%3E%0A%20%20%20%20%20%20%3C%2Fdiv%3E%0A%20%20%20%20%3C%2FforeignObject%3E%3C%2Fsvg%3E);width:100%;height:194px;background:url(https://pic1.zhimg.com/v2-cf70d0759d787c70091857151c1cad4a.jpeg) no-repeat rgba(191,191,191,0.7);background-size:cover;cursor:pointer!important;}</style><style data-emotion-css="1wr1m8">.css-1wr1m8 .LinkCard.new{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;box-sizing:border-box;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:390px;min-height:84px;border-radius:8px;max-width:100%;overflow:hidden;margin:16px auto;padding:12px 12px 9px 12px;background-color:#F6F6F6;}.css-1wr1m8 .LinkCard.new,.css-1wr1m8 .LinkCard.new:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-1wr1m8 .LinkCard.new .LinkCard-contents{display:block;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;position:relative;}.css-1wr1m8 .LinkCard.new .LinkCard-contents .loading{height:14px;background:#EBEBEB;border-radius:7px;}.css-1wr1m8 .LinkCard.new .LinkCard-contents.withTitle{margin-bottom:3px;}.css-1wr1m8 .LinkCard.new .LinkCard-title{display:-webkit-box;font-size:15px;font-weight:500;line-height:1.4;margin-bottom:2px;color:#121212;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1wr1m8 .LinkCard.new .LinkCard-title.two-line{line-height:20px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-1wr1m8 .LinkCard.new .LinkCard-title.loading{margin-bottom:8px;width:80%;}.css-1wr1m8 .LinkCard.new .LinkCard-title.loading.withTitle{margin-bottom:6px;}.css-1wr1m8 .LinkCard.new .LinkCard-title.loadingTitle{margin-bottom:5px;}.css-1wr1m8 .LinkCard.new .LinkCard-excerpt{display:-webkit-box;text-overflow:ellipsis;font-size:13px;line-height:18px;color:#999999;margin-bottom:4px;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1wr1m8 .LinkCard.new .LinkCard-excerpt .LinkCard-author{color:#444444;}.css-1wr1m8 .LinkCard.new .LinkCard-desc{display:-webkit-box;font-size:13px;height:18px;line-height:18px;color:#999999;word-break:break-all;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1wr1m8 .LinkCard.new .LinkCard-desc .LinkCard-tag,.css-1wr1m8 .LinkCard.new .LinkCard-desc .tag{display:inline-block;font-size:11px;margin-left:8px;padding:0 4px;border-radius:3px;background:rgba(211,211,211,0.3);}.css-1wr1m8 .LinkCard.new .LinkCard-desc.loading{width:40%;}.css-1wr1m8 .LinkCard.new .LinkCard-desc svg{margin-right:2px;}.css-1wr1m8 .LinkCard.new .LinkCard-image{-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;background-color:#EBEBEB;background-size:cover;background-position:center;position:relative;display:block;width:60px;height:60px;margin-left:20px;object-fit:cover;border-radius:inherit;overflow:hidden;}.css-1wr1m8 .LinkCard.new .LinkCard-image.LinkCard-image--default{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;background-color:#EBEBEB;color:#D3D3D3;}.css-1wr1m8 .LinkCard.new .LinkCard-image.LinkCard-image--default svg{color:#999999;}.css-1wr1m8 .LinkCard.new .LinkCard-image img{width:100%;height:100%;object-fit:cover;}.css-1wr1m8 .LinkCard.new .LinkCard-image .LinkCard-image--video{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;position:absolute;top:50%;left:50%;-webkit-transform:translateX(-50%) translateY(-50%);-ms-transform:translateX(-50%) translateY(-50%);transform:translateX(-50%) translateY(-50%);width:24px;height:24px;border-radius:12px;background:rgba(255,255,255,0.9);pointer-events:none;}.css-1wr1m8 .LinkCard.new .LinkCard-image .LinkCard-image--video svg{color:#444444;}.css-1wr1m8 .LinkCard.new .LinkCard-richText .text{color:#444444;}.css-1wr1m8 .LinkCard.new .LinkCard-richText .bold{font-weight:600;}.css-1wr1m8 .LinkCard.new .LinkCard-richText .tag{margin-left:4px;}.css-1wr1m8 .LinkCard.old{position:relative;display:block;margin:1em auto;width:390px;box-sizing:border-box;border-radius:12px;max-width:100%;overflow:hidden;}.css-1wr1m8 .LinkCard.old,.css-1wr1m8 .LinkCard.old:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-1wr1m8 .LinkCard-ecommerceLoadingCard{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;padding:12px;border-radius:inherit;height:80px;box-sizing:border-box;background:rgba(246,246,246,0.88);color:#D3D3D3;}.css-1wr1m8 .LinkCard-ecommerceLoadingCardAvatarWrapper{width:60px;height:60px;background:#EBEBEB;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;border-radius:6px;margin-right:10px;}.css-1wr1m8 .LinkCard-ecommerceLoadingCardNetwork{width:20px;height:20px;}.css-1wr1m8 .LinkCard-ecommerceLoadingCardLoadingbar{height:60px;-webkit-flex:1;-ms-flex:1;flex:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.css-1wr1m8 .LinkCard-ecommerceLoadingCardLoadingbar span{height:16px;display:inline-block;background:#EBEBEB;}.css-1wr1m8 .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(1){width:60px;margin-bottom:4px;}.css-1wr1m8 .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(2){width:127px;}</style><style data-emotion-css="1dnyyvy">.css-1dnyyvy .LinkCard.old{position:relative;display:block;margin:1em auto;width:390px;box-sizing:border-box;border-radius:12px;max-width:100%;overflow:hidden;}.css-1dnyyvy .LinkCard.old,.css-1dnyyvy .LinkCard.old:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-1dnyyvy .LinkCard-ecommerceLoadingCard{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;padding:12px;border-radius:inherit;height:80px;box-sizing:border-box;background:rgba(246,246,246,0.88);color:#D3D3D3;}.css-1dnyyvy .LinkCard-ecommerceLoadingCardAvatarWrapper{width:60px;height:60px;background:#EBEBEB;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;border-radius:6px;margin-right:10px;}.css-1dnyyvy .LinkCard-ecommerceLoadingCardNetwork{width:20px;height:20px;}.css-1dnyyvy .LinkCard-ecommerceLoadingCardLoadingbar{height:60px;-webkit-flex:1;-ms-flex:1;flex:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.css-1dnyyvy .LinkCard-ecommerceLoadingCardLoadingbar span{height:16px;display:inline-block;background:#EBEBEB;}.css-1dnyyvy .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(1){width:60px;margin-bottom:4px;}.css-1dnyyvy .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(2){width:127px;}.css-1dnyyvy .LinkCard.new{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;box-sizing:border-box;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:390px;min-height:84px;border-radius:8px;max-width:100%;overflow:hidden;margin:16px auto;padding:12px 12px 9px 12px;background-color:#F6F6F6;}.css-1dnyyvy .LinkCard.new,.css-1dnyyvy .LinkCard.new:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-1dnyyvy .LinkCard.new .LinkCard-contents{display:block;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;position:relative;}.css-1dnyyvy .LinkCard.new .LinkCard-contents .loading{height:14px;background:#EBEBEB;border-radius:7px;}.css-1dnyyvy .LinkCard.new .LinkCard-contents.withTitle{margin-bottom:3px;}.css-1dnyyvy .LinkCard.new .LinkCard-title{display:-webkit-box;font-size:15px;font-weight:500;line-height:1.4;margin-bottom:2px;color:#121212;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1dnyyvy .LinkCard.new .LinkCard-title.two-line{line-height:20px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-1dnyyvy .LinkCard.new .LinkCard-title.loading{margin-bottom:8px;width:80%;}.css-1dnyyvy .LinkCard.new .LinkCard-title.loading.withTitle{margin-bottom:6px;}.css-1dnyyvy .LinkCard.new .LinkCard-title.loadingTitle{margin-bottom:5px;}.css-1dnyyvy .LinkCard.new .LinkCard-excerpt{display:-webkit-box;text-overflow:ellipsis;font-size:13px;line-height:18px;color:#999999;margin-bottom:4px;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1dnyyvy .LinkCard.new .LinkCard-excerpt .LinkCard-author{color:#444444;}.css-1dnyyvy .LinkCard.new .LinkCard-desc{display:-webkit-box;font-size:13px;height:18px;line-height:18px;color:#999999;word-break:break-all;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1dnyyvy .LinkCard.new .LinkCard-desc .LinkCard-tag,.css-1dnyyvy .LinkCard.new .LinkCard-desc .tag{display:inline-block;font-size:11px;margin-left:8px;padding:0 4px;border-radius:3px;background:rgba(211,211,211,0.3);}.css-1dnyyvy .LinkCard.new .LinkCard-desc.loading{width:40%;}.css-1dnyyvy .LinkCard.new .LinkCard-desc svg{margin-right:2px;}.css-1dnyyvy .LinkCard.new .LinkCard-image{-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;background-color:#EBEBEB;background-size:cover;background-position:center;position:relative;display:block;width:60px;height:60px;margin-left:20px;object-fit:cover;border-radius:inherit;overflow:hidden;}.css-1dnyyvy .LinkCard.new .LinkCard-image.LinkCard-image--default{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;background-color:#EBEBEB;color:#D3D3D3;}.css-1dnyyvy .LinkCard.new .LinkCard-image.LinkCard-image--default svg{color:#999999;}.css-1dnyyvy .LinkCard.new .LinkCard-image img{width:100%;height:100%;object-fit:cover;}.css-1dnyyvy .LinkCard.new .LinkCard-image .LinkCard-image--video{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;position:absolute;top:50%;left:50%;-webkit-transform:translateX(-50%) translateY(-50%);-ms-transform:translateX(-50%) translateY(-50%);transform:translateX(-50%) translateY(-50%);width:24px;height:24px;border-radius:12px;background:rgba(255,255,255,0.9);pointer-events:none;}.css-1dnyyvy .LinkCard.new .LinkCard-image .LinkCard-image--video svg{color:#444444;}.css-1dnyyvy .LinkCard.new .LinkCard-richText .text{color:#444444;}.css-1dnyyvy .LinkCard.new .LinkCard-richText .bold{font-weight:600;}.css-1dnyyvy .LinkCard.new .LinkCard-richText .tag{margin-left:4px;}.css-1dnyyvy .FileLinkCard{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:rgba(246,246,246,0.88);border-radius:12px;box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;margin:1em auto;max-width:100%;overflow:hidden;padding:12px;position:relative;width:390px;}.css-1dnyyvy .FileLinkCard-icon{-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;height:30px;width:30px;}.css-1dnyyvy .FileLinkCard-info{margin-left:12px;}.css-1dnyyvy .FileLinkCard-name{color:#121212;font-size:15px;font-weight:500;line-height:21px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-1dnyyvy .FileLinkCard-meta{color:#999999;font-size:12px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;line-height:14px;margin-top:5px;}.css-1dnyyvy .FileLinkCard-source{white-space:pre;}.css-1dnyyvy img[data-uncomfortable]{content:url(data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20viewBox%3D%220%200%20344.88888888888886%20194%22%3E%3CforeignObject%20width%3D%22344.88888888888886%22%20height%3D%22194%22%3E%0A%20%20%20%20%20%20%3Cdiv%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxhtml%22%20style%3D%22font-size%3A%2013px%3B%20font-family%3A%20-apple-system%2C%20BlinkMacSystemFont%2C%20Microsoft%20YaHei%2C%20sans-serif%3B%20color%3A%20%23fff%3B%20width%3A100%25%3B%20height%3A194px%3B%22%3E%0A%20%20%20%20%20%20%20%20%3Cdiv%20style%3D%22display%3A%20flex%3B%20flex-direction%3A%20column%3B%20align-items%3A%20center%3B%20justify-content%3A%20center%3B%20height%3A%20100%25%3B%22%3E%0A%20%20%20%20%20%20%20%20%20%20%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20width%3D%2218%22%20height%3D%2218%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22currentColor%22%3E%3Cpath%20d%3D%22M8%203.65a7%207%200%2000-1.353.128.65.65%200%2011-.25-1.275A8.3%208.3%200%20018%202.35c2.387%200%204.172.954%205.357%202.125C14.511%205.615%2015.15%207.022%2015.15%208c0%20.621-.257%201.391-.699%202.134a7.076%207.076%200%2001-1.403%201.68l.495.46a.65.65%200%2011-.886.951l-.998-.929a.645.645%200%2001-.104-.097L9.73%2010.501a.647.647%200%2001-.29.301%203.15%203.15%200%2001-4.313-4.094.647.647%200%2001.234-.275L3.908%205.08a5.774%205.774%200%2000-1.283%201.522C2.282%207.198%202.15%207.707%202.15%208c0%20.522.41%201.616%201.407%202.6.965.954%202.43%201.75%204.443%201.75.468%200%20.905-.043%201.311-.12a.65.65%200%2001.243%201.277A8.322%208.322%200%20018%2013.65c-2.387%200-4.172-.954-5.357-2.125C1.49%2010.385.85%208.978.85%208c0-.598.238-1.333.648-2.046A7.054%207.054%200%20012.95%204.188l-.547-.509a.65.65%200%2011.886-.951l8.8%208.194a5.793%205.793%200%20001.244-1.453c.372-.624.516-1.163.516-1.469%200-.522-.41-1.616-1.407-2.6-.965-.954-2.43-1.75-4.443-1.75zM6.29%207.296a1.85%201.85%200%20002.534%202.36l-2.535-2.36zM8%204.85a.65.65%200%20100%201.3%201.85%201.85%200%20011.843%201.694.65.65%200%20101.296-.11A3.15%203.15%200%20008%204.85z%22%20fill-rule%3D%22evenodd%22%20clip-rule%3D%22evenodd%22%3E%3C%2Fpath%3E%3C%2Fsvg%3E%0A%20%20%20%20%20%20%20%20%20%20%3Cdiv%20style%3D%22margin%3A%20.6em%200%201.2em%22%3E%E8%AF%A5%E5%9B%BE%E7%89%87%E6%9C%89%E5%8F%AF%E8%83%BD%E4%BC%9A%E5%BC%95%E8%B5%B7%E4%B8%8D%E9%80%82%3C%2Fdiv%3E%0A%20%20%20%20%20%20%20%20%20%20%3Cbutton%20style%3D%22padding%3A%204px%201em%3B%20font-size%3A%201.1em%3B%20color%3A%20inherit%3B%20background%3A%20none%3B%20border%3A%201px%20solid%20rgba%28255%2C255%2C255%2C.5%29%3B%20border-radius%3A%209999px%3B%22%3E%E7%BB%A7%E7%BB%AD%E6%9F%A5%E7%9C%8B%3C%2Fbutton%3E%0A%20%20%20%20%20%20%20%20%3C%2Fdiv%3E%0A%20%20%20%20%20%20%3C%2Fdiv%3E%0A%20%20%20%20%3C%2FforeignObject%3E%3C%2Fsvg%3E);width:100%;height:194px;background:url(https://pic1.zhimg.com/v2-cf70d0759d787c70091857151c1cad4a.jpeg) no-repeat rgba(191,191,191,0.7);background-size:cover;cursor:pointer!important;}</style><style data-emotion-css="1g0fqss animation-1yvu044">.css-1g0fqss{word-break:break-word;line-height:1.6;}.css-1g0fqss > [data-first-child]{margin-top:0;}.css-1g0fqss > :last-child{margin-bottom:0;}.css-1g0fqss h1,.css-1g0fqss h2{clear:left;margin-top:calc((1.4em * 2) / 1.2);margin-bottom:calc(1.4em / 1.2);font-size:1.2em;line-height:1.5;font-weight:600;}.css-1g0fqss h3,.css-1g0fqss h4,.css-1g0fqss h5,.css-1g0fqss h6{clear:left;margin-top:calc((1.4em * 1.5) / 1.1);margin-bottom:calc(1.4em / 1.1);font-size:1.1em;line-height:1.5;font-weight:600;}.css-1g0fqss u{-webkit-text-decoration:none;text-decoration:none;border-bottom:1px solid #444444;}.css-1g0fqss b{font-weight:600;}.css-1g0fqss sup{font-size:0.8em;}.css-1g0fqss sup[data-draft-type='reference']{color:#175199;}.css-1g0fqss a:focus{outline:none;-webkit-transition:box-shadow 0.3s;transition:box-shadow 0.3s;}html[data-focus-visible] .css-1g0fqss a:focus{box-shadow:0 0 0 2px #FFFFFF,0 0 0 4px rgba(5,109,232,0.3);}.css-1g0fqss a.ztext-link,.css-1g0fqss a.internal,.css-1g0fqss a.external{-webkit-text-decoration:none;text-decoration:none;cursor:pointer;border-bottom:1px solid #808080;}.css-1g0fqss a.ztext-link:hover,.css-1g0fqss a.internal:hover,.css-1g0fqss a.external:hover{color:#175199;border-bottom:1px solid #175199;}.css-1g0fqss a.ztext-link > .ellipsis::after,.css-1g0fqss a.internal > .ellipsis::after,.css-1g0fqss a.external > .ellipsis::after{content:'...';}.css-1g0fqss a.ztext-link > .invisible,.css-1g0fqss a.internal > .invisible,.css-1g0fqss a.external > .invisible{font:0/0 a;color:transparent;text-shadow:none;background-color:transparent;}.css-1g0fqss a.ztext-link u,.css-1g0fqss a.internal u,.css-1g0fqss a.external u{border:none;}.css-1g0fqss a.member_mention{color:#175199;}.css-1g0fqss a.member_mention:hover{border-bottom:1px solid #175199;}.css-1g0fqss a.UserLink-link{color:#175199;}.css-1g0fqss a.UserLink-link:hover{border-bottom:1px solid #175199;}.css-1g0fqss p{margin:1.4em 0;}.css-1g0fqss p.ztext-empty-paragraph{margin:calc((2.8em- (1.4em * 2 + 1.6em)) / 2) 0;}.css-1g0fqss p.ztext-empty-paragraph + .ztext-empty-paragraph{margin:1.4em 0;}.css-1g0fqss hr{margin:4em auto;width:240px;max-width:100%;border:none;border-top:1px solid #D3D3D3;}.css-1g0fqss img[eeimg]{max-width:100%;vertical-align:middle;}.css-1g0fqss img[eeimg="1"]{margin:0 3px;max-width:calc(100% - 6px);display:inline-block;}.css-1g0fqss img[eeimg="2"]{margin:1.4em auto;display:block;}.css-1g0fqss blockquote{margin:1.4em 0;padding-left:1em;color:#646464;border-left:3px solid #D3D3D3;}.css-1g0fqss ol,.css-1g0fqss ul{margin:1.4em 0;padding:0;width:100%;}.css-1g0fqss ol ol,.css-1g0fqss ul ol,.css-1g0fqss ol ul,.css-1g0fqss ul ul{margin:0;}.css-1g0fqss ol li::before,.css-1g0fqss ul li::before{width:1em;}.css-1g0fqss ol > ol,.css-1g0fqss ul > ol,.css-1g0fqss ol > ul,.css-1g0fqss ul > ul{display:table-row;}.css-1g0fqss ol > ol::before,.css-1g0fqss ul > ol::before,.css-1g0fqss ol > ul::before,.css-1g0fqss ul > ul::before{display:table-cell;content:'';}.css-1g0fqss ul{display:table;}.css-1g0fqss ul>li{display:table-row;list-style:none;}.css-1g0fqss ul>li::before{display:table-cell;content:'•  ';white-space:pre;}.css-1g0fqss ol{display:table;counter-reset:ol;}.css-1g0fqss ol > li{display:table-row;list-style:none;}.css-1g0fqss ol > li::before{display:table-cell;text-align:right;counter-increment:ol;content:counter(ol) '. ';white-space:pre;}.css-1g0fqss ol ol{counter-reset:ol2;}.css-1g0fqss ol ol li::before{counter-increment:ol2;content:counter(ol2) '. ';}.css-1g0fqss ol ol ol{counter-reset:ol3;}.css-1g0fqss ol ol ol li::before{counter-increment:ol3;content:counter(ol3) '. ';}.css-1g0fqss ol ol ol ol{counter-reset:ol4;}.css-1g0fqss ol ol ol ol li::before{counter-increment:ol4;content:counter(ol4) '. ';}.css-1g0fqss figure{margin:1.4em 0;}.css-1g0fqss figure .content_image,.css-1g0fqss figure .origin_image{margin:0 auto;}.css-1g0fqss figure figcaption{margin-top:calc(0.6em / 0.9);padding:0 1em;font-size:0.9em;line-height:1.5;text-align:center;color:#999999;}.css-1g0fqss figure + figure{margin-top:calc(1.4em * 1.6);}.css-1g0fqss figure[data-size='small'],.css-1g0fqss figure:not([data-size]) > [data-size='small']{clear:both;}.css-1g0fqss figure[data-size='left'],.css-1g0fqss figure:not([data-size]) > [data-size='left']{float:left;margin:0 20px 20px 0;max-width:33%;}.css-1g0fqss figure[data-size='right'],.css-1g0fqss figure:not([data-size]) > [data-size='right']{float:right;margin:0 0 20px 20px;max-width:33%;}.css-1g0fqss figure[data-size='collapse']{margin-bottom:0;}.css-1g0fqss figure[data-size='collapse'] + figure{margin-top:0;}.css-1g0fqss .content_image,.css-1g0fqss .origin_image{display:block;max-width:100%;height:auto;margin:1.4em auto;}.css-1g0fqss .content_image[data-size='small'],.css-1g0fqss .origin_image[data-size='small']{max-width:40%;}.css-1g0fqss .content_image.zh-lightbox-thumb,.css-1g0fqss .origin_image.zh-lightbox-thumb{cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;}.css-1g0fqss code{margin:0 2px;padding:3px 4px;border-radius:3px;font-family:Menlo,Monaco,Consolas,'Andale Mono','lucida console','Courier New',monospace;font-size:0.9em;background-color:#F6F6F6;}.css-1g0fqss pre{margin:1.4em 0;padding:calc(0.8em / 0.9);font-size:0.9em;word-break:initial;word-wrap:initial;white-space:pre;overflow:auto;-webkit-overflow-scrolling:touch;background:#F6F6F6;border-radius:4px;}.css-1g0fqss pre code{margin:0;padding:0;font-size:inherit;border-radius:0;background-color:inherit;}.css-1g0fqss li pre{white-space:pre-wrap;}.css-1g0fqss table[data-draft-type='table']{border-collapse:collapse;font-size:15px;margin:1.4em auto;max-width:100%;table-layout:fixed;text-align:left;width:100%;}.css-1g0fqss table[data-draft-type='table'][data-size='small']{min-width:260px;width:40%;}.css-1g0fqss table[data-draft-type='table'][data-row-style='striped'] tr:nth-of-type(2n + 1){background:#F6F6F6;}.css-1g0fqss table[data-draft-type='table'] td,.css-1g0fqss table[data-draft-type='table'] th{border:1px solid #D3D3D3;line-height:24px;height:24px;padding:3px 12px;}.css-1g0fqss table[data-draft-type='table'] th{background:#EBEBEB;color:#121212;font-weight:500;}.css-1g0fqss .video-box,.css-1g0fqss .link-box{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;margin:1.4em 0;overflow:auto;white-space:normal;cursor:pointer;border:solid 1px #EBEBEB;border-radius:4px;}.css-1g0fqss .lazy[data-lazy-status]{background-color:#F6F6F6;}.css-1g0fqss .lazy[data-lazy-status="ok"]{background-color:transparent;-webkit-animation:animation-1yvu044 0.5s ease-in;animation:animation-1yvu044 0.5s ease-in;}.css-1g0fqss .highlight{margin:1em 0;}.css-1g0fqss .highlight pre{margin:0;}.css-1g0fqss .highlight .hll{background-color:#FDFDFD;}.css-1g0fqss .highlight .c{font-style:italic;color:#999999;}.css-1g0fqss .highlight .err{color:#F1403C;}.css-1g0fqss .highlight .k{font-weight:600;}.css-1g0fqss .highlight .o{font-weight:600;}.css-1g0fqss .highlight .cm{font-style:italic;color:#999999;}.css-1g0fqss .highlight .cp{font-weight:600;color:#999999;}.css-1g0fqss .highlight .c1{font-style:italic;color:#999999;}.css-1g0fqss .highlight .cs{font-style:italic;font-weight:600;color:#999999;}.css-1g0fqss .highlight .gd{color:#FF3366;}.css-1g0fqss .highlight .ge{font-style:italic;}.css-1g0fqss .highlight .gr{color:#F1403C;}.css-1g0fqss .highlight .gh{color:#999999;}.css-1g0fqss .highlight .gi{color:#12b370;}.css-1g0fqss .highlight .go{color:#808080;}.css-1g0fqss .highlight .gp{color:#646464;}.css-1g0fqss .highlight .gs{font-weight:600;}.css-1g0fqss .highlight .gu{color:#999999;}.css-1g0fqss .highlight .gt{color:#F1403C;}.css-1g0fqss .highlight .kc{font-weight:600;}.css-1g0fqss .highlight .kd{font-weight:600;}.css-1g0fqss .highlight .kn{font-weight:600;}.css-1g0fqss .highlight .kp{font-weight:600;}.css-1g0fqss .highlight .kr{font-weight:600;}.css-1g0fqss .highlight .kt{font-weight:600;color:#175199;}.css-1g0fqss .highlight .m{color:#056DE8;}.css-1g0fqss .highlight .s{color:#F1403C;}.css-1g0fqss .highlight .na{color:#056DE8;}.css-1g0fqss .highlight .nb{color:#056DE8;}.css-1g0fqss .highlight .nc{font-weight:600;color:#175199;}.css-1g0fqss .highlight .no{color:#056DE8;}.css-1g0fqss .highlight .ni{color:#5555DD;}.css-1g0fqss .highlight .ne{font-weight:600;color:#F1403C;}.css-1g0fqss .highlight .nf{font-weight:600;color:#F1403C;}.css-1g0fqss .highlight .nn{color:#646464;}.css-1g0fqss .highlight .nt{color:#175199;}.css-1g0fqss .highlight .nv{color:#056DE8;}.css-1g0fqss .highlight .ow{font-weight:600;}.css-1g0fqss .highlight .w{color:#BFBFBF;}.css-1g0fqss .highlight .mf{color:#056DE8;}.css-1g0fqss .highlight .mh{color:#056DE8;}.css-1g0fqss .highlight .mi{color:#056DE8;}.css-1g0fqss .highlight .mo{color:#056DE8;}.css-1g0fqss .highlight .sb{color:#F1403C;}.css-1g0fqss .highlight .sc{color:#F1403C;}.css-1g0fqss .highlight .sd{color:#F1403C;}.css-1g0fqss .highlight .s2{color:#F1403C;}.css-1g0fqss .highlight .se{color:#F1403C;}.css-1g0fqss .highlight .sh{color:#F1403C;}.css-1g0fqss .highlight .si{color:#F1403C;}.css-1g0fqss .highlight .sx{color:#F1403C;}.css-1g0fqss .highlight .sr{color:#A5542F;}.css-1g0fqss .highlight .s1{color:#F1403C;}.css-1g0fqss .highlight .ss{color:#F1403C;}.css-1g0fqss .highlight .bp{color:#999999;}.css-1g0fqss .highlight .vc{color:#056DE8;}.css-1g0fqss .highlight .vg{color:#056DE8;}.css-1g0fqss .highlight .vi{color:#056DE8;}.css-1g0fqss .highlight .il{color:#056DE8;}.css-1g0fqss .highlight::-webkit-scrollbar{width:6px;height:6px;}.css-1g0fqss .highlight::-webkit-scrollbar-thumb:horizontal{background-color:rgba(18,18,18,0.5);border-radius:6px;}.css-1g0fqss .highlight::-webkit-scrollbar-thumb:horizontal:hover{background-color:rgba(18,18,18,0.6);}.css-1g0fqss .LinkCard.old{position:relative;display:block;margin:1em auto;width:390px;box-sizing:border-box;border-radius:12px;max-width:100%;overflow:hidden;}.css-1g0fqss .LinkCard.old,.css-1g0fqss .LinkCard.old:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-1g0fqss .LinkCard-ecommerceLoadingCard{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;padding:12px;border-radius:inherit;height:80px;box-sizing:border-box;background:rgba(246,246,246,0.88);color:#D3D3D3;}.css-1g0fqss .LinkCard-ecommerceLoadingCardAvatarWrapper{width:60px;height:60px;background:#EBEBEB;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;border-radius:6px;margin-right:10px;}.css-1g0fqss .LinkCard-ecommerceLoadingCardNetwork{width:20px;height:20px;}.css-1g0fqss .LinkCard-ecommerceLoadingCardLoadingbar{height:60px;-webkit-flex:1;-ms-flex:1;flex:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.css-1g0fqss .LinkCard-ecommerceLoadingCardLoadingbar span{height:16px;display:inline-block;background:#EBEBEB;}.css-1g0fqss .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(1){width:60px;margin-bottom:4px;}.css-1g0fqss .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(2){width:127px;}.css-1g0fqss .LinkCard.new{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;box-sizing:border-box;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:390px;min-height:84px;border-radius:8px;max-width:100%;overflow:hidden;margin:16px auto;padding:12px 12px 9px 12px;background-color:#F6F6F6;}.css-1g0fqss .LinkCard.new,.css-1g0fqss .LinkCard.new:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-1g0fqss .LinkCard.new .LinkCard-contents{display:block;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;position:relative;}.css-1g0fqss .LinkCard.new .LinkCard-contents .loading{height:14px;background:#EBEBEB;border-radius:7px;}.css-1g0fqss .LinkCard.new .LinkCard-contents.withTitle{margin-bottom:3px;}.css-1g0fqss .LinkCard.new .LinkCard-title{display:-webkit-box;font-size:15px;font-weight:500;line-height:1.4;margin-bottom:2px;color:#121212;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1g0fqss .LinkCard.new .LinkCard-title.two-line{line-height:20px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-1g0fqss .LinkCard.new .LinkCard-title.loading{margin-bottom:8px;width:80%;}.css-1g0fqss .LinkCard.new .LinkCard-title.loading.withTitle{margin-bottom:6px;}.css-1g0fqss .LinkCard.new .LinkCard-title.loadingTitle{margin-bottom:5px;}.css-1g0fqss .LinkCard.new .LinkCard-excerpt{display:-webkit-box;text-overflow:ellipsis;font-size:13px;line-height:18px;color:#999999;margin-bottom:4px;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1g0fqss .LinkCard.new .LinkCard-excerpt .LinkCard-author{color:#444444;}.css-1g0fqss .LinkCard.new .LinkCard-desc{display:-webkit-box;font-size:13px;height:18px;line-height:18px;color:#999999;word-break:break-all;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1g0fqss .LinkCard.new .LinkCard-desc .LinkCard-tag,.css-1g0fqss .LinkCard.new .LinkCard-desc .tag{display:inline-block;font-size:11px;margin-left:8px;padding:0 4px;border-radius:3px;background:rgba(211,211,211,0.3);}.css-1g0fqss .LinkCard.new .LinkCard-desc.loading{width:40%;}.css-1g0fqss .LinkCard.new .LinkCard-desc svg{margin-right:2px;}.css-1g0fqss .LinkCard.new .LinkCard-image{-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;background-color:#EBEBEB;background-size:cover;background-position:center;position:relative;display:block;width:60px;height:60px;margin-left:20px;object-fit:cover;border-radius:inherit;overflow:hidden;}.css-1g0fqss .LinkCard.new .LinkCard-image.LinkCard-image--default{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;background-color:#EBEBEB;color:#D3D3D3;}.css-1g0fqss .LinkCard.new .LinkCard-image.LinkCard-image--default svg{color:#999999;}.css-1g0fqss .LinkCard.new .LinkCard-image img{width:100%;height:100%;object-fit:cover;}.css-1g0fqss .LinkCard.new .LinkCard-image .LinkCard-image--video{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;position:absolute;top:50%;left:50%;-webkit-transform:translateX(-50%) translateY(-50%);-ms-transform:translateX(-50%) translateY(-50%);transform:translateX(-50%) translateY(-50%);width:24px;height:24px;border-radius:12px;background:rgba(255,255,255,0.9);pointer-events:none;}.css-1g0fqss .LinkCard.new .LinkCard-image .LinkCard-image--video svg{color:#444444;}.css-1g0fqss .LinkCard.new .LinkCard-richText .text{color:#444444;}.css-1g0fqss .LinkCard.new .LinkCard-richText .bold{font-weight:600;}.css-1g0fqss .LinkCard.new .LinkCard-richText .tag{margin-left:4px;}.css-1g0fqss .FileLinkCard{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:rgba(246,246,246,0.88);border-radius:12px;box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;margin:1em auto;max-width:100%;overflow:hidden;padding:12px;position:relative;width:390px;}.css-1g0fqss .FileLinkCard-icon{-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;height:30px;width:30px;}.css-1g0fqss .FileLinkCard-info{margin-left:12px;}.css-1g0fqss .FileLinkCard-name{color:#121212;font-size:15px;font-weight:500;line-height:21px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-1g0fqss .FileLinkCard-meta{color:#999999;font-size:12px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;line-height:14px;margin-top:5px;}.css-1g0fqss .FileLinkCard-source{white-space:pre;}.css-1g0fqss img[data-uncomfortable]{content:url(data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20viewBox%3D%220%200%20344.88888888888886%20194%22%3E%3CforeignObject%20width%3D%22344.88888888888886%22%20height%3D%22194%22%3E%0A%20%20%20%20%20%20%3Cdiv%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxhtml%22%20style%3D%22font-size%3A%2013px%3B%20font-family%3A%20-apple-system%2C%20BlinkMacSystemFont%2C%20Microsoft%20YaHei%2C%20sans-serif%3B%20color%3A%20%23fff%3B%20width%3A100%25%3B%20height%3A194px%3B%22%3E%0A%20%20%20%20%20%20%20%20%3Cdiv%20style%3D%22display%3A%20flex%3B%20flex-direction%3A%20column%3B%20align-items%3A%20center%3B%20justify-content%3A%20center%3B%20height%3A%20100%25%3B%22%3E%0A%20%20%20%20%20%20%20%20%20%20%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20width%3D%2218%22%20height%3D%2218%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22currentColor%22%3E%3Cpath%20d%3D%22M8%203.65a7%207%200%2000-1.353.128.65.65%200%2011-.25-1.275A8.3%208.3%200%20018%202.35c2.387%200%204.172.954%205.357%202.125C14.511%205.615%2015.15%207.022%2015.15%208c0%20.621-.257%201.391-.699%202.134a7.076%207.076%200%2001-1.403%201.68l.495.46a.65.65%200%2011-.886.951l-.998-.929a.645.645%200%2001-.104-.097L9.73%2010.501a.647.647%200%2001-.29.301%203.15%203.15%200%2001-4.313-4.094.647.647%200%2001.234-.275L3.908%205.08a5.774%205.774%200%2000-1.283%201.522C2.282%207.198%202.15%207.707%202.15%208c0%20.522.41%201.616%201.407%202.6.965.954%202.43%201.75%204.443%201.75.468%200%20.905-.043%201.311-.12a.65.65%200%2001.243%201.277A8.322%208.322%200%20018%2013.65c-2.387%200-4.172-.954-5.357-2.125C1.49%2010.385.85%208.978.85%208c0-.598.238-1.333.648-2.046A7.054%207.054%200%20012.95%204.188l-.547-.509a.65.65%200%2011.886-.951l8.8%208.194a5.793%205.793%200%20001.244-1.453c.372-.624.516-1.163.516-1.469%200-.522-.41-1.616-1.407-2.6-.965-.954-2.43-1.75-4.443-1.75zM6.29%207.296a1.85%201.85%200%20002.534%202.36l-2.535-2.36zM8%204.85a.65.65%200%20100%201.3%201.85%201.85%200%20011.843%201.694.65.65%200%20101.296-.11A3.15%203.15%200%20008%204.85z%22%20fill-rule%3D%22evenodd%22%20clip-rule%3D%22evenodd%22%3E%3C%2Fpath%3E%3C%2Fsvg%3E%0A%20%20%20%20%20%20%20%20%20%20%3Cdiv%20style%3D%22margin%3A%20.6em%200%201.2em%22%3E%E8%AF%A5%E5%9B%BE%E7%89%87%E6%9C%89%E5%8F%AF%E8%83%BD%E4%BC%9A%E5%BC%95%E8%B5%B7%E4%B8%8D%E9%80%82%3C%2Fdiv%3E%0A%20%20%20%20%20%20%20%20%20%20%3Cbutton%20style%3D%22padding%3A%204px%201em%3B%20font-size%3A%201.1em%3B%20color%3A%20inherit%3B%20background%3A%20none%3B%20border%3A%201px%20solid%20rgba%28255%2C255%2C255%2C.5%29%3B%20border-radius%3A%209999px%3B%22%3E%E7%BB%A7%E7%BB%AD%E6%9F%A5%E7%9C%8B%3C%2Fbutton%3E%0A%20%20%20%20%20%20%20%20%3C%2Fdiv%3E%0A%20%20%20%20%20%20%3C%2Fdiv%3E%0A%20%20%20%20%3C%2FforeignObject%3E%3C%2Fsvg%3E);width:100%;height:194px;background:url(https://pic1.zhimg.com/v2-cf70d0759d787c70091857151c1cad4a.jpeg) no-repeat rgba(191,191,191,0.7);background-size:cover;cursor:pointer!important;}@-webkit-keyframes animation-1yvu044{from{opacity:0;}to{opacity:1;}}@keyframes animation-1yvu044{from{opacity:0;}to{opacity:1;}}</style><style data-emotion-css="bypjk1 animation-js282a animation-18pxbu9">.css-bypjk1{position:absolute;top:50%;left:50%;margin:-15px 0 0 -15px;-webkit-animation:animation-18pxbu9 1s linear infinite;animation:animation-18pxbu9 1s linear infinite;}.css-bypjk1 .path{stroke:#056DE8;stroke-dasharray:187;stroke-dashoffset:0;-webkit-transform-origin:center;-ms-transform-origin:center;transform-origin:center;-webkit-animation:animation-js282a 1s ease-in-out infinite;animation:animation-js282a 1s ease-in-out infinite;}@-webkit-keyframes animation-js282a{0%{stroke-dashoffset:187;}50%{stroke-dashoffset:46.75;-webkit-transform:rotate(135deg);-ms-transform:rotate(135deg);transform:rotate(135deg);}100%{stroke-dashoffset:187;-webkit-transform:rotate(450deg);-ms-transform:rotate(450deg);transform:rotate(450deg);}}@keyframes animation-js282a{0%{stroke-dashoffset:187;}50%{stroke-dashoffset:46.75;-webkit-transform:rotate(135deg);-ms-transform:rotate(135deg);transform:rotate(135deg);}100%{stroke-dashoffset:187;-webkit-transform:rotate(450deg);-ms-transform:rotate(450deg);transform:rotate(450deg);}}@-webkit-keyframes animation-18pxbu9{0%{-webkit-transform:rotate(0deg);-ms-transform:rotate(0deg);transform:rotate(0deg);}100%{-webkit-transform:rotate(270deg);-ms-transform:rotate(270deg);transform:rotate(270deg);}}@keyframes animation-18pxbu9{0%{-webkit-transform:rotate(0deg);-ms-transform:rotate(0deg);transform:rotate(0deg);}100%{-webkit-transform:rotate(270deg);-ms-transform:rotate(270deg);transform:rotate(270deg);}}</style><style data-emotion-css="n99yhz">.css-n99yhz{box-sizing:border-box;margin:0;min-width:0;color:#175199;display:inline-block;margin-left:.3em;}</style><style data-emotion-css="18biwo">.css-18biwo{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><style data-emotion-css="1ifz0go">.css-1ifz0go{overflow:visible!important;}</style><style data-emotion-css="1m3x3v9">.css-1m3x3v9{width:1em;height:1em;}</style><style data-emotion="css"></style><style>.MathJax_Preview ~ .math-holder {display: none}</style></head><body aria-basefontsize="15" style="overflow: auto;"><a id="ariaTipText" role="pagedescription" aria-label="欢迎进入 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎,盲人用户使用操作智能引导，请按快捷键Ctrl+Alt+R；阅读详细操作说明请按快捷键Ctrl+Alt+问号键。" aria-atomic="true" href="javascript:void(0)" class="skipAutoFix" style="width: 1px; height: 1px;"><img src="https://www.zhihu.com/question/592844544" style="width:1px !important;height:1px !important;position:absolute;top:0;"></a><div id="root"><div><div class="LoadingBar  css-uzm3ri"></div><div><header role="banner" class="Sticky AppHeader css-1x8hcdw" data-za-module="TopNavBar" style=""><div><span role="log" aria-live="assertive" style="position: absolute; top: -10000px; left: -10000px;"></span></div><div class="AppHeader-inner css-l2ygoj"><a href="https://www.zhihu.com/" aria-label="知乎"><svg viewBox="0 0 64 30" fill="#056DE8" width="64" height="30" class="css-1hlrcxk"><path d="M29.05 4.582H16.733V25.94h3.018l.403 2.572 4.081-2.572h4.815V4.582zm-5.207 18.69l-2.396 1.509-.235-1.508h-1.724V7.233h6.78v16.04h-2.425zM14.46 14.191H9.982c0-.471.033-.954.039-1.458v-5.5h5.106V5.935a1.352 1.352 0 0 0-.404-.957 1.378 1.378 0 0 0-.968-.396H5.783c.028-.088.056-.177.084-.255.274-.82 1.153-3.326 1.153-3.326a4.262 4.262 0 0 0-2.413.698c-.57.4-.912.682-1.371 1.946-.532 1.453-.997 2.856-1.31 3.693C1.444 8.674.28 11.025.28 11.025a5.85 5.85 0 0 0 2.52-.61c1.119-.593 1.679-1.502 2.054-2.883l.09-.3h2.334v5.5c0 .5-.045.982-.073 1.46h-4.12c-.71 0-1.39.278-1.893.775a2.638 2.638 0 0 0-.783 1.874h6.527a17.717 17.717 0 0 1-.778 3.649 16.796 16.796 0 0 1-3.012 5.273A33.104 33.104 0 0 1 0 28.74s3.13 1.175 5.425-.954c1.388-1.292 2.631-3.814 3.23-5.727a28.09 28.09 0 0 0 1.12-5.229h5.967v-1.37a1.254 1.254 0 0 0-.373-.899 1.279 1.279 0 0 0-.909-.37z"></path><path d="M11.27 19.675l-2.312 1.491 5.038 7.458a6.905 6.905 0 0 0 .672-2.218 3.15 3.15 0 0 0-.28-2.168l-3.118-4.563zM51.449 15.195V5.842c4.181-.205 7.988-.405 9.438-.483l.851-.05c.387-.399.885-2.395.689-3.021-.073-.25-.213-.666-.638-.555a33.279 33.279 0 0 1-4.277.727c-2.766.321-3.97.404-7.804.682-6.718.487-12.709.72-12.709.72a2.518 2.518 0 0 0 .788 1.834 2.567 2.567 0 0 0 1.883.706c2.278-.095 5.598-.25 8.996-.41v9.203h-12.78c0 .703.281 1.377.783 1.874a2.69 2.69 0 0 0 1.892.777h10.105v7.075c0 .887-.464 1.192-1.231 1.214h-3.92a4.15 4.15 0 0 0 .837 1.544 4.2 4.2 0 0 0 1.403 1.067 6.215 6.215 0 0 0 2.71.277c1.36-.066 2.967-.826 2.967-3.57v-7.607h11.28c.342 0 .67-.135.91-.374.242-.239.378-.563.378-.902v-1.375H51.449z"></path><path d="M42.614 8.873a2.304 2.304 0 0 0-1.508-.926 2.334 2.334 0 0 0-1.727.405l-.376.272 4.255 5.85 2.24-1.62-2.884-3.98zM57.35 8.68l-3.125 4.097 2.24 1.663 4.517-5.927-.375-.277a2.32 2.32 0 0 0-1.722-.452 2.327 2.327 0 0 0-1.536.896z"></path></svg></a><ul role="tablist" class="Tabs AppHeader-Tabs css-g0ay3v"><li role="tab" class="Tabs-item AppHeader-Tab Tabs-item--noMeta"><a class="Tabs-link AppHeader-TabsLink css-12n8klz" tabindex="0" data-za-not-track-link="true" href="https://www.zhihu.com/">首页</a></li><li role="tab" class="Tabs-item AppHeader-Tab Tabs-item--noMeta"><a class="Tabs-link AppHeader-TabsLink css-12n8klz" tabindex="0" data-za-not-track-link="true" href="https://www.zhihu.com/education/learning">知学堂</a></li><li role="tab" class="Tabs-item AppHeader-Tab Tabs-item--noMeta"><a class="Tabs-link AppHeader-TabsLink css-12n8klz" tabindex="0" data-za-not-track-link="true" href="https://www.zhihu.com/xen/vip-web">会员</a></li><li role="tab" class="Tabs-item AppHeader-Tab Tabs-item--noMeta"><a class="Tabs-link AppHeader-TabsLink css-12n8klz" tabindex="0" data-za-not-track-link="true" href="https://www.zhihu.com/explore">发现</a></li><li role="tab" class="Tabs-item AppHeader-Tab Tabs-item--noMeta"><a class="Tabs-link AppHeader-TabsLink css-12n8klz" tabindex="0" data-za-not-track-link="true" href="https://www.zhihu.com/question/waiting">等你来答</a></li></ul><div class="css-1acwmmj"><div class="SearchBar css-10fy1q8" role="search" data-za-module="PresetWordItem"><form class="SearchBar-tool"><div><div class="Popover"><label class="SearchBar-input css-11bw1mm Input-wrapper QZcfWkCJoarhIYxlM_sG Input-wrapper--grey evPjxqnqXpIBzSRrcIDv"><input type="text" maxlength="100" autocomplete="off" role="combobox" aria-expanded="false" aria-autocomplete="list" aria-activedescendant="AutoComplete3--1" id="Popover2-toggle" aria-haspopup="true" aria-owns="Popover2-content" class="Input i7cW1UcwT6ThdhTakqFm" placeholder="工作能力还是学历重要" value=""><button aria-label="搜索" type="button" class="Button SearchBar-searchButton FEfUrdfMIKpQDJDqkjte Button--primary epMJl0lFQuYbC7jrwr_o"><span style="display: inline-flex; align-items: center;">​<svg width="18" height="18" viewBox="0 0 24 24" class="Zi Zi--Search SearchBar-searchIcon css-1dlt5yv" fill="currentColor"><g fill-rule="evenodd" clip-rule="evenodd"><path d="M11.5 18.389c3.875 0 7-3.118 7-6.945 0-3.826-3.125-6.944-7-6.944s-7 3.118-7 6.944 3.125 6.945 7 6.945Zm0 1.5c4.694 0 8.5-3.78 8.5-8.445C20 6.781 16.194 3 11.5 3S3 6.78 3 11.444c0 4.664 3.806 8.445 8.5 8.445Z"></path><path d="M16.47 16.97a.75.75 0 0 1 1.06 0l3.5 3.5a.75.75 0 1 1-1.06 1.06l-3.5-3.5a.75.75 0 0 1 0-1.06Z"></path></g></svg></span></button></label></div></div></form><button type="button" class="Button SearchBar-askButton css-3q84jd FEfUrdfMIKpQDJDqkjte Button--primary Button--blue epMJl0lFQuYbC7jrwr_o JmYzaky7MEPMFcJDLNMG">提问</button></div></div><div class="AppHeader-userInfo"><div class="Popover"><button aria-label="通知" id="Popover23-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover23-content" type="button" class="Button AppHeader-notifications css-79elbk FEfUrdfMIKpQDJDqkjte Button--plain fEPKGkUK5jyc4fUuT0QP"><span style="display: inline-flex; align-items: center;">​<svg width="18" height="18" viewBox="0 0 24 24" class="ZDI ZDI--BellFill24 css-7dgah8" fill="currentColor"><path fill-rule="evenodd" d="M9.723 21.271c0-.42.34-.76.76-.76h3.043a.76.76 0 0 1 0 1.521h-3.043a.76.76 0 0 1-.76-.76Z" clip-rule="evenodd"></path><path d="M11.153 3.115c0-.618.376-1.115.844-1.115.469 0 .845.499.845 1.115v.183c3.997.369 7.012 4.117 7.024 8.515V17.468h.253a.76.76 0 1 1 0 1.521H3.891a.76.76 0 0 1 0-1.521h.253V11.813c.011-4.392 3.02-8.137 7.009-8.514v-.184Z"></path></svg></span><div class="css-1m60na">消息</div></button></div><div><div class="Popover"><button aria-label="1条私信" id="Popover24-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover24-content" type="button" class="Button AppHeader-messages css-79elbk FEfUrdfMIKpQDJDqkjte Button--plain fEPKGkUK5jyc4fUuT0QP"><span style="display: inline-flex; align-items: center;">​<svg width="18" height="18" viewBox="0 0 24 24" class="ZDI ZDI--ChatBubbleTwoFill24 css-7dgah8" fill="currentColor"><path fill-rule="evenodd" d="M2 11c0 1.79.553 3.45 1.498 4.82L2.6 18.667a.6.6 0 0 0 .751.753l3.07-.96A8.5 8.5 0 1 0 2 11Zm11.46 9.414c-.457.16-.506.794-.034.904A6.96 6.96 0 0 0 15 21.5c1.148 0 2.422-.31 3.444-.912.357-.217.658-.378 1.043-.252l1.414.42c.357.112.679-.168.574-.546l-.47-1.57a.736.736 0 0 1 .05-.632c.602-1.108.945-2.32.945-3.498 0-1.07-.248-2.11-.7-3.046-.21-.435-.815-.25-.872.23-.47 3.954-3.211 7.394-6.968 8.72Z" clip-rule="evenodd"></path></svg></span><div class="css-11oarr3">1</div><div class="css-1m60na">私信</div></button></div></div><div class="AppHeader-profile"><div class="Popover AppHeader-menu"><button id="Popover11-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover11-content" type="button" class="Button AppHeader-profileEntry FEfUrdfMIKpQDJDqkjte Button--plain fEPKGkUK5jyc4fUuT0QP"><img class="Avatar AppHeader-profileAvatar css-1tzrnvf" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/v2-f0e1a2fef748f964e7076ddd7c61d545_l.jpg" srcset="https://picx.zhimg.com/v2-f0e1a2fef748f964e7076ddd7c61d545_l.jpg?source=32738c0c 2x" alt="点击打开全栈工程师的主页"></button></div><div class="Popover ddLajxN_Q0AuobBZjX9m AppHeaderProfileMenu-creatorHintPopover"><div class="AppHeaderProfileMenu-creatorHintToggler" id="Popover12-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover12-content"></div></div></div></div></div><div><div class="PageHeader"><div class="QuestionHeader-content isLogin"><div class="QuestionHeader-main"><h1 class="QuestionHeader-title">为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？</h1></div><div class="QuestionHeader-side" data-za-detail-view-path-module="ToolBar" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;592844544&quot;}}}"><div class="QuestionButtonGroup"><button type="button" class="Button FollowButton FEfUrdfMIKpQDJDqkjte Button--primary Button--blue epMJl0lFQuYbC7jrwr_o JmYzaky7MEPMFcJDLNMG">关注问题</button><a><button type="button" class="Button FEfUrdfMIKpQDJDqkjte Button--blue JmYzaky7MEPMFcJDLNMG"><span style="display: inline-flex; align-items: center;">​<svg width="16" height="16" viewBox="0 0 24 24" class="Zi Zi--Edit QuestionButton-icon" fill="currentColor"><path d="m7.841 20.043-4.328 1.18a.6.6 0 0 1-.737-.736l1.18-4.324a1.2 1.2 0 0 1 .314-.539l8.094-7.995a.9.9 0 0 1 1.268.003l2.736 2.736a.9.9 0 0 1 .004 1.268l-7.196 7.296-.802.802a1.2 1.2 0 0 1-.533.31ZM19.703 4.81l-.514-.513a2.542 2.542 0 0 0-3.595 0l-.999 1.067a.9.9 0 0 0 .02 1.252l2.77 2.768a.9.9 0 0 0 1.25.02l1.068-.999a2.542 2.542 0 0 0 0-3.594Z"></path></svg></span>写回答</button></a></div></div><div class="QuestionHeader-profile"><div class="Popover AppHeader-menu"><button id="Popover19-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover19-content" type="button" class="Button AppHeader-profileEntry FEfUrdfMIKpQDJDqkjte Button--plain fEPKGkUK5jyc4fUuT0QP"><img class="Avatar AppHeader-profileAvatar css-1tzrnvf" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/v2-f0e1a2fef748f964e7076ddd7c61d545_l.jpg" srcset="https://picx.zhimg.com/v2-f0e1a2fef748f964e7076ddd7c61d545_l.jpg?source=32738c0c 2x" alt="点击打开全栈工程师的主页"></button></div><div class="Popover ddLajxN_Q0AuobBZjX9m AppHeaderProfileMenu-creatorHintPopover"><div class="AppHeaderProfileMenu-creatorHintToggler" id="Popover20-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover20-content"></div></div></div></div></div></div></header></div><div><span role="log" aria-live="assertive" style="position: absolute; top: -10000px; left: -10000px;"></span></div><main role="main" class="App-main"><div><div class="QuestionPage" itemscope="" itemprop="mainEntity" itemtype="http://schema.org/Question" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;592844544&quot;}}}"><meta itemprop="name" content="为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？"><meta itemprop="url" content="https://www.zhihu.com/question/592844544"><meta itemprop="keywords" content="互联网,人工智能,AI技术,AIGC,ChatGPT"><meta itemprop="answerCount" content="6"><meta itemprop="commentCount" content="1"><meta itemprop="dateCreated" content="2023-03-30T12:36:23.000Z"><meta itemprop="dateModified" content="2023-03-30T12:36:23.000Z"><meta itemprop="zhihu:visitsCount"><meta itemprop="zhihu:followerCount" content="149"><div data-zop-question="{&quot;title&quot;:&quot;为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？&quot;,&quot;topics&quot;:[{&quot;name&quot;:&quot;互联网&quot;,&quot;id&quot;:&quot;19550517&quot;},{&quot;name&quot;:&quot;人工智能&quot;,&quot;id&quot;:&quot;19551275&quot;},{&quot;name&quot;:&quot;AI技术&quot;,&quot;id&quot;:&quot;20106982&quot;},{&quot;name&quot;:&quot;AIGC&quot;,&quot;id&quot;:&quot;26215901&quot;},{&quot;name&quot;:&quot;ChatGPT&quot;,&quot;id&quot;:&quot;26691895&quot;}],&quot;id&quot;:592844544,&quot;isEditable&quot;:true}"><div class="QuestionStatus"><div><span role="log" aria-live="assertive" style="position: absolute; top: -10000px; left: -10000px;"></span></div><div><span role="log" aria-live="assertive" style="position: absolute; top: -10000px; left: -10000px;"></span></div></div><div data-za-detail-view-path-module="QuestionDescription" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;592844544&quot;}}}"><div class="QuestionHeader"><div class="QuestionHeader-content"><div class="QuestionHeader-main"><div class="QuestionHeader-tags"><div class="QuestionHeader-topics"><div class="Tag QuestionTopic css-1s3a4zw" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19550517&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19550517" target="_blank"><div class="css-1gomreu">互联网</div></a></span></div><div class="Tag QuestionTopic css-1s3a4zw" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19551275&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19551275" target="_blank"><div class="css-1gomreu">人工智能</div></a></span></div><div class="Tag QuestionTopic css-1s3a4zw" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;20106982&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/20106982" target="_blank"><div class="css-1gomreu">AI技术</div></a></span></div><div class="Tag QuestionTopic css-1s3a4zw" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;26215901&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/26215901" target="_blank"><div class="css-1gomreu">AIGC</div></a></span></div><div class="Tag QuestionTopic css-1s3a4zw" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;26691895&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/26691895" target="_blank"><div class="css-1gomreu">ChatGPT</div></a></span></div></div></div><h1 class="QuestionHeader-title">为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？</h1><div class="css-4cffwv"><div class="LabelContainer-wrapper"></div></div><div><div class="css-eew49z"><div class="QuestionRichText QuestionRichText--expandable QuestionRichText--collapsed"><div><span itemprop="text">即使是按使用量付费的api也同样有这个限制 具体到不同模型可能限制还不一样，少的只有几千</span><button type="button" class="Button QuestionRichText-more FEfUrdfMIKpQDJDqkjte Button--plain fEPKGkUK5jyc4fUuT0QP">显示全部 <span style="display: inline-flex; align-items: center;">​<svg width="24" height="24" viewBox="0 0 24 24" class="Zi Zi--ArrowDown" fill="currentColor"><path fill-rule="evenodd" d="M12 13.248 8.22 9.223a.684.684 0 0 0-1.01 0 .796.796 0 0 0 0 1.075l4.15 4.42a.867.867 0 0 0 1.28 0l4.15-4.42a.796.796 0 0 0 0-1.075.684.684 0 0 0-1.01 0L12 13.248Z" clip-rule="evenodd"></path></svg></span></button></div></div></div></div></div><div class="QuestionHeader-side"><div class="QuestionHeader-follow-status"><div class="QuestionFollowStatus"><div class="NumberBoard QuestionFollowStatus-counts NumberBoard--divider"><button type="button" class="Button NumberBoard-item FEfUrdfMIKpQDJDqkjte Button--plain fEPKGkUK5jyc4fUuT0QP"><div class="NumberBoard-itemInner"><div class="NumberBoard-itemName">关注者</div><strong class="NumberBoard-itemValue" title="149">149</strong></div></button><div class="NumberBoard-item"><div class="NumberBoard-itemInner"><div class="NumberBoard-itemName">被浏览</div><strong class="NumberBoard-itemValue" title="70496">70,496</strong></div></div></div></div></div></div></div><div class="QuestionHeader-footer"><div class="QuestionHeader-footer-inner"><div class="QuestionHeader-main QuestionHeader-footer-main"><div class="QuestionButtonGroup"><button type="button" class="Button FollowButton FEfUrdfMIKpQDJDqkjte Button--primary Button--blue epMJl0lFQuYbC7jrwr_o JmYzaky7MEPMFcJDLNMG">关注问题</button><a><button type="button" class="Button FEfUrdfMIKpQDJDqkjte Button--blue JmYzaky7MEPMFcJDLNMG"><span style="display: inline-flex; align-items: center;">​<svg width="16" height="16" viewBox="0 0 24 24" class="Zi Zi--Edit QuestionButton-icon" fill="currentColor"><path d="m7.841 20.043-4.328 1.18a.6.6 0 0 1-.737-.736l1.18-4.324a1.2 1.2 0 0 1 .314-.539l8.094-7.995a.9.9 0 0 1 1.268.003l2.736 2.736a.9.9 0 0 1 .004 1.268l-7.196 7.296-.802.802a1.2 1.2 0 0 1-.533.31ZM19.703 4.81l-.514-.513a2.542 2.542 0 0 0-3.595 0l-.999 1.067a.9.9 0 0 0 .02 1.252l2.77 2.768a.9.9 0 0 0 1.25.02l1.068-.999a2.542 2.542 0 0 0 0-3.594Z"></path></svg></span>写回答</button></a></div><div class="QuestionHeaderActions"><button type="button" class="Button FEfUrdfMIKpQDJDqkjte Button--grey Button--withIcon Button--withLabel ZdfrHW7Ef5ZjwFiiBJuS B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp" style="margin-right: 16px;"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Invite Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path fill-rule="evenodd" d="M5.5 7.5A5.5 5.5 0 0 1 11 2a5.5 5.5 0 0 1 5.5 5.5A5.5 5.5 0 0 1 11 13a5.5 5.5 0 0 1-5.5-5.5Zm8.11 9.498c.404-.408.91-1 1.17-1.51.067-.133.13-.284.165-.442.034-.15.058-.373-.033-.602a.872.872 0 0 0-.545-.509 1.37 1.37 0 0 0-.604-.043c-.657.082-1.518.184-2.373.24-.867.055-1.68.058-2.254-.041-1.189-.204-2.045-.19-2.781.087-.722.272-1.25.773-1.804 1.302-1.533 1.462-2.434 3.311-2.65 4.831-.11.78.535 1.339 1.199 1.339h8.1a.96.96 0 0 0 .955-.929c.06-1.767.7-2.96 1.456-3.723Zm5.596-2.292a.706.706 0 0 0-1.412 0v2.588h-2.588a.706.706 0 0 0 0 1.412h2.588v2.588a.706.706 0 1 0 1.412 0v-2.588h2.588a.706.706 0 0 0 0-1.412h-2.588v-2.588Z" clip-rule="evenodd"></path></svg></span>邀请回答</button><div class="GoodQuestionAction"><button type="button" class="Button GoodQuestionAction-commonBtn FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Like Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path d="M8.5 4.078c0-1.834 1.986-2.979 3.573-2.06a4.826 4.826 0 0 1 2.379 4.71l-.114 1.022h3.581c2.53 0 4.334 2.454 3.58 4.868l-1.823 5.833a3.784 3.784 0 0 1-3.848 2.64c-2.372-.147-6.042-.341-8.828-.341H4.5A1.75 1.75 0 0 1 2.75 19V9.5c0-.967.784-1.75 1.75-1.75h.637a3.418 3.418 0 0 0 3.19-2.191c.115-.296.173-.611.173-.928v-.553Z"></path></svg></span>好问题 4</button></div><div class="QuestionHeader-Comment"><button type="button" class="Button FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Comment Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>1 条评论</button></div><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="Popover4-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover4-content"><button type="button" class="Button FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Share Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path d="M19.47 1.914a.8.8 0 0 1 1.204.778l-1.872 16.386a.9.9 0 0 1-1.204.743l-4.615-1.692a.7.7 0 0 0-.831.28l-1.927 3.02c-.43.674-1.474.369-1.474-.43v-3.865a.8.8 0 0 1 .179-.504l5.808-7.148a.595.595 0 0 0-.897-.781l-5.93 6.354a1.1 1.1 0 0 1-1.258.252L2.57 13.46a.8.8 0 0 1-.08-1.415l16.98-10.13Z"></path></svg></span>分享</button></div></div><div class="Popover"><button aria-label="更多" id="Popover5-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover5-content" type="button" class="Button FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--iconOnly fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY hIwDV_tcL6XN1HprrnAq"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Dots Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path fill-rule="evenodd" d="M5.83 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Zm7.835 0a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Zm6.17 1.665a1.665 1.665 0 1 0 0-3.33 1.665 1.665 0 0 0 0 3.33Z" clip-rule="evenodd"></path></svg></span></button></div></div><div class="QuestionHeader-actions"></div></div></div></div></div></div><div><div class="Sticky" style=""></div></div></div><div id="AnswerFormPortalContainer" class="css-124jo1g"><div class="QuestionAnswers-statusWrapper"></div></div><div class="Question-main"><div class="Question-mainColumn"><div><div id="QuestionAnswers-answers" class="QuestionAnswers-answers" data-zop-feedlistmap="0,0,1,0" data-za-detail-view-path-module="ContentList" data-za-extra-module="{}"><div class="Card AnswersNavWrapper"><div class="ListShortcut"><div class="List"><div class="List-header"><h4 class="List-headerText"><span>6 个回答</span></h4><div class="List-headerOptions"><div class="Popover ddLajxN_Q0AuobBZjX9m css-yvuz2o"><button role="combobox" aria-expanded="false" id="Popover6-toggle" aria-haspopup="true" aria-owns="Popover6-content" type="button" class="Button InputLike EQvEDwRqICOvs_x_kUWW InputButton W2ewFU1c0bx6tahox08q Select-button HrHisPElmD_XxB9LKi6j Select-plainButton h39gnJsHf0NWeCTuygxV FEfUrdfMIKpQDJDqkjte Button--plain fEPKGkUK5jyc4fUuT0QP">默认排序<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ArrowUpDown24 Select-arrow RXYxRcvJ7zVIfNZSOcHa" fill="currentColor"><path fill-rule="evenodd" d="M12.53 3.47a.75.75 0 0 0-1.06 0l-5 5a.75.75 0 0 0 1.06 1.06L12 5.06l4.47 4.47a.75.75 0 1 0 1.06-1.06l-5-5Zm-5 11a.75.75 0 0 0-1.06 1.06l5 5a.75.75 0 0 0 1.06 0l5-5a.75.75 0 1 0-1.06-1.06L12 18.94l-4.47-4.47Z" clip-rule="evenodd"></path></svg></button></div></div></div><div><div class=" css-0" role="list"><div role="listitem"></div><div class="List-item" tabindex="0"><div><div class="ContentItem AnswerItem" data-za-index="0" data-zop="{&quot;authorName&quot;:&quot;苏剑林&quot;,&quot;itemId&quot;:3027439372,&quot;title&quot;:&quot;为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="3027439372" itemprop="acceptedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="0" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;3027439372&quot;,&quot;upvote_num&quot;:319,&quot;comment_num&quot;:32,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;592844544&quot;,&quot;author_member_hash_id&quot;:&quot;ba2e8e5e459263c8074c6a50cd2763a4&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><div class="AuthorInfo"><meta itemprop="name" content="苏剑林"><meta itemprop="image" content="https://picx.zhimg.com/v2-66f6fe0483a9e7ac39392659c121a0db_l.jpg?source=1940ef5c"><meta itemprop="url" content="https://www.zhihu.com/people/su-jian-lin-22"><meta itemprop="zhihu:followerCount" content="39618"><span class="UserLink AuthorInfo-avatarWrapper"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/su-jian-lin-22" target="_blank" class="UserLink-link" data-za-detail-view-element_name="User"><img class="Avatar AuthorInfo-avatar css-1oqflzh" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/v2-66f6fe0483a9e7ac39392659c121a0db_l.jpg" srcset="https://picx.zhimg.com/v2-66f6fe0483a9e7ac39392659c121a0db_l.jpg?source=1940ef5c 2x" alt="苏剑林"></a></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/su-jian-lin-22" target="_blank" class="UserLink-link" data-za-detail-view-element_name="User">苏剑林</a></div><span rel="noopener noreferrer" class="css-2dtzk2">​<svg width="24" height="24" fill="none" viewBox="0 0 24 24" class="ZDI ZDI--Vip24 css-wu55un"><path fill="url(#id-1978186969-a)" d="M21.351 4.858a3.114 3.114 0 0 1 1.577 2.702v8.88a3.114 3.114 0 0 1-1.577 2.702l-7.774 4.44a3.184 3.184 0 0 1-3.155 0l-7.773-4.44A3.114 3.114 0 0 1 1.07 16.44V7.56c0-1.115.602-2.145 1.578-2.702l7.774-4.44a3.185 3.185 0 0 1 3.154 0l7.774 4.44Zm-2.475 1.904L13.16 3.507a2.347 2.347 0 0 0-2.32 0L5.124 6.762a2.283 2.283 0 0 0-1.16 1.982v6.512c0 .817.442 1.573 1.16 1.982l5.716 3.255a2.347 2.347 0 0 0 2.32 0l5.716-3.255a2.283 2.283 0 0 0 1.16-1.982V8.744c0-.817-.443-1.573-1.16-1.982Zm-9.874 4.13L12 5.76l2.998 5.131 2.466-2.12v6.343c0 .49-.4.886-.893.886H7.429a.89.89 0 0 1-.893-.887V8.771l2.466 2.12Z"></path><defs><lineargradient id="id-1978186969-a" x1="3.479" x2="21.812" y1="4.463" y2="21.357" gradientUnits="userSpaceOnUse"><stop stop-color="#F2D9B6"></stop><stop offset="1" stop-color="#BF8B43"></stop></lineargradient></defs></svg></span></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="ztext AuthorInfo-badgeText css-14ur8a8">数学、python、数据挖掘、天文</div></div></div></div></div><button type="button" class="Button FollowButton css-upmq18 FEfUrdfMIKpQDJDqkjte Button--secondary Button--blue E211w_M7Hzs0GMB7BEyA JmYzaky7MEPMFcJDLNMG"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--PlusFill24" fill="currentColor"><path fill-rule="evenodd" d="M13.25 3.25a1.25 1.25 0 1 0-2.5 0v7.5h-7.5a1.25 1.25 0 1 0 0 2.5h7.5v7.5a1.25 1.25 0 1 0 2.5 0v-7.5h7.5a1.25 1.25 0 0 0 0-2.5h-7.5v-7.5Z" clip-rule="evenodd"></path></svg></span> 关注</button><div class="css-124ezq8"></div></div><div class="LabelContainer-wrapper"></div><div class="css-1k5dpte"><span><span class="Voters"><button type="button" class="Button FEfUrdfMIKpQDJDqkjte Button--plain fEPKGkUK5jyc4fUuT0QP">319 人赞同了该回答</button></span></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="319"><meta itemprop="url" content="https://www.zhihu.com/question/592844544/answer/3027439372"><meta itemprop="dateCreated" content="2023-05-14T08:00:50.000Z"><meta itemprop="dateModified" content="2023-05-15T03:33:54.000Z"><meta itemprop="commentCount" content="32"><div class="RichContent RichContent--unescapable"><span><div class="RichContent-inner"><div class="css-376mun"><span class="RichText ztext CopyrightRichText-richText css-1g0fqss" options="[object Object]" itemprop="text"><p data-first-child="" data-pid="Kze_JE-0">既是技术问题，也是算力问题。</p><hr><p data-pid="8sPcJO03">算力问题很容易理解，GPT4大概率还是用Transformer模型。Transformer如果经过各种Linear技术的优化（如Sparse），那么Scaling Law难以保证；如果保持原样，那么复杂度是二次的，序列一长，训练成本和推理成本都比较难顶。</p><hr><p data-pid="0hEppDtz">技术问题则主要是Transformer的长度外推性并不好。如果你想处理1000长度的文本，那么预训练阶段就拿1000长度的文本去训，那自然没有问题，但如果你只拿500长度的文本去预训练，那么得到的模型通常无法很好地处理1000长度的文本，尤其是生成模型场景。</p><p data-pid="ntKbwGcP">也就是说，<span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=%E7%9F%AD%E6%96%87%E6%9C%AC&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3027439372%7D" target="_blank">短文本<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>训练的模型，通常无法直接处理长文本，这就是长度外推问题，这个问题不只是Transformer有，RNN甚至CNN都会有。注意这里的长短是相对的，如果你想处理10000长度的文本，那么5000长度都算短文本了。</p><p data-pid="evmtO9PU">已经有一些工作试图解决这个问题，比如ALIBI、KERPLE、XPOS等，可以参考 <a href="https://link.zhihu.com/?target=https%3A//kexue.fm/archives/9431" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">Transformer升级之路：7、长度外推性与局部注意力 - 科学空间|Scientific Spaces</a> ，但是这类工作都是基于局部化Attention思想强行赋予平移不变性，无法做到全局依赖，在LLM场景下意义不大。最近笔者也进行了一些尝试，初步看下述方案能保留全局依赖：</p><div class="RichText-LinkCardContainer"><a target="_blank" href="https://link.zhihu.com/?target=https%3A//kexue.fm/archives/9603" data-draft-node="block" data-draft-type="link-card" data-text="Transformer升级之路：9、一种全局长度外推的新思路 - 科学空间|Scientific Spaces" class="LinkCard new css-1wr1m8" data-za-detail-view-id="172"><span class="LinkCard-contents"><span class="LinkCard-title two-line">Transformer升级之路：9、一种全局长度外推的新思路 - 科学空间|Scientific Spaces</span><span class="LinkCard-desc"><span style="display: inline-flex; align-items: center;">​<svg width="14" height="14" viewBox="0 0 24 24" class="Zi Zi--InsertLink" fill="currentColor"><path fill-rule="evenodd" d="M5.327 18.883a3.005 3.005 0 0 1 0-4.25l2.608-2.607a.75.75 0 1 0-1.06-1.06l-2.608 2.607a4.505 4.505 0 0 0 6.37 6.37l2.608-2.607a.75.75 0 0 0-1.06-1.06l-2.608 2.607a3.005 3.005 0 0 1-4.25 0Zm5.428-11.799a.75.75 0 0 0 1.06 1.06L14.48 5.48a3.005 3.005 0 0 1 4.25 4.25l-2.665 2.665a.75.75 0 0 0 1.061 1.06l2.665-2.664a4.505 4.505 0 0 0-6.371-6.372l-2.665 2.665Zm5.323 2.117a.75.75 0 1 0-1.06-1.06l-7.072 7.07a.75.75 0 0 0 1.061 1.06l7.071-7.07Z" clip-rule="evenodd"></path></svg></span>kexue.fm/archives/9603</span></span></a></div><p data-pid="dSulfxbC">此外，还有一个名为Parallel Context Window的方法值得一提，它是一种事后修改方案，能够增强训练好的模型的处理长度，并且理论上能保持全局依赖：</p><div class="RichText-LinkCardContainer"><a target="_blank" href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2212.10947" data-draft-node="block" data-draft-type="link-card" data-text="Parallel Context Windows Improve In-Context Learning of Large Language Models" class="LinkCard new css-1wr1m8" data-image="https://pic4.zhimg.com/v2-4baaae2386ede0213c693947a141a747_180x120.jpg" data-image-width="1200" data-image-height="700" data-za-detail-view-id="172"><span class="LinkCard-contents"><span class="LinkCard-title two-line">Parallel Context Windows Improve In-Context Learning of Large Language Models</span><span class="LinkCard-desc"><span style="display: inline-flex; align-items: center;">​<svg width="14" height="14" viewBox="0 0 24 24" class="Zi Zi--InsertLink" fill="currentColor"><path fill-rule="evenodd" d="M5.327 18.883a3.005 3.005 0 0 1 0-4.25l2.608-2.607a.75.75 0 1 0-1.06-1.06l-2.608 2.607a4.505 4.505 0 0 0 6.37 6.37l2.608-2.607a.75.75 0 0 0-1.06-1.06l-2.608 2.607a3.005 3.005 0 0 1-4.25 0Zm5.428-11.799a.75.75 0 0 0 1.06 1.06L14.48 5.48a3.005 3.005 0 0 1 4.25 4.25l-2.665 2.665a.75.75 0 0 0 1.061 1.06l2.665-2.664a4.505 4.505 0 0 0-6.371-6.372l-2.665 2.665Zm5.323 2.117a.75.75 0 1 0-1.06-1.06l-7.072 7.07a.75.75 0 0 0 1.061 1.06l7.071-7.07Z" clip-rule="evenodd"></path></svg></span>arxiv.org/abs/2212.10947</span></span><span class="LinkCard-image" style="height: 60px;"><img src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/v2-4baaae2386ede0213c693947a141a747_180x120.jpg" alt=""></span></a></div><hr><p data-pid="tLqmZ05h">至于Claude的<span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=100k%20token&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3027439372%7D" target="_blank">100k token<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>或者GPT4的32k token是怎么做的，这个没有任何技术细节披露，没法猜。个人感觉100k其实还在能想象的范围内，硬训也是也可能的。说到这里，顺便提一件事：OpenAI在2019年的论文 <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1904.10509" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">Generating Long Sequences with Sparse Transformers</a> 就已经做到了一万多token的自回归生成，并且它的Sparsity Pattern跟Parallel Context Learning很相似。</p><p data-pid="1x6WRvia">也就是说，人家19年的<span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=%E8%87%AA%E5%9B%9E%E5%BD%92%E7%94%9F%E6%88%90%E9%95%BF%E5%BA%A6&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3027439372%7D" target="_blank">自回归生成长度<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>就能够突破10k（并且还预见了接近Parallel Context Learning的方案），现在<span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=32k&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3027439372%7D" target="_blank">32k<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>就是“洒洒水”了。</p></span></div></div></span><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/592844544/answer/3027439372"><span data-tooltip="发布于 2023-05-14 16:00" aria-label="发布于 2023-05-14 16:00">编辑于 2023-05-15 11:33</span></a></div><div class="Reward"><div><div class="Reward-tagline">真诚赞赏，手留余香</div><button class="Reward-rewardBtn">赞赏</button></div><div class="Reward-countZero">还没有人赞赏，快来当第一个赞赏的人吧！</div></div></div><span></span><div><div class="ContentItem-actions Sticky RichContent-actions is-fixed is-bottom" style="width: 694px; bottom: 0px; left: 340px;"><span><button aria-label="赞同 319 " aria-live="polite" type="button" class="Button VoteButton VoteButton--up FEfUrdfMIKpQDJDqkjte"><span style="display: inline-flex; align-items: center;">​<svg width="10" height="10" viewBox="0 0 24 24" class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor"><path fill-rule="evenodd" d="M13.792 3.681c-.781-1.406-2.803-1.406-3.584 0l-7.79 14.023c-.76 1.367.228 3.046 1.791 3.046h15.582c1.563 0 2.55-1.68 1.791-3.046l-7.79-14.023Z" clip-rule="evenodd"></path></svg></span>赞同 319</button><button aria-label="反对" aria-live="polite" type="button" class="Button VoteButton VoteButton--down FEfUrdfMIKpQDJDqkjte"><span style="display: inline-flex; align-items: center;">​<svg width="10" height="10" viewBox="0 0 24 24" class="Zi Zi--TriangleDown" fill="currentColor"><path fill-rule="evenodd" d="M13.792 20.319c-.781 1.406-2.803 1.406-3.584 0L2.418 6.296c-.76-1.367.228-3.046 1.791-3.046h15.582c1.563 0 2.55 1.68 1.791 3.046l-7.79 14.023Z" clip-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Comment Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>32 条评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover21-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover21-content"><button type="button" class="Button FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Share Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path d="M19.47 1.914a.8.8 0 0 1 1.204.778l-1.872 16.386a.9.9 0 0 1-1.204.743l-4.615-1.692a.7.7 0 0 0-.831.28l-1.927 3.02c-.43.674-1.474.369-1.474-.43v-3.865a.8.8 0 0 1 .179-.504l5.808-7.148a.595.595 0 0 0-.897-.781l-5.93 6.354a1.1 1.1 0 0 1-1.258.252L2.57 13.46a.8.8 0 0 1-.08-1.415l16.98-10.13Z"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Star Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path d="M10.484 3.307c.673-1.168 2.358-1.168 3.032 0l2.377 4.122a.25.25 0 0 0 .165.12l4.655.987c1.319.28 1.84 1.882.937 2.884l-3.186 3.535a.25.25 0 0 0-.063.193l.5 4.733c.142 1.34-1.222 2.33-2.453 1.782l-4.346-1.938a.25.25 0 0 0-.204 0l-4.346 1.938c-1.231.549-2.595-.442-2.453-1.782l.5-4.733a.25.25 0 0 0-.064-.193L2.35 11.42c-.903-1.002-.382-2.604.937-2.884l4.655-.987a.25.25 0 0 0 .164-.12l2.378-4.122Z"></path></svg></span>收藏</button><button aria-live="polite" type="button" class="Button ContentItem-action FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Heart Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path fill-rule="evenodd" d="M12.004 4.934c1.015-.944 2.484-1.618 3.98-1.618 3.48 0 6.53 3.265 6.15 7.614-.11 1.254-.686 2.55-1.458 3.753-.778 1.215-1.79 2.392-2.845 3.419-1.054 1.028-2.168 1.923-3.161 2.566a9.96 9.96 0 0 1-1.41.777c-.418.182-.862.32-1.268.32s-.848-.137-1.267-.317a9.918 9.918 0 0 1-1.407-.771c-.992-.64-2.103-1.53-3.156-2.555-1.052-1.024-2.062-2.2-2.84-3.417-.77-1.208-1.346-2.51-1.456-3.775-.38-4.349 2.67-7.614 6.15-7.614 1.484 0 2.983.673 3.988 1.618Z" clip-rule="evenodd"></path></svg></span>喜欢</button><div class="Popover ContentItem-action"><button aria-label="更多" id="Popover22-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover22-content" type="button" class="Button OptionsButton FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--iconOnly fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY hIwDV_tcL6XN1HprrnAq"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Dots Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path fill-rule="evenodd" d="M5.83 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Zm7.835 0a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Zm6.17 1.665a1.665 1.665 0 1 0 0-3.33 1.665 1.665 0 0 0 0 3.33Z" clip-rule="evenodd"></path></svg></span></button></div><button data-zop-retract-question="true" type="button" class="Button ContentItem-action ContentItem-rightButton FEfUrdfMIKpQDJDqkjte Button--plain fEPKGkUK5jyc4fUuT0QP"><span class="RichContent-collapsedText">收起</span><span style="display: inline-flex; align-items: center;">​<svg width="24" height="24" viewBox="0 0 24 24" class="Zi Zi--ArrowDown ContentItem-arrowIcon is-active" fill="currentColor"><path fill-rule="evenodd" d="M12 13.248 8.22 9.223a.684.684 0 0 0-1.01 0 .796.796 0 0 0 0 1.075l4.15 4.42a.867.867 0 0 0 1.28 0l4.15-4.42a.796.796 0 0 0 0-1.075.684.684 0 0 0-1.01 0L12 13.248Z" clip-rule="evenodd"></path></svg></span></button></div><div class="Sticky--holder" style="position: static; inset: auto; display: flex; float: none; margin: 0px -20px -10px; height: 54px; width: 694px;"></div></div></div><div><div><div class=""></div><div class="ModalLoading-content"><svg width="30" height="30" viewBox="0 0 66 66" xmlns="http://www.w3.org/2000/svg" class="CircleLoadingBar  css-bypjk1" aria-hidden="true"><g><circle class="path" fill="none" stroke-width="6" stroke-linecap="round" cx="33" cy="33" r="30"></circle></g></svg></div></div></div></div></div></div><div class="List-item" tabindex="0"><div><div class="ContentItem AnswerItem" data-za-index="1" data-zop="{&quot;authorName&quot;:&quot;数据学习&quot;,&quot;itemId&quot;:3031186320,&quot;title&quot;:&quot;为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="3031186320" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="1" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;3031186320&quot;,&quot;upvote_num&quot;:6,&quot;comment_num&quot;:1,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;592844544&quot;,&quot;author_member_hash_id&quot;:&quot;0b41335b2041b4a052f123e492fbf45a&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><div class="AuthorInfo"><meta itemprop="name" content="数据学习"><meta itemprop="image" content="https://pic1.zhimg.com/v2-d2570783a2ad9d096539faf0f2beea83_l.jpg?source=1940ef5c"><meta itemprop="url" content="https://www.zhihu.com/people/datalearner"><meta itemprop="zhihu:followerCount" content="1473"><span class="UserLink AuthorInfo-avatarWrapper"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/datalearner" target="_blank" class="UserLink-link" data-za-detail-view-element_name="User"><img class="Avatar AuthorInfo-avatar css-1oqflzh" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/v2-d2570783a2ad9d096539faf0f2beea83_l.jpg" srcset="https://pic1.zhimg.com/v2-d2570783a2ad9d096539faf0f2beea83_l.jpg?source=1940ef5c 2x" alt="数据学习"></a></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/datalearner" target="_blank" class="UserLink-link" data-za-detail-view-element_name="User">数据学习</a></div><a href="https://www.zhihu.com/question/48510028" target="_blank" class="css-n99yhz" aria-label="合肥工业大学 管理科学与工程博士" data-tooltip="合肥工业大学 管理科学与工程博士"><span class="css-18biwo">​<svg viewBox="0 0 24 24" class="css-1ifz0go" width="18" height="18"><defs><lineargradient id="biz-badges-blue" x1="0" y1="0" x2="100%" y2="73.32%"><stop offset="15%" stop-color="#47D3FF"></stop><stop offset="81.54%" stop-color="#4A7DFF"></stop></lineargradient></defs><svg viewBox="0 0 24 24" x="-3" y="-3" fill="#FFFFFF" width="30" height="30"><path d="M3.56231227,13.8535307 C2.40051305,12.768677 2.41398885,11.0669203 3.59484487,9.99979213 L3.59222085,9.99654885 C4.26730143,9.45036719 4.79446755,8.21005186 4.7184197,7.34453784 L4.72305873,7.34412719 C4.66942824,5.75539997 5.8824188,4.56066914 7.47188965,4.64242381 L7.47229112,4.6386236 C8.33515314,4.72977993 9.58467253,4.22534048 10.1426329,3.55925173 L10.1462611,3.56228565 C11.2316055,2.40008701 12.9353108,2.41394456 14.0015072,3.59634088 L14.0047263,3.59374004 C14.5498229,4.26841874 15.7896857,4.79521622 16.6545744,4.71844347 L16.6549836,4.72304294 C18.245027,4.66894057 19.4396947,5.88213996 19.3575031,7.47241135 L19.3623099,7.47292747 C19.2704388,8.3358681 19.7742711,9.58421483 20.4407199,10.1424506 L20.437686,10.1460789 C21.5997217,11.2312209 21.5860695,12.9345218 20.4042441,14.0007396 L20.4072865,14.0045125 C19.7325967,14.5495925 19.2055209,15.7896954 19.2815865,16.6561959 L19.2770449,16.6565978 C19.3315454,18.2453037 18.1173775,19.4393568 16.5274188,19.3571512 L16.5269029,19.3619539 C15.6647098,19.270083 14.415408,19.7741709 13.8573671,20.4403558 L13.8537409,20.4373235 C12.76842,21.5995708 11.0650432,21.5864553 9.99899434,20.4039226 L9.99527367,20.406923 C9.45025436,19.7323399 8.21017638,19.2051872 7.34461983,19.2812352 L7.344304,19.2776405 C5.75448683,19.3312904 4.55977145,18.1170085 4.64254978,16.527117 L4.63769921,16.5265942 C4.72957031,15.6644394 4.22547659,14.4151814 3.55928015,13.8571569 L3.56231227,13.8535307 Z"></path></svg><path d="M2.63951518,13.3895441 C3.70763333,14.2842292 4.44777637,16.1226061 4.30075305,17.5023312 L4.32211542,17.3063047 C4.17509209,18.6910561 5.17786655,19.7063729 6.5613937,19.5844846 L6.364106,19.6008202 C7.75140298,19.4789319 9.57474349,20.2554985 10.4468305,21.3349009 L10.3224262,21.1803415 C11.1982831,22.2647703 12.6257916,22.2723098 13.5167278,21.2079863 L13.3898102,21.3600325 C14.2845162,20.2919393 16.1229361,19.5518136 17.5026934,19.6988334 L17.3054057,19.6774716 C18.6914461,19.8244915 19.7067866,18.8217404 19.5836389,17.4395022 L19.6012314,17.6367853 C19.4793403,16.2482641 20.255925,14.4249662 21.3353526,13.5528995 L21.1807897,13.677301 C22.2639871,12.8014646 22.2727834,11.3739894 21.2084351,10.483074 L21.3604848,10.6099886 C20.2923667,9.71530351 19.5522236,7.87818322 19.6992469,6.49720154 L19.6778846,6.69448464 C19.8249079,5.30847665 18.8221335,4.2944164 17.4386063,4.41630468 L17.635894,4.39871256 C16.248597,4.52185742 14.4252565,3.74529084 13.5531695,2.66588842 L13.6775738,2.81919121 C12.8017169,1.73601905 11.3742084,1.72722299 10.4832722,2.79154644 L10.6101898,2.63950024 C9.71548377,3.70759343 7.87706394,4.44771919 6.49730661,4.30195588 L6.69459432,4.32206116 C5.30855394,4.17504128 4.29447,5.17904888 4.41636114,6.56128713 L4.3987686,6.36400404 C4.52065973,7.75126861 3.74407501,9.57456653 2.66464737,10.4478898 L2.81921035,10.3222318 C1.73601288,11.1993248 1.72721662,12.6255433 2.79156494,13.5164587 L2.63951518,13.3895441 Z" fill="url(#biz-badges-blue)"></path><svg width="12" height="12" viewBox="0 0 24 24" fill="#fff" x="6" y="6" class="Zi Zi--Check"><path fill-rule="evenodd" d="M21.864 5.347a1.25 1.25 0 0 1 .04 1.767L10.976 18.537a1.35 1.35 0 0 1-1.965-.014L2.584 11.6a1.25 1.25 0 0 1 1.832-1.702l5.598 6.029L20.096 5.386a1.25 1.25 0 0 1 1.767-.04Z" clip-rule="evenodd"></path></svg></svg></span></a><span rel="noopener noreferrer" class="css-2dtzk2">​<img src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/v2-4812630bc27d642f7cafcd6cdeca3d7a.jpg" alt="" class="css-1m3x3v9"></span></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="ztext AuthorInfo-badgeText css-14ur8a8">合肥工业大学 管理科学与工程博士</div></div></div></div></div><button type="button" class="Button FollowButton css-upmq18 FEfUrdfMIKpQDJDqkjte Button--secondary Button--blue E211w_M7Hzs0GMB7BEyA JmYzaky7MEPMFcJDLNMG"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--PlusFill24" fill="currentColor"><path fill-rule="evenodd" d="M13.25 3.25a1.25 1.25 0 1 0-2.5 0v7.5h-7.5a1.25 1.25 0 1 0 0 2.5h7.5v7.5a1.25 1.25 0 1 0 2.5 0v-7.5h7.5a1.25 1.25 0 0 0 0-2.5h-7.5v-7.5Z" clip-rule="evenodd"></path></svg></span> 关注</button><div class="css-124ezq8"></div></div><div class="LabelContainer-wrapper"></div><div class="css-1k5dpte"><span><span class="Voters"><button type="button" class="Button FEfUrdfMIKpQDJDqkjte Button--plain fEPKGkUK5jyc4fUuT0QP">6 人赞同了该回答</button></span></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="6"><meta itemprop="url" content="https://www.zhihu.com/question/592844544/answer/3031186320"><meta itemprop="dateCreated" content="2023-05-16T16:25:37.000Z"><meta itemprop="dateModified" content="2023-05-16T16:25:37.000Z"><meta itemprop="commentCount" content="1"><div class="RichContent RichContent--unescapable"><span><div class="RichContent-inner"><div class="css-376mun"><span class="RichText ztext CopyrightRichText-richText css-1g0fqss" options="[object Object]" itemprop="text"><p data-first-child="" data-pid="4rRDdzsN"><b>本文原文转载自DataLearner官方博客：</b></p><div class="RichText-LinkCardContainer"><a target="_blank" href="https://link.zhihu.com/?target=https%3A//www.datalearner.com/blog/1051684253569005" data-draft-node="block" data-draft-type="link-card" data-text="解决大语言模型的长输入限制：MetaAI发布MegaByte最高支持几百万上下文输入！ | 数据学习者官方网站(Datalearner)" class="LinkCard new css-1wr1m8" data-image="https://pic3.zhimg.com/v2-6630f86a46d914bf5c0e7874a480e1ca_ipico.jpg" data-image-width="372" data-image-height="372"><span class="LinkCard-contents"><span class="LinkCard-title loading" data-text="true"></span><span class="LinkCard-desc loading"></span></span><span class="LinkCard-image LinkCard-image--default"></span></a></div><p data-pid="F690fSK9">尽管OpenAI的ChatGPT很火爆，但是这类大语言模型有一个非常严重的问题就是对输入的内容长度有着很大的限制。例如，ChatGPT-3.5的输入限制是4096个tokens。MetaAI在前几天提交了一个论文，提出了MegaByte方法，几乎可以让模型接受任意长度的限制！</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/50/v2-5cfc5eb464c458cf71d0ef3f145cb802_720w.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="721" data-rawheight="725" data-original-token="v2-a3c1c55d51401d6a2621940443493513" class="origin_image zh-lightbox-thumb" width="721" data-original="https://picx.zhimg.com/v2-5cfc5eb464c458cf71d0ef3f145cb802_r.jpg?source=1940ef5c"/></noscript><div><img src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/v2-5cfc5eb464c458cf71d0ef3f145cb802_1440w.webp" data-caption="" data-size="normal" data-rawwidth="721" data-rawheight="725" data-original-token="v2-a3c1c55d51401d6a2621940443493513" class="origin_image zh-lightbox-thumb lazy" width="721" data-original="https://picx.zhimg.com/v2-5cfc5eb464c458cf71d0ef3f145cb802_r.jpg?source=1940ef5c" data-actualsrc="https://pic1.zhimg.com/50/v2-5cfc5eb464c458cf71d0ef3f145cb802_720w.jpg?source=1940ef5c" height="725" data-lazy-status="ok"></div></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="uI5ND4ck">本文将简单介绍这个方法！论文名称：《MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers》。</p><ul><li data-pid="-8eN7a60"><a href="https://link.zhihu.com/?target=https%3A//www.datalearner.com/blog/1051684253569005%23transformer%25E6%25A8%25A1%25E5%259E%258B%25E7%259A%2584%25E8%25BE%2593%25E5%2585%25A5%25E9%2599%2590%25E5%2588%25B6%25E9%2597%25AE%25E9%25A2%2598" class=" wrap external" target="_blank" rel="nofollow noreferrer">transformer模型的输入限制问题</a></li><li data-pid="xBZ016RW"><a href="https://link.zhihu.com/?target=https%3A//www.datalearner.com/blog/1051684253569005%23MetaByte%25E6%25A0%25B8%25E5%25BF%2583%25E6%2580%259D%25E6%2583%25B3%25E7%25AE%2580%25E4%25BB%258B" class=" wrap external" target="_blank" rel="nofollow noreferrer">MetaByte核心思想简介</a></li><li data-pid="AVTZ3_hR"><a href="https://link.zhihu.com/?target=https%3A//www.datalearner.com/blog/1051684253569005%23MetaByte%25E7%259A%2584%25E6%25B5%258B%25E8%25AF%2595%25E7%25BB%2593%25E6%259E%259C" class=" wrap external" target="_blank" rel="nofollow noreferrer">MetaByte的测试结果</a></li><li data-pid="4RI6ucCn"><a href="https://link.zhihu.com/?target=https%3A//www.datalearner.com/blog/1051684253569005%23MetaByte%25E7%259A%2584%25E6%2580%25BB%25E7%25BB%2593" class=" wrap external" target="_blank" rel="nofollow noreferrer">MetaByte的总结</a></li></ul><h3><b>transformer模型的输入限制问题</b></h3><p data-pid="aNoH64LV">尽管目前的大语言模型在较短序列（几千个tokens以内）的应用场景下有着惊人的效果，但是数百万序列的输入依然是一种刚需，包括音乐、图片、视频、小说、代码等，它们的输入长度通常都是远远超过当前大语言模型的输入限制。主要原因是<span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3031186320%7D" target="_blank">自注意力机制<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>的“成本”随着输入成二次方增长，以及每个位置都需要计算大型的<span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3031186320%7D" target="_blank">前馈网络<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>。</p><h3><b>MetaByte核心思想简介</b></h3><p data-pid="m25c0dNB">为了解决当前transformer模型对输入的限制，MetaAI提出的MetaByte引入了一个概念，称为<span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=patch&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3031186320%7D" target="_blank">patch<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>，将模型的输入序列分割成固定大小的patches，这是一个类似于token的概念，但是显然比token覆盖的范围要宽。然后通过一个全局的模块，建立一个大的<span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=%E8%87%AA%E5%9B%9E%E5%BD%92transformer&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3031186320%7D" target="_blank">自回归transformer<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>，把输入和输出从tokens变成<span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=patches&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3031186320%7D" target="_blank">patches<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>。同时，引入了一个本地的模块，用于每个patch内部的字节的预测，其输入是从全局模块来的上下文patches表示结果，输出是预测下一个patch，这是一个小的<span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3031186320%7D" target="_blank">自回归模型<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>。</p><p data-pid="l6ULrXOC">这个方法与当前transformer模型还有一个最重要的区别是<b>不需要做<span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=tokenization&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3031186320%7D" target="_blank">tokenization<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span></b>！这是一个非常重要的特性。tokenization是当前大语言模型的一种通用做法，尽管<span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=tokens&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3031186320%7D" target="_blank">tokens<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>相比较单词具有更好的一致性，处理起来更加容易。但是，Tokenization意味着LLMs实际上不是完全的端到端。还有一个完全不同的阶段，有自己的训练和推理，并且需要额外的库。它使得引入其他模态变得复杂。</p><p data-pid="Y6GRPNJ9">总结一下，MetaByte相比现有的transformer模型的差异为：</p><ol><li data-pid="JZbUmDpV">大多数优化长序列输入的模型都是关注减少自注意力机制的二次方成本上，而MetaByte将长序列分解为两个较短的序列，并通过优化patch大小将自注意力成本降至O(n^{4/3})，这仍然适用于长序列。</li><li data-pid="OlA0KPca">在GPT-3这样大小的模型上，98%的计算都是在做基于位置的前馈网络，而MetaByte计算的是基于patch的前馈网络。Patch的大小如果是P，那么它的计算效率就是同等规模模型的P倍。</li><li data-pid="9mQsxqoh">Transformers在生成过程中必须按照顺序生成。而MetaByte可以针对patches做并行的生成。例如，15亿参数的MetaByte生成速度比3.5亿参数规模的transformer模型还要快40%！</li></ol><h3><b>MetaByte的总结</b></h3><p data-pid="EYn2-jq3">MetaAI这项工作引起了很多人的注意。个人觉得MetaByte的最大的价值有2个：一个是将大语言模型的上下文限制能力大大拓展，按照论文的描述，可以支持到数百万的字节输入，这几乎是目前常规大语言模型的几百倍，也远超过前几天ClaudeAI-100k和GPT-4-32k，几乎可以解决当下大部分输入限制问题。而另一个最大的优点是没有tokenization，几乎可以完成一个端到端的transformer模型。</p><p data-pid="sEyN6CbZ"><b>这个模型的开源实现和其它信息参考原文：<a href="https://link.zhihu.com/?target=https%3A//www.datalearner.com/blog/1051684253569005" class=" wrap external" target="_blank" rel="nofollow noreferrer">解决大语言模型的长输入限制：MetaAI发布MegaByte最高支持几百万上下文输入！ | 数据学习者官方网站(Datalearner)</a></b></p></span></div></div></span><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/592844544/answer/3031186320"><span data-tooltip="发布于 2023-05-17 00:25" aria-label="发布于 2023-05-17 00:25">发布于 2023-05-17 00:25</span></a></div></div><span></span><div class="ContentItem-actions RichContent-actions"><span><button aria-label="赞同 6 " aria-live="polite" type="button" class="Button VoteButton VoteButton--up FEfUrdfMIKpQDJDqkjte"><span style="display: inline-flex; align-items: center;">​<svg width="10" height="10" viewBox="0 0 24 24" class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor"><path fill-rule="evenodd" d="M13.792 3.681c-.781-1.406-2.803-1.406-3.584 0l-7.79 14.023c-.76 1.367.228 3.046 1.791 3.046h15.582c1.563 0 2.55-1.68 1.791-3.046l-7.79-14.023Z" clip-rule="evenodd"></path></svg></span>赞同 6</button><button aria-label="反对" aria-live="polite" type="button" class="Button VoteButton VoteButton--down FEfUrdfMIKpQDJDqkjte"><span style="display: inline-flex; align-items: center;">​<svg width="10" height="10" viewBox="0 0 24 24" class="Zi Zi--TriangleDown" fill="currentColor"><path fill-rule="evenodd" d="M13.792 20.319c-.781 1.406-2.803 1.406-3.584 0L2.418 6.296c-.76-1.367.228-3.046 1.791-3.046h15.582c1.563 0 2.55 1.68 1.791 3.046l-7.79 14.023Z" clip-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Comment Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>1 条评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover9-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover9-content"><button type="button" class="Button FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Share Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path d="M19.47 1.914a.8.8 0 0 1 1.204.778l-1.872 16.386a.9.9 0 0 1-1.204.743l-4.615-1.692a.7.7 0 0 0-.831.28l-1.927 3.02c-.43.674-1.474.369-1.474-.43v-3.865a.8.8 0 0 1 .179-.504l5.808-7.148a.595.595 0 0 0-.897-.781l-5.93 6.354a1.1 1.1 0 0 1-1.258.252L2.57 13.46a.8.8 0 0 1-.08-1.415l16.98-10.13Z"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Star Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path d="M10.484 3.307c.673-1.168 2.358-1.168 3.032 0l2.377 4.122a.25.25 0 0 0 .165.12l4.655.987c1.319.28 1.84 1.882.937 2.884l-3.186 3.535a.25.25 0 0 0-.063.193l.5 4.733c.142 1.34-1.222 2.33-2.453 1.782l-4.346-1.938a.25.25 0 0 0-.204 0l-4.346 1.938c-1.231.549-2.595-.442-2.453-1.782l.5-4.733a.25.25 0 0 0-.064-.193L2.35 11.42c-.903-1.002-.382-2.604.937-2.884l4.655-.987a.25.25 0 0 0 .164-.12l2.378-4.122Z"></path></svg></span>收藏</button><button aria-live="polite" type="button" class="Button ContentItem-action FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Heart Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path fill-rule="evenodd" d="M12.004 4.934c1.015-.944 2.484-1.618 3.98-1.618 3.48 0 6.53 3.265 6.15 7.614-.11 1.254-.686 2.55-1.458 3.753-.778 1.215-1.79 2.392-2.845 3.419-1.054 1.028-2.168 1.923-3.161 2.566a9.96 9.96 0 0 1-1.41.777c-.418.182-.862.32-1.268.32s-.848-.137-1.267-.317a9.918 9.918 0 0 1-1.407-.771c-.992-.64-2.103-1.53-3.156-2.555-1.052-1.024-2.062-2.2-2.84-3.417-.77-1.208-1.346-2.51-1.456-3.775-.38-4.349 2.67-7.614 6.15-7.614 1.484 0 2.983.673 3.988 1.618Z" clip-rule="evenodd"></path></svg></span>喜欢</button><div class="Popover ContentItem-action"><button aria-label="更多" id="Popover10-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover10-content" type="button" class="Button OptionsButton FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--iconOnly fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY hIwDV_tcL6XN1HprrnAq"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Dots Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path fill-rule="evenodd" d="M5.83 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Zm7.835 0a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Zm6.17 1.665a1.665 1.665 0 1 0 0-3.33 1.665 1.665 0 0 0 0 3.33Z" clip-rule="evenodd"></path></svg></span></button></div><button data-zop-retract-question="true" type="button" class="Button ContentItem-action ContentItem-rightButton FEfUrdfMIKpQDJDqkjte Button--plain fEPKGkUK5jyc4fUuT0QP"><span class="RichContent-collapsedText">收起</span><span style="display: inline-flex; align-items: center;">​<svg width="24" height="24" viewBox="0 0 24 24" class="Zi Zi--ArrowDown ContentItem-arrowIcon is-active" fill="currentColor"><path fill-rule="evenodd" d="M12 13.248 8.22 9.223a.684.684 0 0 0-1.01 0 .796.796 0 0 0 0 1.075l4.15 4.42a.867.867 0 0 0 1.28 0l4.15-4.42a.796.796 0 0 0 0-1.075.684.684 0 0 0-1.01 0L12 13.248Z" clip-rule="evenodd"></path></svg></span></button></div></div><div><div><div class=""></div><div class="ModalLoading-content"><svg width="30" height="30" viewBox="0 0 66 66" xmlns="http://www.w3.org/2000/svg" class="CircleLoadingBar  css-bypjk1" aria-hidden="true"><g><circle class="path" fill="none" stroke-width="6" stroke-linecap="round" cx="33" cy="33" r="30"></circle></g></svg></div></div></div></div></div></div><div class="List-item" tabindex="0"><div><div class="ContentItem AnswerItem" data-za-index="2" data-zop="{&quot;authorName&quot;:&quot;无数据不智能&quot;,&quot;itemId&quot;:3026367037,&quot;title&quot;:&quot;为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="3026367037" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="2" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;3026367037&quot;,&quot;upvote_num&quot;:38,&quot;comment_num&quot;:0,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;592844544&quot;,&quot;author_member_hash_id&quot;:&quot;3bbb23c6e8a40ced4db0b11342aeb257&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><div class="AuthorInfo"><meta itemprop="name" content="无数据不智能"><meta itemprop="image" content="https://pic1.zhimg.com/v2-97ad403a44e44e66b784c604a40288c7_l.jpg?source=1940ef5c"><meta itemprop="url" content="https://www.zhihu.com/people/ha-ha-67-11-64"><meta itemprop="zhihu:followerCount" content="182"><span class="UserLink AuthorInfo-avatarWrapper"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/ha-ha-67-11-64" target="_blank" class="UserLink-link" data-za-detail-view-element_name="User"><img class="Avatar AuthorInfo-avatar css-1oqflzh" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/v2-97ad403a44e44e66b784c604a40288c7_l.jpg" srcset="https://pic1.zhimg.com/v2-97ad403a44e44e66b784c604a40288c7_l.jpg?source=1940ef5c 2x" alt="无数据不智能"></a></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/ha-ha-67-11-64" target="_blank" class="UserLink-link" data-za-detail-view-element_name="User">无数据不智能</a></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"></div></div></div></div><div class="css-124ezq8"></div></div><div class="LabelContainer-wrapper"></div><div class="css-1k5dpte"><span><span class="Voters"><button type="button" class="Button FEfUrdfMIKpQDJDqkjte Button--plain fEPKGkUK5jyc4fUuT0QP">38 人赞同了该回答</button></span></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="38"><meta itemprop="url" content="https://www.zhihu.com/question/592844544/answer/3026367037"><meta itemprop="dateCreated" content="2023-05-13T11:27:14.000Z"><meta itemprop="dateModified" content="2023-05-13T14:13:24.000Z"><meta itemprop="commentCount" content="0"><div class="RichContent RichContent--unescapable"><span><div class="RichContent-inner"><div class="css-376mun"><span class="RichText ztext CopyrightRichText-richText css-1g0fqss" options="[object Object]" itemprop="text"><blockquote data-first-child="" data-pid="NbVR2iEO"> 本文着重于对各种的方法的思想总结，非严谨推导</blockquote><h2><b>循环记忆输入</b></h2><blockquote data-pid="wJjor0Re"> Recurrent Memory Transformer (RMT)</blockquote><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/50/v2-9a9aced7c5b9bc4cd0b7b94bdd699cff_720w.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="1356" data-rawheight="584" data-original-token="v2-aa7dd794ecdf027b15bd94e52047de26" class="origin_image zh-lightbox-thumb" width="1356" data-original="https://picx.zhimg.com/v2-9a9aced7c5b9bc4cd0b7b94bdd699cff_r.jpg?source=1940ef5c"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1356&#39; height=&#39;584&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1356" data-rawheight="584" data-original-token="v2-aa7dd794ecdf027b15bd94e52047de26" class="origin_image zh-lightbox-thumb lazy" width="1356" data-original="https://picx.zhimg.com/v2-9a9aced7c5b9bc4cd0b7b94bdd699cff_r.jpg?source=1940ef5c" data-actualsrc="https://picx.zhimg.com/50/v2-9a9aced7c5b9bc4cd0b7b94bdd699cff_720w.jpg?source=1940ef5c"></div></figure><p data-pid="wOHKO9i-">总体思想：将长文本分段之后得到嵌入向量与记忆向量拼接，得到新的记忆向量之后与下一段再循环输入transformer。</p><p data-pid="yHUwzWbB">注意：此论文实验结果在bert-base-cased（encoder-only上进行实验）</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/50/v2-40c7dfcffee6506a0e422e5050d681a3_720w.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="1030" data-rawheight="328" data-original-token="v2-ce874ec5804ee94ca78cbb292a655c1b" class="origin_image zh-lightbox-thumb" width="1030" data-original="https://picx.zhimg.com/v2-40c7dfcffee6506a0e422e5050d681a3_r.jpg?source=1940ef5c"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1030&#39; height=&#39;328&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1030" data-rawheight="328" data-original-token="v2-ce874ec5804ee94ca78cbb292a655c1b" class="origin_image zh-lightbox-thumb lazy" width="1030" data-original="https://picx.zhimg.com/v2-40c7dfcffee6506a0e422e5050d681a3_r.jpg?source=1940ef5c" data-actualsrc="https://picx.zhimg.com/50/v2-40c7dfcffee6506a0e422e5050d681a3_720w.jpg?source=1940ef5c"></div></figure><p data-pid="h23f0ro4">CoLT5达到64K，GPT-4达到32K长度，而RMT在实验结果中长度加到4096个分段2048000词汇，效果依然强劲。</p><h2><b>用提示词</b></h2><blockquote data-pid="-qoUtuZR"> Self-Controlled Memory (SCM)</blockquote><figure data-size="normal"><noscript><img src="https://pica.zhimg.com/50/v2-90b073ca331935a81fd3fe9e2e8afb0f_720w.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="1459" data-rawheight="679" data-original-token="v2-38a1ad8e80c3d00f020152f7ae597cc7" class="origin_image zh-lightbox-thumb" width="1459" data-original="https://picx.zhimg.com/v2-90b073ca331935a81fd3fe9e2e8afb0f_r.jpg?source=1940ef5c"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1459&#39; height=&#39;679&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1459" data-rawheight="679" data-original-token="v2-38a1ad8e80c3d00f020152f7ae597cc7" class="origin_image zh-lightbox-thumb lazy" width="1459" data-original="https://picx.zhimg.com/v2-90b073ca331935a81fd3fe9e2e8afb0f_r.jpg?source=1940ef5c" data-actualsrc="https://pica.zhimg.com/50/v2-90b073ca331935a81fd3fe9e2e8afb0f_720w.jpg?source=1940ef5c"></div></figure><p data-pid="l1E7ZSqn">如上图所示，此方法号称可以将输入延申至无限长，具体流程为：</p><ol><li data-pid="YdD6EmAC">用户输入<br> </li><li data-pid="8fRBei0O">判断是否需要从历史会话中获得记忆，提示词如下：<br> 给定一个<span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=%E7%94%A8%E6%88%B7%E6%8C%87%E4%BB%A4&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3026367037%7D" target="_blank">用户指令<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>，判断执行该指令是否需要历史信<br>息或者上文的信息，或者需要回忆对话内容，只需要<br>回答是(A)或者否(B)，不需要解释信息：<br>指令：[用户输入]<br><br> </li><li data-pid="-3O6Kow-">如果需要获取记忆，通过相关性（<span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E5%BA%A6&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3026367037%7D" target="_blank">余弦相似度<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>）、近期性分数相加对历史记忆进行排序<br> </li><li data-pid="1d7dfcHw">将记忆摘要<br> 以下是用户和人工智能助手的一段对话，请分<br>别用一句话写出用户摘要、助手摘要，分段列<br>出，要求尽可能保留用户问题和助手回答的关<br>键信息。<br>对话内容：<br>用户：[用户输入]<br>助手：[系统回复]<br>摘要：<br><br> </li><li data-pid="hiA76f49">将记忆和输入拼接输入模型<br> 以下是用户和人工智能助手的对话，请根据历史<br>对话内容，回答用户当前问题：<br>相关历史对话：<br>[历史轮对话内容]<br>上一轮对话：<br>[上一轮对话内容]<br><i>###</i><br>用户：[用户问题]<br>助手：<br><br> </li><li data-pid="QfkTcuwx">回复<br> </li></ol><p data-pid="WEtcAQ7j">注意：此论文中只进行了定性分析，没有<span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=%E5%AE%9A%E9%87%8F%E5%AE%9E%E9%AA%8C&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3026367037%7D" target="_blank">定量实验<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>。以下是效果图：</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/50/v2-c23b706d65efa0d94bd065da95f08b99_720w.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="726" data-rawheight="602" data-original-token="v2-8cbb05dac851be081593a8e99d7a6303" class="origin_image zh-lightbox-thumb" width="726" data-original="https://pic1.zhimg.com/v2-c23b706d65efa0d94bd065da95f08b99_r.jpg?source=1940ef5c"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;726&#39; height=&#39;602&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="726" data-rawheight="602" data-original-token="v2-8cbb05dac851be081593a8e99d7a6303" class="origin_image zh-lightbox-thumb lazy" width="726" data-original="https://pic1.zhimg.com/v2-c23b706d65efa0d94bd065da95f08b99_r.jpg?source=1940ef5c" data-actualsrc="https://picx.zhimg.com/50/v2-c23b706d65efa0d94bd065da95f08b99_720w.jpg?source=1940ef5c"></div></figure><h2><b>词汇压缩</b></h2><blockquote data-pid="uLiUsnyL"> VIP-token centric compression (Vcc)</blockquote><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/50/v2-3d9db54d4962c0e9d61213f067e52e86_720w.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="1021" data-rawheight="792" data-original-token="v2-55d599c3eae34148f84e84b2882326a6" class="origin_image zh-lightbox-thumb" width="1021" data-original="https://picx.zhimg.com/v2-3d9db54d4962c0e9d61213f067e52e86_r.jpg?source=1940ef5c"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1021&#39; height=&#39;792&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1021" data-rawheight="792" data-original-token="v2-55d599c3eae34148f84e84b2882326a6" class="origin_image zh-lightbox-thumb lazy" width="1021" data-original="https://picx.zhimg.com/v2-3d9db54d4962c0e9d61213f067e52e86_r.jpg?source=1940ef5c" data-actualsrc="https://picx.zhimg.com/50/v2-3d9db54d4962c0e9d61213f067e52e86_720w.jpg?source=1940ef5c"></div></figure><p data-pid="nzNiAlVZ">该方法使得模型输入延申至128K，并在Encoder-Only、Encoder-Decoder两种模型架构上都进行了实验。</p><p data-pid="MR1ZOX8A">一句话描述思想：使模型输入长度独立于<span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=%E6%96%87%E6%9C%AC%E9%95%BF%E5%BA%A6&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3026367037%7D" target="_blank">文本长度<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>。</p><p data-pid="HcyRoBfC">具体一点：</p><ol><li data-pid="-mTY9XX-">将当前问句视为<span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=vip-token&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3026367037%7D" target="_blank">vip-token<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span></li><li data-pid="QUfaVCPO">利用当前问句与历史记忆的关系，压缩历史记忆到模型输入长度，无论历史记忆有多长</li><li data-pid="3CBgN-Do">transformer层输出之后再进行解压缩</li></ol><p data-pid="sU1R1jMJ">Encoder-Only架构表现：</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/50/v2-5b27ff419149fc1a0a3ce33911dfb657_720w.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="668" data-rawheight="288" data-original-token="v2-7592fc65184d68d29e93b9a3f43ac708" class="origin_image zh-lightbox-thumb" width="668" data-original="https://picx.zhimg.com/v2-5b27ff419149fc1a0a3ce33911dfb657_r.jpg?source=1940ef5c"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;668&#39; height=&#39;288&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="668" data-rawheight="288" data-original-token="v2-7592fc65184d68d29e93b9a3f43ac708" class="origin_image zh-lightbox-thumb lazy" width="668" data-original="https://picx.zhimg.com/v2-5b27ff419149fc1a0a3ce33911dfb657_r.jpg?source=1940ef5c" data-actualsrc="https://picx.zhimg.com/50/v2-5b27ff419149fc1a0a3ce33911dfb657_720w.jpg?source=1940ef5c"></div></figure><p data-pid="2v0xozWG">Encoder-Decoder表现：</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/50/v2-deb7498d320aa9dc209409bf09718ff9_720w.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="1033" data-rawheight="769" data-original-token="v2-e935866876c479163317d26477ad1d1d" class="origin_image zh-lightbox-thumb" width="1033" data-original="https://pic1.zhimg.com/v2-deb7498d320aa9dc209409bf09718ff9_r.jpg?source=1940ef5c"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1033&#39; height=&#39;769&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1033" data-rawheight="769" data-original-token="v2-e935866876c479163317d26477ad1d1d" class="origin_image zh-lightbox-thumb lazy" width="1033" data-original="https://pic1.zhimg.com/v2-deb7498d320aa9dc209409bf09718ff9_r.jpg?source=1940ef5c" data-actualsrc="https://pic1.zhimg.com/50/v2-deb7498d320aa9dc209409bf09718ff9_720w.jpg?source=1940ef5c"></div></figure><h2><b>检索+交叉注意力</b></h2><blockquote data-pid="W1Gk0cDd"> Unlimited Length Input Transformers (Unlimiformer)</blockquote><figure data-size="normal"><noscript><img src="https://pica.zhimg.com/50/v2-917b060abda753ad352f98bcce863477_720w.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="1260" data-rawheight="499" data-original-token="v2-bd2d39e0ce9ac2946ed55f92531144aa" class="origin_image zh-lightbox-thumb" width="1260" data-original="https://pica.zhimg.com/v2-917b060abda753ad352f98bcce863477_r.jpg?source=1940ef5c"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1260&#39; height=&#39;499&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1260" data-rawheight="499" data-original-token="v2-bd2d39e0ce9ac2946ed55f92531144aa" class="origin_image zh-lightbox-thumb lazy" width="1260" data-original="https://pica.zhimg.com/v2-917b060abda753ad352f98bcce863477_r.jpg?source=1940ef5c" data-actualsrc="https://pica.zhimg.com/50/v2-917b060abda753ad352f98bcce863477_720w.jpg?source=1940ef5c"></div></figure><p data-pid="12MRVfrL">此方法只试用于Encoder-Decoder架构，其也称可以将输入长度延申至无限长。</p><p data-pid="YAbtthMf">思路如下：</p><ol><li data-pid="dK1Y7UOu">将长文本分成多个部分，将每一段进行编码</li><li data-pid="j4JAWeab">利用query KNN检索长文本topN</li><li data-pid="9MXan1XI"><span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=%E8%A7%A3%E7%A0%81%E5%99%A8&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3026367037%7D" target="_blank">解码器<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>对相关段落编码后的隐藏状态进行交叉注意力</li><li data-pid="FOXkU74x">得到输出</li></ol><p data-pid="oAaAfNX-">可以看到此方法在长文本摘要任务上都取得了优异的结果</p><figure data-size="normal"><noscript><img src="https://pica.zhimg.com/50/v2-d67e99e2c7ff3fa38d5a6b6ad1606509_720w.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="986" data-rawheight="311" data-original-token="v2-7d3c6a5d15d36f0ac56989c46f6d5cd3" class="origin_image zh-lightbox-thumb" width="986" data-original="https://pic1.zhimg.com/v2-d67e99e2c7ff3fa38d5a6b6ad1606509_r.jpg?source=1940ef5c"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;986&#39; height=&#39;311&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="986" data-rawheight="311" data-original-token="v2-7d3c6a5d15d36f0ac56989c46f6d5cd3" class="origin_image zh-lightbox-thumb lazy" width="986" data-original="https://pic1.zhimg.com/v2-d67e99e2c7ff3fa38d5a6b6ad1606509_r.jpg?source=1940ef5c" data-actualsrc="https://pica.zhimg.com/50/v2-d67e99e2c7ff3fa38d5a6b6ad1606509_720w.jpg?source=1940ef5c"></div></figure><h2><b><span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=%E7%B4%AF%E5%8A%A0&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3026367037%7D" target="_blank">累加<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span></b></h2><blockquote data-pid="vmwcy2Eh"> ALiBi（attention with linear biases），输出累加</blockquote><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/50/v2-c009872174bc00e8bf1e6e53c02ed4b7_720w.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="514" data-rawheight="247" data-original-token="v2-d7926e6a550d571eac942d9d370ed0c3" class="origin_image zh-lightbox-thumb" width="514" data-original="https://pic1.zhimg.com/v2-c009872174bc00e8bf1e6e53c02ed4b7_r.jpg?source=1940ef5c"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;514&#39; height=&#39;247&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="514" data-rawheight="247" data-original-token="v2-d7926e6a550d571eac942d9d370ed0c3" class="origin_image zh-lightbox-thumb lazy" width="514" data-original="https://pic1.zhimg.com/v2-c009872174bc00e8bf1e6e53c02ed4b7_r.jpg?source=1940ef5c" data-actualsrc="https://picx.zhimg.com/50/v2-c009872174bc00e8bf1e6e53c02ed4b7_720w.jpg?source=1940ef5c"></div></figure><p data-pid="iY6jIkvx">简单介绍一下ALiBi:</p><ol><li data-pid="1FT93yyi">不再输入层保留位置向量</li><li data-pid="aDRLBC7D">而在每层注入线性的偏移量，注意力分数从：</li></ol><p data-pid="-B99BsuD"><span class="ztext-math" data-eeimg="1" data-tex="\operatorname { softmax}  ( q_iK ^ { T } ) \\"><span></span><span><script type="math/tex;mode=display">\operatorname { softmax}  ( q_iK ^ { T } ) \\</script><span class="tex2jax_ignore math-holder">\operatorname { softmax}  ( q_iK ^ { T } ) \\</span></span></span></p><p data-pid="JVF5ZRyO">变成了：</p><p data-pid="-AUml9rD"><span class="ztext-math" data-eeimg="1" data-tex="softmax(q_{i}K^{T}+m \cdot \left[ -(i-1), \ldots ,-2,-1,0 \right]) \\"><span></span><span><script type="math/tex;mode=display">softmax(q_{i}K^{T}+m \cdot \left[ -(i-1), \ldots ,-2,-1,0 \right]) \\</script><span class="tex2jax_ignore math-holder">softmax(q_{i}K^{T}+m \cdot \left[ -(i-1), \ldots ,-2,-1,0 \right]) \\</span></span></span></p><p data-pid="euwTHgB6">可以看到ALiBi比Sinusoidal、Rotary、T5 Bias在长距离输入上效果都要好得多。</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/50/v2-02ef9c04064fd44fe7b9b5e101c02e9f_720w.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="834" data-rawheight="339" data-original-token="v2-b93c26f800a298e8af934395b50d5fee" class="origin_image zh-lightbox-thumb" width="834" data-original="https://pic1.zhimg.com/v2-02ef9c04064fd44fe7b9b5e101c02e9f_r.jpg?source=1940ef5c"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;834&#39; height=&#39;339&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="834" data-rawheight="339" data-original-token="v2-b93c26f800a298e8af934395b50d5fee" class="origin_image zh-lightbox-thumb lazy" width="834" data-original="https://pic1.zhimg.com/v2-02ef9c04064fd44fe7b9b5e101c02e9f_r.jpg?source=1940ef5c" data-actualsrc="https://picx.zhimg.com/50/v2-02ef9c04064fd44fe7b9b5e101c02e9f_720w.jpg?source=1940ef5c"></div></figure><p data-pid="REwr0jAQ">mosaicml/mpt-7b模型利用ALiBi将输入长度扩展至了84k，核心的思想为一下几行代码：</p><div class="highlight"><pre><code class="language-text">all_hidden_states = () if output_hidden_states else None
for (b_idx, block) in enumerate(self.blocks):
    if output_hidden_states:
        assert all_hidden_states is not None
        all_hidden_states = all_hidden_states + (x,)
    past_key_value = past_key_values[b_idx] if past_key_values is not None else None
    (x, past_key_value) = block(x, <span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=past_key_value%3Dpast_key_value&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3026367037%7D" target="_blank">past_key_value=past_key_value<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>, attn_bias=attn_bias, <span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=attention_mask%3Dattention_mask&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3026367037%7D" target="_blank">attention_mask=attention_mask<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>, is_causal=self.is_causal)
    if past_key_values is not None:
        past_key_values[b_idx] = past_key_value
</code></pre></div><p data-pid="FraY-rXa">即MPT会对上次得到隐藏状态与本次的输入进行相加。</p><h2><b>引用</b></h2><ol><li data-pid="uOmolzDn">Vcc: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens</li><li data-pid="tk6PlAF7">Scaling Transformer to 1M tokens and beyond with RMT</li><li data-pid="--4wkXNq">Unleashing Infinite-Length Input Capacity for Large-scale Language Models with Self-Controlled Memory System</li><li data-pid="rz4jAp53">Unlimiformer: Long-Range Transformers with Unlimited Length Input</li><li data-pid="DR0cgxZF"><b><a href="https://link.zhihu.com/?target=https%3A//huggingface.co/mosaicml/mpt-7b" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">https://</span><span class="visible">huggingface.co/mosaicml</span><span class="invisible">/mpt-7b</span><span class="ellipsis"></span></a></b></li></ol></span></div></div></span><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/592844544/answer/3026367037"><span data-tooltip="发布于 2023-05-13 19:27" aria-label="发布于 2023-05-13 19:27">编辑于 2023-05-13 22:13</span></a></div><div class="Reward"><div><div class="Reward-tagline">真诚赞赏，手留余香</div><button class="Reward-rewardBtn">赞赏</button></div><div class="Reward-countZero">还没有人赞赏，快来当第一个赞赏的人吧！</div></div></div><span></span><div class="ContentItem-actions RichContent-actions"><span><button aria-label="赞同 38 " aria-live="polite" type="button" class="Button VoteButton VoteButton--up FEfUrdfMIKpQDJDqkjte"><span style="display: inline-flex; align-items: center;">​<svg width="10" height="10" viewBox="0 0 24 24" class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor"><path fill-rule="evenodd" d="M13.792 3.681c-.781-1.406-2.803-1.406-3.584 0l-7.79 14.023c-.76 1.367.228 3.046 1.791 3.046h15.582c1.563 0 2.55-1.68 1.791-3.046l-7.79-14.023Z" clip-rule="evenodd"></path></svg></span>赞同 38</button><button aria-label="反对" aria-live="polite" type="button" class="Button VoteButton VoteButton--down FEfUrdfMIKpQDJDqkjte"><span style="display: inline-flex; align-items: center;">​<svg width="10" height="10" viewBox="0 0 24 24" class="Zi Zi--TriangleDown" fill="currentColor"><path fill-rule="evenodd" d="M13.792 20.319c-.781 1.406-2.803 1.406-3.584 0L2.418 6.296c-.76-1.367.228-3.046 1.791-3.046h15.582c1.563 0 2.55 1.68 1.791 3.046l-7.79 14.023Z" clip-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Comment Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>添加评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover13-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover13-content"><button type="button" class="Button FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Share Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path d="M19.47 1.914a.8.8 0 0 1 1.204.778l-1.872 16.386a.9.9 0 0 1-1.204.743l-4.615-1.692a.7.7 0 0 0-.831.28l-1.927 3.02c-.43.674-1.474.369-1.474-.43v-3.865a.8.8 0 0 1 .179-.504l5.808-7.148a.595.595 0 0 0-.897-.781l-5.93 6.354a1.1 1.1 0 0 1-1.258.252L2.57 13.46a.8.8 0 0 1-.08-1.415l16.98-10.13Z"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Star Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path d="M10.484 3.307c.673-1.168 2.358-1.168 3.032 0l2.377 4.122a.25.25 0 0 0 .165.12l4.655.987c1.319.28 1.84 1.882.937 2.884l-3.186 3.535a.25.25 0 0 0-.063.193l.5 4.733c.142 1.34-1.222 2.33-2.453 1.782l-4.346-1.938a.25.25 0 0 0-.204 0l-4.346 1.938c-1.231.549-2.595-.442-2.453-1.782l.5-4.733a.25.25 0 0 0-.064-.193L2.35 11.42c-.903-1.002-.382-2.604.937-2.884l4.655-.987a.25.25 0 0 0 .164-.12l2.378-4.122Z"></path></svg></span>收藏</button><button aria-live="polite" type="button" class="Button ContentItem-action FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Heart Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path fill-rule="evenodd" d="M12.004 4.934c1.015-.944 2.484-1.618 3.98-1.618 3.48 0 6.53 3.265 6.15 7.614-.11 1.254-.686 2.55-1.458 3.753-.778 1.215-1.79 2.392-2.845 3.419-1.054 1.028-2.168 1.923-3.161 2.566a9.96 9.96 0 0 1-1.41.777c-.418.182-.862.32-1.268.32s-.848-.137-1.267-.317a9.918 9.918 0 0 1-1.407-.771c-.992-.64-2.103-1.53-3.156-2.555-1.052-1.024-2.062-2.2-2.84-3.417-.77-1.208-1.346-2.51-1.456-3.775-.38-4.349 2.67-7.614 6.15-7.614 1.484 0 2.983.673 3.988 1.618Z" clip-rule="evenodd"></path></svg></span>喜欢</button><div class="Popover ContentItem-action"><button aria-label="更多" id="Popover14-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover14-content" type="button" class="Button OptionsButton FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--iconOnly fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY hIwDV_tcL6XN1HprrnAq"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Dots Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path fill-rule="evenodd" d="M5.83 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Zm7.835 0a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Zm6.17 1.665a1.665 1.665 0 1 0 0-3.33 1.665 1.665 0 0 0 0 3.33Z" clip-rule="evenodd"></path></svg></span></button></div><button data-zop-retract-question="true" type="button" class="Button ContentItem-action ContentItem-rightButton FEfUrdfMIKpQDJDqkjte Button--plain fEPKGkUK5jyc4fUuT0QP"><span class="RichContent-collapsedText">收起</span><span style="display: inline-flex; align-items: center;">​<svg width="24" height="24" viewBox="0 0 24 24" class="Zi Zi--ArrowDown ContentItem-arrowIcon is-active" fill="currentColor"><path fill-rule="evenodd" d="M12 13.248 8.22 9.223a.684.684 0 0 0-1.01 0 .796.796 0 0 0 0 1.075l4.15 4.42a.867.867 0 0 0 1.28 0l4.15-4.42a.796.796 0 0 0 0-1.075.684.684 0 0 0-1.01 0L12 13.248Z" clip-rule="evenodd"></path></svg></span></button></div></div><div><div><div class=""></div><div class="ModalLoading-content"><svg width="30" height="30" viewBox="0 0 66 66" xmlns="http://www.w3.org/2000/svg" class="CircleLoadingBar  css-bypjk1" aria-hidden="true"><g><circle class="path" fill="none" stroke-width="6" stroke-linecap="round" cx="33" cy="33" r="30"></circle></g></svg></div></div></div></div></div></div><div class="List-item" tabindex="0"><div><div class="ContentItem AnswerItem" data-za-index="3" data-zop="{&quot;authorName&quot;:&quot;互联网知识的力量&quot;,&quot;itemId&quot;:3094076650,&quot;title&quot;:&quot;为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="3094076650" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="3" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;3094076650&quot;,&quot;upvote_num&quot;:0,&quot;comment_num&quot;:0,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;592844544&quot;,&quot;author_member_hash_id&quot;:&quot;ae26aafcdd3defe7e4c32d0fc3897e54&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><div class="AuthorInfo"><meta itemprop="name" content="互联网知识的力量"><meta itemprop="image" content="https://picx.zhimg.com/v2-9e1c1a0ab7e4f6e5a98490e8a1a6bb87_l.jpg?source=1940ef5c"><meta itemprop="url" content="https://www.zhihu.com/people/wjhfsc"><meta itemprop="zhihu:followerCount" content="3227"><span class="UserLink AuthorInfo-avatarWrapper"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/wjhfsc" target="_blank" class="UserLink-link" data-za-detail-view-element_name="User"><img class="Avatar AuthorInfo-avatar css-1oqflzh" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/v2-9e1c1a0ab7e4f6e5a98490e8a1a6bb87_l.jpg" srcset="https://picx.zhimg.com/v2-9e1c1a0ab7e4f6e5a98490e8a1a6bb87_l.jpg?source=1940ef5c 2x" alt="互联网知识的力量"></a></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/wjhfsc" target="_blank" class="UserLink-link" data-za-detail-view-element_name="User">互联网知识的力量</a></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="ztext AuthorInfo-badgeText css-14ur8a8">生活不易，请坚持走下去！</div></div></div></div></div><div class="css-124ezq8"></div></div><div class="LabelContainer-wrapper"></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="0"><meta itemprop="url" content="https://www.zhihu.com/question/592844544/answer/3094076650"><meta itemprop="dateCreated" content="2023-06-28T06:01:52.000Z"><meta itemprop="dateModified" content="2023-06-28T06:01:52.000Z"><meta itemprop="commentCount" content="0"><div class="RichContent RichContent--unescapable"><span><div class="RichContent-inner"><div class="css-376mun"><span class="RichText ztext CopyrightRichText-richText css-1g0fqss" options="[object Object]" itemprop="text"><p data-first-child="" data-pid="HlEAtF57">GPT模型输入token数量的限制并不完全是技术问题，而是由于多个因素综合考虑所做出的权衡决策。下面我会解释一些原因：</p><ol><li data-pid="Ob70lfZT">内存和计算资源限制：较长的输入序列需要更多的内存和计算资源进行处理。大模型相对较大的参数量和计算需求意味着我们需要在处理上做一些妥协，以平衡性能和<span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3094076650%7D" target="_blank">可扩展性<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>。</li><li data-pid="OrJj1OMk">模型复杂性：增加输入序列的长度会增加模型的复杂性。长序列可能导致梯度消失或梯度爆炸等问题，从而降低模型的训练效果和性能。</li><li data-pid="gyGXmSq6"><span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3094076650%7D" target="_blank">训练数据<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>：通常情况下，训练数据中的大多数样本包含的信息主要集中在较短的上下文中。<span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=%E9%95%BF%E5%BA%8F%E5%88%97&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3094076650%7D" target="_blank">长序列<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>的训练数据相对较少，因此将其作为输入可能会导致信息冗余和效率低下。</li><li data-pid="4LJq_obf">推理时间：处理更长的输入序列会增加推理时间。对于在线应用和实时交互，较长的推理时间可能无法满足用户的响应需求。</li></ol><p data-pid="ZX8bWkVZ">综上所述，将GPT模型输入token数量限制在几万的范围内是考虑到了内存、计算资源限制、模型复杂性、训练数据和推理时间等多个因素所做的权衡决策。这样的限制可以帮助平衡性能、效率和模型的应用范围。然而，对于特定任务或需求，也可以通过拆分较长的输入<span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=%E5%BA%8F%E5%88%97&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3094076650%7D" target="_blank">序列<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>或使用更大的模型来尝试克服这些限制。</p><figure data-size="normal"><noscript><img src="https://pica.zhimg.com/50/v2-c4c548848b91d08a499a272b58166e44_720w.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="848" data-rawheight="848" data-original-token="v2-c4c548848b91d08a499a272b58166e44" class="origin_image zh-lightbox-thumb" width="848" data-original="https://picx.zhimg.com/v2-c4c548848b91d08a499a272b58166e44_r.jpg?source=1940ef5c"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;848&#39; height=&#39;848&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="848" data-rawheight="848" data-original-token="v2-c4c548848b91d08a499a272b58166e44" class="origin_image zh-lightbox-thumb lazy" width="848" data-original="https://picx.zhimg.com/v2-c4c548848b91d08a499a272b58166e44_r.jpg?source=1940ef5c" data-actualsrc="https://pica.zhimg.com/50/v2-c4c548848b91d08a499a272b58166e44_720w.jpg?source=1940ef5c"></div></figure><p data-pid="c5vNncl9">有大佬撰写了一个原创<b>有所有权版‬认证以在及‬北国京‬信<span class="nolink">公证处</span>进行公证</b>的<b>26万字</b>的实时在线文档<b>《玩赚：108种chatgpt创业变现和创业思维手册》</b>，目前<b>国内有很多行业大佬就是靠这个手册启蒙的，所以它很适合刚接触chatgpt的朋友！</b></p><figure data-size="normal"><noscript><img src="https://pica.zhimg.com/50/v2-b63bfc11603aa53b5f11d750c4cbf90a_720w.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="1200" data-rawheight="4698" data-original-token="v2-b63bfc11603aa53b5f11d750c4cbf90a" class="origin_image zh-lightbox-thumb" width="1200" data-original="https://picx.zhimg.com/v2-b63bfc11603aa53b5f11d750c4cbf90a_r.jpg?source=1940ef5c"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;4698&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1200" data-rawheight="4698" data-original-token="v2-b63bfc11603aa53b5f11d750c4cbf90a" class="origin_image zh-lightbox-thumb lazy" width="1200" data-original="https://picx.zhimg.com/v2-b63bfc11603aa53b5f11d750c4cbf90a_r.jpg?source=1940ef5c" data-actualsrc="https://pica.zhimg.com/50/v2-b63bfc11603aa53b5f11d750c4cbf90a_720w.jpg?source=1940ef5c"></div></figure><p data-pid="raX5Gfjo">哪怕你是小白，<b>你也可以不用注册、不用登录、不用科学上网、不限时长、纯免费无限制畅玩chatgpt</b>，更有大量的<b>精准搜索指令</b>供你在短时间内学会<b>让chatgpt来提升你的工作技能</b>，<b>让你一个人轻松干10个人的活！</b>更有不少利用chatgpt<b>创业和变现的小项目</b>供你参考。</p><p data-pid="13zZ5tfF">如果你已经是<b>精通chatgpt使用的大佬</b>，或者你更<b>侧重于利用chatgpt来创业和变现</b>，那么这个<b>26万字</b>的<b>《玩赚：108种chatgpt创业变现和创业思维手册》</b>更适合你！它包含了《<span class="nolink">chatgpt无障碍使用手册</span>》的内容，有108种chatgpt变现和创业的项目，每个项目都包含了<b><span class="nolink">项目名称</span>、项目概述、适合人群、项目变现方式、操作步骤提示、<span class="nolink">网络宣传</span>渠道、网络宣传文案参考、扩展思路、注意事项、<span class="nolink">chatgpt指令</span>参考（截图）等十个方面进行了阐述。</b></p><p data-pid="sNM-n2FE"><b>更有不用科学上网、不用注册、不用账号和密码，更不限时长就能纯免费畅玩<span class="nolink">chatgpt4.0</span>的<span class="nolink">镜像站</span>推荐，而且还是联网的！（稀缺资源）。</b>具体的完整介绍可直接查看下面的内容：</p><div class="RichText-LinkCardContainer"><a target="_blank" href="https://link.zhihu.com/?target=https%3A//muyin.360.cn/pc/shop.html%3Fshop_id%3DvyzixHVqWq" data-draft-node="block" data-draft-type="link-card" data-text="《玩赚：108种chatgpt创业变现和创业思维手册》" class="LinkCard new css-1wr1m8"><span class="LinkCard-contents"><span class="LinkCard-title loading" data-text="true"></span><span class="LinkCard-desc loading"></span></span><span class="LinkCard-image LinkCard-image--default"></span></a></div><p></p></span></div></div></span><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/592844544/answer/3094076650"><span data-tooltip="发布于 2023-06-28 14:01" aria-label="发布于 2023-06-28 14:01">发布于 2023-06-28 14:01</span></a></div><div class="Reward"><div><div class="Reward-tagline">真诚赞赏，手留余香</div><button class="Reward-rewardBtn">赞赏</button></div><div class="Reward-countZero">还没有人赞赏，快来当第一个赞赏的人吧！</div></div></div><span></span><div class="ContentItem-actions RichContent-actions"><span><button aria-label="赞同 0 " aria-live="polite" type="button" class="Button VoteButton VoteButton--up FEfUrdfMIKpQDJDqkjte"><span style="display: inline-flex; align-items: center;">​<svg width="10" height="10" viewBox="0 0 24 24" class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor"><path fill-rule="evenodd" d="M13.792 3.681c-.781-1.406-2.803-1.406-3.584 0l-7.79 14.023c-.76 1.367.228 3.046 1.791 3.046h15.582c1.563 0 2.55-1.68 1.791-3.046l-7.79-14.023Z" clip-rule="evenodd"></path></svg></span>赞同</button><button aria-label="反对" aria-live="polite" type="button" class="Button VoteButton VoteButton--down FEfUrdfMIKpQDJDqkjte"><span style="display: inline-flex; align-items: center;">​<svg width="10" height="10" viewBox="0 0 24 24" class="Zi Zi--TriangleDown" fill="currentColor"><path fill-rule="evenodd" d="M13.792 20.319c-.781 1.406-2.803 1.406-3.584 0L2.418 6.296c-.76-1.367.228-3.046 1.791-3.046h15.582c1.563 0 2.55 1.68 1.791 3.046l-7.79 14.023Z" clip-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Comment Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>添加评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover15-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover15-content"><button type="button" class="Button FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Share Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path d="M19.47 1.914a.8.8 0 0 1 1.204.778l-1.872 16.386a.9.9 0 0 1-1.204.743l-4.615-1.692a.7.7 0 0 0-.831.28l-1.927 3.02c-.43.674-1.474.369-1.474-.43v-3.865a.8.8 0 0 1 .179-.504l5.808-7.148a.595.595 0 0 0-.897-.781l-5.93 6.354a1.1 1.1 0 0 1-1.258.252L2.57 13.46a.8.8 0 0 1-.08-1.415l16.98-10.13Z"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Star Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path d="M10.484 3.307c.673-1.168 2.358-1.168 3.032 0l2.377 4.122a.25.25 0 0 0 .165.12l4.655.987c1.319.28 1.84 1.882.937 2.884l-3.186 3.535a.25.25 0 0 0-.063.193l.5 4.733c.142 1.34-1.222 2.33-2.453 1.782l-4.346-1.938a.25.25 0 0 0-.204 0l-4.346 1.938c-1.231.549-2.595-.442-2.453-1.782l.5-4.733a.25.25 0 0 0-.064-.193L2.35 11.42c-.903-1.002-.382-2.604.937-2.884l4.655-.987a.25.25 0 0 0 .164-.12l2.378-4.122Z"></path></svg></span>收藏</button><button aria-live="polite" type="button" class="Button ContentItem-action FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Heart Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path fill-rule="evenodd" d="M12.004 4.934c1.015-.944 2.484-1.618 3.98-1.618 3.48 0 6.53 3.265 6.15 7.614-.11 1.254-.686 2.55-1.458 3.753-.778 1.215-1.79 2.392-2.845 3.419-1.054 1.028-2.168 1.923-3.161 2.566a9.96 9.96 0 0 1-1.41.777c-.418.182-.862.32-1.268.32s-.848-.137-1.267-.317a9.918 9.918 0 0 1-1.407-.771c-.992-.64-2.103-1.53-3.156-2.555-1.052-1.024-2.062-2.2-2.84-3.417-.77-1.208-1.346-2.51-1.456-3.775-.38-4.349 2.67-7.614 6.15-7.614 1.484 0 2.983.673 3.988 1.618Z" clip-rule="evenodd"></path></svg></span>喜欢</button><div class="Popover ContentItem-action"><button aria-label="更多" id="Popover16-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover16-content" type="button" class="Button OptionsButton FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--iconOnly fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY hIwDV_tcL6XN1HprrnAq"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Dots Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path fill-rule="evenodd" d="M5.83 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Zm7.835 0a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Zm6.17 1.665a1.665 1.665 0 1 0 0-3.33 1.665 1.665 0 0 0 0 3.33Z" clip-rule="evenodd"></path></svg></span></button></div><button data-zop-retract-question="true" type="button" class="Button ContentItem-action ContentItem-rightButton FEfUrdfMIKpQDJDqkjte Button--plain fEPKGkUK5jyc4fUuT0QP"><span class="RichContent-collapsedText">收起</span><span style="display: inline-flex; align-items: center;">​<svg width="24" height="24" viewBox="0 0 24 24" class="Zi Zi--ArrowDown ContentItem-arrowIcon is-active" fill="currentColor"><path fill-rule="evenodd" d="M12 13.248 8.22 9.223a.684.684 0 0 0-1.01 0 .796.796 0 0 0 0 1.075l4.15 4.42a.867.867 0 0 0 1.28 0l4.15-4.42a.796.796 0 0 0 0-1.075.684.684 0 0 0-1.01 0L12 13.248Z" clip-rule="evenodd"></path></svg></span></button></div></div><div><div><div class=""></div><div class="ModalLoading-content"><svg width="30" height="30" viewBox="0 0 66 66" xmlns="http://www.w3.org/2000/svg" class="CircleLoadingBar  css-bypjk1" aria-hidden="true"><g><circle class="path" fill="none" stroke-width="6" stroke-linecap="round" cx="33" cy="33" r="30"></circle></g></svg></div></div></div></div></div></div><div class="List-item" tabindex="0"><div><div class="ContentItem AnswerItem" data-za-index="4" data-zop="{&quot;authorName&quot;:&quot;AI财富学院大熊喵&quot;,&quot;itemId&quot;:3050301434,&quot;title&quot;:&quot;为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="3050301434" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="4" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;3050301434&quot;,&quot;upvote_num&quot;:0,&quot;comment_num&quot;:0,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;592844544&quot;,&quot;author_member_hash_id&quot;:&quot;1137751d68e7d494da93bca89d275bbf&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><div class="AuthorInfo"><meta itemprop="name" content="AI财富学院大熊喵"><meta itemprop="image" content="https://picx.zhimg.com/v2-c60e031a7866520434165a26e8eaade3_l.jpg?source=1940ef5c"><meta itemprop="url" content="https://www.zhihu.com/people/heavenskybird"><meta itemprop="zhihu:followerCount" content="13"><span class="UserLink AuthorInfo-avatarWrapper"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/heavenskybird" target="_blank" class="UserLink-link" data-za-detail-view-element_name="User"><img class="Avatar AuthorInfo-avatar css-1oqflzh" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/v2-c60e031a7866520434165a26e8eaade3_l.jpg" srcset="https://picx.zhimg.com/v2-c60e031a7866520434165a26e8eaade3_l.jpg?source=1940ef5c 2x" alt="AI财富学院大熊喵"></a></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/heavenskybird" target="_blank" class="UserLink-link" data-za-detail-view-element_name="User">AI财富学院大熊喵</a></div><a href="https://www.zhihu.com/question/48510028" target="_blank" class="css-n99yhz" aria-label="信息技术行业 监事" data-tooltip="信息技术行业 监事"><span class="css-18biwo">​<svg viewBox="0 0 24 24" class="css-1ifz0go" width="18" height="18"><defs><lineargradient id="biz-badges-blue" x1="0" y1="0" x2="100%" y2="73.32%"><stop offset="15%" stop-color="#47D3FF"></stop><stop offset="81.54%" stop-color="#4A7DFF"></stop></lineargradient></defs><svg viewBox="0 0 24 24" x="-3" y="-3" fill="#FFFFFF" width="30" height="30"><path d="M3.56231227,13.8535307 C2.40051305,12.768677 2.41398885,11.0669203 3.59484487,9.99979213 L3.59222085,9.99654885 C4.26730143,9.45036719 4.79446755,8.21005186 4.7184197,7.34453784 L4.72305873,7.34412719 C4.66942824,5.75539997 5.8824188,4.56066914 7.47188965,4.64242381 L7.47229112,4.6386236 C8.33515314,4.72977993 9.58467253,4.22534048 10.1426329,3.55925173 L10.1462611,3.56228565 C11.2316055,2.40008701 12.9353108,2.41394456 14.0015072,3.59634088 L14.0047263,3.59374004 C14.5498229,4.26841874 15.7896857,4.79521622 16.6545744,4.71844347 L16.6549836,4.72304294 C18.245027,4.66894057 19.4396947,5.88213996 19.3575031,7.47241135 L19.3623099,7.47292747 C19.2704388,8.3358681 19.7742711,9.58421483 20.4407199,10.1424506 L20.437686,10.1460789 C21.5997217,11.2312209 21.5860695,12.9345218 20.4042441,14.0007396 L20.4072865,14.0045125 C19.7325967,14.5495925 19.2055209,15.7896954 19.2815865,16.6561959 L19.2770449,16.6565978 C19.3315454,18.2453037 18.1173775,19.4393568 16.5274188,19.3571512 L16.5269029,19.3619539 C15.6647098,19.270083 14.415408,19.7741709 13.8573671,20.4403558 L13.8537409,20.4373235 C12.76842,21.5995708 11.0650432,21.5864553 9.99899434,20.4039226 L9.99527367,20.406923 C9.45025436,19.7323399 8.21017638,19.2051872 7.34461983,19.2812352 L7.344304,19.2776405 C5.75448683,19.3312904 4.55977145,18.1170085 4.64254978,16.527117 L4.63769921,16.5265942 C4.72957031,15.6644394 4.22547659,14.4151814 3.55928015,13.8571569 L3.56231227,13.8535307 Z"></path></svg><path d="M2.63951518,13.3895441 C3.70763333,14.2842292 4.44777637,16.1226061 4.30075305,17.5023312 L4.32211542,17.3063047 C4.17509209,18.6910561 5.17786655,19.7063729 6.5613937,19.5844846 L6.364106,19.6008202 C7.75140298,19.4789319 9.57474349,20.2554985 10.4468305,21.3349009 L10.3224262,21.1803415 C11.1982831,22.2647703 12.6257916,22.2723098 13.5167278,21.2079863 L13.3898102,21.3600325 C14.2845162,20.2919393 16.1229361,19.5518136 17.5026934,19.6988334 L17.3054057,19.6774716 C18.6914461,19.8244915 19.7067866,18.8217404 19.5836389,17.4395022 L19.6012314,17.6367853 C19.4793403,16.2482641 20.255925,14.4249662 21.3353526,13.5528995 L21.1807897,13.677301 C22.2639871,12.8014646 22.2727834,11.3739894 21.2084351,10.483074 L21.3604848,10.6099886 C20.2923667,9.71530351 19.5522236,7.87818322 19.6992469,6.49720154 L19.6778846,6.69448464 C19.8249079,5.30847665 18.8221335,4.2944164 17.4386063,4.41630468 L17.635894,4.39871256 C16.248597,4.52185742 14.4252565,3.74529084 13.5531695,2.66588842 L13.6775738,2.81919121 C12.8017169,1.73601905 11.3742084,1.72722299 10.4832722,2.79154644 L10.6101898,2.63950024 C9.71548377,3.70759343 7.87706394,4.44771919 6.49730661,4.30195588 L6.69459432,4.32206116 C5.30855394,4.17504128 4.29447,5.17904888 4.41636114,6.56128713 L4.3987686,6.36400404 C4.52065973,7.75126861 3.74407501,9.57456653 2.66464737,10.4478898 L2.81921035,10.3222318 C1.73601288,11.1993248 1.72721662,12.6255433 2.79156494,13.5164587 L2.63951518,13.3895441 Z" fill="url(#biz-badges-blue)"></path><svg width="12" height="12" viewBox="0 0 24 24" fill="#fff" x="6" y="6" class="Zi Zi--Check"><path fill-rule="evenodd" d="M21.864 5.347a1.25 1.25 0 0 1 .04 1.767L10.976 18.537a1.35 1.35 0 0 1-1.965-.014L2.584 11.6a1.25 1.25 0 0 1 1.832-1.702l5.598 6.029L20.096 5.386a1.25 1.25 0 0 1 1.767-.04Z" clip-rule="evenodd"></path></svg></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="ztext AuthorInfo-badgeText css-14ur8a8">信息技术行业 监事</div></div></div></div></div><div class="css-124ezq8"></div></div><div class="LabelContainer-wrapper"><div class="LabelContainer css-36m2wi"><div><div class="css-9mu7l2" style="cursor: pointer;"><div class="css-4rl99m"><div class="css-140fcia">创作声明：包含 AI 辅助创作</div></div><svg width="26" height="26" viewBox="0 0 24 24" fill="#175199" class="Zi Zi--ArrowDown css-ouwgqw"><path fill-rule="evenodd" d="M12 13.248 8.22 9.223a.684.684 0 0 0-1.01 0 .796.796 0 0 0 0 1.075l4.15 4.42a.867.867 0 0 0 1.28 0l4.15-4.42a.796.796 0 0 0 0-1.075.684.684 0 0 0-1.01 0L12 13.248Z" clip-rule="evenodd"></path></svg></div></div></div></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="0"><meta itemprop="url" content="https://www.zhihu.com/question/592844544/answer/3050301434"><meta itemprop="dateCreated" content="2023-05-29T14:02:42.000Z"><meta itemprop="dateModified" content="2023-05-29T14:02:42.000Z"><meta itemprop="commentCount" content="0"><div class="RichContent RichContent--unescapable"><span><div class="RichContent-inner"><div class="css-376mun"><span class="RichText ztext CopyrightRichText-richText css-1g0fqss" options="[object Object]" itemprop="text"><p data-first-child="" data-pid="7KT-vlfg">GPT模型输入的<span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=token&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3050301434%7D" target="_blank">token<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>数量被限制在较小的范围（几千到几万）有以下技术上的原因：</p><p data-pid="QEZHcidV">1. **计算资源限制**：GPT模型是一个非常庞大的<span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3050301434%7D" target="_blank">神经网络<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>，具有数以亿计的参数。对于处理大量的token输入，需要大量的计算资源和内存来执行模型的前向计算和反向传播。限制输入token数量可以<span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=%E6%8E%A7%E5%88%B6%E6%A8%A1%E5%9E%8B&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3050301434%7D" target="_blank">控制模型<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>的计算量，以适应计算资源的限制。</p><p data-pid="tleBm6f_">2. **序列长度对齐**：在使用<span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=%E6%89%B9%E9%87%8F%E8%AE%A1%E7%AE%97&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3050301434%7D" target="_blank">批量计算<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>时，输入序列的长度需要对齐。如果不限制输入token的数量，那么在一个批次中，不同样本的token长度可能会有很大的差异，这会导致需要填充或截断不同长度的序列，增加计算复杂度和内存需求。</p><p data-pid="SDk0_NvH">3. **模型训练的限制**：GPT模型通常是通过将输入序列分为固定长度的小块进行训练的。如果输入序列太长，会导致训练过程中的内存和计算需求剧增，从而增加训练时间和成本。限制输入token数量可以降低训练的复杂性和资源消耗。</p><p data-pid="Usady0Pq">4. **推理效率考量**：在实际应用中，高效的<span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3050301434%7D" target="_blank">模型推理<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>是非常重要的。限制输入token数量可以降低推理过程中的计算和内存需求，从而提高模型的运行效率和响应速度。</p><p data-pid="6x1owaxY">不同的GPT模型可能会有不同的输入token数量限制，这取决于模型的架构和设计。此外，即使通过按使用量付费的API，也需要限制输入token数量，以确保公平的资源分配和稳定的服务质量。</p><p data-pid="JR7zQnV5">总之，限制GPT模型输入token数量的主要原因是为了控制计算资源的消耗、提高模型的效率，并确保稳定的服务提供。这是为了<span><a class="RichContent-EntityWord css-pgtd2j" data-za-not-track-link="true" href="https://www.zhihu.com/search?q=%E5%B9%B3%E8%A1%A1%E6%A8%A1%E5%9E%8B&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3050301434%7D" target="_blank">平衡模型<svg width="10px" height="10px" viewBox="0 0 15 15" class="css-1dvsrp"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>的性能和可用性之间的技术考量。</p></span></div></div></span><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/592844544/answer/3050301434"><span data-tooltip="发布于 2023-05-29 22:02" aria-label="发布于 2023-05-29 22:02">发布于 2023-05-29 22:02</span></a></div></div><span></span><div class="ContentItem-actions RichContent-actions"><span><button aria-label="赞同 0 " aria-live="polite" type="button" class="Button VoteButton VoteButton--up FEfUrdfMIKpQDJDqkjte"><span style="display: inline-flex; align-items: center;">​<svg width="10" height="10" viewBox="0 0 24 24" class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor"><path fill-rule="evenodd" d="M13.792 3.681c-.781-1.406-2.803-1.406-3.584 0l-7.79 14.023c-.76 1.367.228 3.046 1.791 3.046h15.582c1.563 0 2.55-1.68 1.791-3.046l-7.79-14.023Z" clip-rule="evenodd"></path></svg></span>赞同</button><button aria-label="反对" aria-live="polite" type="button" class="Button VoteButton VoteButton--down FEfUrdfMIKpQDJDqkjte"><span style="display: inline-flex; align-items: center;">​<svg width="10" height="10" viewBox="0 0 24 24" class="Zi Zi--TriangleDown" fill="currentColor"><path fill-rule="evenodd" d="M13.792 20.319c-.781 1.406-2.803 1.406-3.584 0L2.418 6.296c-.76-1.367.228-3.046 1.791-3.046h15.582c1.563 0 2.55 1.68 1.791 3.046l-7.79 14.023Z" clip-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Comment Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>添加评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover17-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover17-content"><button type="button" class="Button FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Share Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path d="M19.47 1.914a.8.8 0 0 1 1.204.778l-1.872 16.386a.9.9 0 0 1-1.204.743l-4.615-1.692a.7.7 0 0 0-.831.28l-1.927 3.02c-.43.674-1.474.369-1.474-.43v-3.865a.8.8 0 0 1 .179-.504l5.808-7.148a.595.595 0 0 0-.897-.781l-5.93 6.354a1.1 1.1 0 0 1-1.258.252L2.57 13.46a.8.8 0 0 1-.08-1.415l16.98-10.13Z"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Star Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path d="M10.484 3.307c.673-1.168 2.358-1.168 3.032 0l2.377 4.122a.25.25 0 0 0 .165.12l4.655.987c1.319.28 1.84 1.882.937 2.884l-3.186 3.535a.25.25 0 0 0-.063.193l.5 4.733c.142 1.34-1.222 2.33-2.453 1.782l-4.346-1.938a.25.25 0 0 0-.204 0l-4.346 1.938c-1.231.549-2.595-.442-2.453-1.782l.5-4.733a.25.25 0 0 0-.064-.193L2.35 11.42c-.903-1.002-.382-2.604.937-2.884l4.655-.987a.25.25 0 0 0 .164-.12l2.378-4.122Z"></path></svg></span>收藏</button><button aria-live="polite" type="button" class="Button ContentItem-action FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Heart Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path fill-rule="evenodd" d="M12.004 4.934c1.015-.944 2.484-1.618 3.98-1.618 3.48 0 6.53 3.265 6.15 7.614-.11 1.254-.686 2.55-1.458 3.753-.778 1.215-1.79 2.392-2.845 3.419-1.054 1.028-2.168 1.923-3.161 2.566a9.96 9.96 0 0 1-1.41.777c-.418.182-.862.32-1.268.32s-.848-.137-1.267-.317a9.918 9.918 0 0 1-1.407-.771c-.992-.64-2.103-1.53-3.156-2.555-1.052-1.024-2.062-2.2-2.84-3.417-.77-1.208-1.346-2.51-1.456-3.775-.38-4.349 2.67-7.614 6.15-7.614 1.484 0 2.983.673 3.988 1.618Z" clip-rule="evenodd"></path></svg></span>喜欢</button><div class="Popover ContentItem-action"><button aria-label="更多" id="Popover18-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover18-content" type="button" class="Button OptionsButton FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--iconOnly fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY hIwDV_tcL6XN1HprrnAq"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Dots Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path fill-rule="evenodd" d="M5.83 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Zm7.835 0a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Zm6.17 1.665a1.665 1.665 0 1 0 0-3.33 1.665 1.665 0 0 0 0 3.33Z" clip-rule="evenodd"></path></svg></span></button></div><button data-zop-retract-question="true" type="button" class="Button ContentItem-action ContentItem-rightButton FEfUrdfMIKpQDJDqkjte Button--plain fEPKGkUK5jyc4fUuT0QP"><span class="RichContent-collapsedText">收起</span><span style="display: inline-flex; align-items: center;">​<svg width="24" height="24" viewBox="0 0 24 24" class="Zi Zi--ArrowDown ContentItem-arrowIcon is-active" fill="currentColor"><path fill-rule="evenodd" d="M12 13.248 8.22 9.223a.684.684 0 0 0-1.01 0 .796.796 0 0 0 0 1.075l4.15 4.42a.867.867 0 0 0 1.28 0l4.15-4.42a.796.796 0 0 0 0-1.075.684.684 0 0 0-1.01 0L12 13.248Z" clip-rule="evenodd"></path></svg></span></button></div></div><div><div><div class=""></div><div class="ModalLoading-content"><svg width="30" height="30" viewBox="0 0 66 66" xmlns="http://www.w3.org/2000/svg" class="CircleLoadingBar  css-bypjk1" aria-hidden="true"><g><circle class="path" fill="none" stroke-width="6" stroke-linecap="round" cx="33" cy="33" r="30"></circle></g></svg></div></div></div></div></div></div><div role="listitem"></div></div></div></div></div></div></div></div></div><div class="Question-sideColumn Question-sideColumn--sticky css-1qyytj7" data-za-detail-view-path-module="RightSideBar" data-za-extra-module="{}"><a aria-label="边栏锚点" aria-keyshortcuts="Shift+S" class="css-h9cq7d"></a><div class="css-1oy4rvw" style="top: -1143.33px;"><div class="Card"><div class="Pc-card Card"><a class="Banner-link" href="https://dc.ones.ai/t/Eh8" target="_blank"><div class="Banner-adTag">广告</div><div class="AdvertImg AdvertImg--isLoaded Banner-image"><img alt="广告" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/v2-a372417c69be0bf1b0d260973dd6ca6a_720w.webp"></div></a><div class="Pc-card-button-close " data-tooltip="不再显示" data-tooltip-position="bottom"><svg viewBox="0 0 12 12" class="Icon Icon--close Pc-card-button-close-svg "><path fill-rule="evenodd" d="M3 2L2 3l3 3-3 3 1 1 3-3 3 3 1-1-3-3 3-3-1-1-3 3"></path></svg></div></div></div><div class="Card css-oyqdpg" role="complementary" aria-label="相关问题" data-za-detail-view-path-module="RelatedQuestions" data-za-extra-module="{}"><div class="Card-header SimilarQuestions-title"><div class="Card-headerText">相关问题</div></div><div class="Card-section SimilarQuestions-list"><div class="SimilarQuestions-item" itemprop="zhihu:similarQuestion" itemtype="http://schema.org/Question" itemscope="" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;291728013&quot;}}}"><meta itemprop="name" content="关于token的使用，网上查了一下资料，说法不一，感觉都有漏洞，请帮忙指教一下？"><meta itemprop="url" content="https://www.zhihu.com/question/291728013"><meta itemprop="answerCount" content="5"><meta itemprop="zhihu:followerCount" content="9"><a target="_blank" type="button" class="Button FEfUrdfMIKpQDJDqkjte Button--plain fEPKGkUK5jyc4fUuT0QP" href="https://www.zhihu.com/question/291728013">关于token的使用，网上查了一下资料，说法不一，感觉都有漏洞，请帮忙指教一下？</a> 5 个回答</div><div class="SimilarQuestions-item" itemprop="zhihu:similarQuestion" itemtype="http://schema.org/Question" itemscope="" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;577428781&quot;}}}"><meta itemprop="name" content="token 解析错误怎么解决？"><meta itemprop="url" content="https://www.zhihu.com/question/577428781"><meta itemprop="answerCount" content="0"><meta itemprop="zhihu:followerCount" content="1"><a target="_blank" type="button" class="Button FEfUrdfMIKpQDJDqkjte Button--plain fEPKGkUK5jyc4fUuT0QP" href="https://www.zhihu.com/question/577428781">token 解析错误怎么解决？</a> 0 个回答</div><div class="SimilarQuestions-item" itemprop="zhihu:similarQuestion" itemtype="http://schema.org/Question" itemscope="" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;499334767&quot;}}}"><meta itemprop="name" content="接口测试中，做其他操作时要用到token，但token有效时间5分钟，要怎么操作？"><meta itemprop="url" content="https://www.zhihu.com/question/499334767"><meta itemprop="answerCount" content="1"><meta itemprop="zhihu:followerCount" content="2"><a target="_blank" type="button" class="Button FEfUrdfMIKpQDJDqkjte Button--plain fEPKGkUK5jyc4fUuT0QP" href="https://www.zhihu.com/question/499334767">接口测试中，做其他操作时要用到token，但token有效时间5分钟，要怎么操作？</a> 1 个回答</div><div class="SimilarQuestions-item" itemprop="zhihu:similarQuestion" itemtype="http://schema.org/Question" itemscope="" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;35907668&quot;}}}"><meta itemprop="name" content="SVM分类中一个token属于多个类别的问题？"><meta itemprop="url" content="https://www.zhihu.com/question/35907668"><meta itemprop="answerCount" content="2"><meta itemprop="zhihu:followerCount" content="8"><a target="_blank" type="button" class="Button FEfUrdfMIKpQDJDqkjte Button--plain fEPKGkUK5jyc4fUuT0QP" href="https://www.zhihu.com/question/35907668">SVM分类中一个token属于多个类别的问题？</a> 2 个回答</div><div class="SimilarQuestions-item" itemprop="zhihu:similarQuestion" itemtype="http://schema.org/Question" itemscope="" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;306073522&quot;}}}"><meta itemprop="name" content="开局拉盘，20%，对于FPLUS富加Token可用于投其它项目，有人试过吗？"><meta itemprop="url" content="https://www.zhihu.com/question/306073522"><meta itemprop="answerCount" content="0"><meta itemprop="zhihu:followerCount" content="1"><a target="_blank" type="button" class="Button FEfUrdfMIKpQDJDqkjte Button--plain fEPKGkUK5jyc4fUuT0QP" href="https://www.zhihu.com/question/306073522">开局拉盘，20%，对于FPLUS富加Token可用于投其它项目，有人试过吗？</a> 0 个回答</div></div></div><div class="Card" data-za-detail-view-path-module="ContentList" data-za-detail-view-path-module_name="相关推荐" data-za-extra-module="{}"><div class="Card-header RelatedCommodities-title"><div class="Card-headerText">相关推荐</div></div><div class="Card-section RelatedCommodities-list"><a target="_blank" href="https://www.zhihu.com/pub/book/119974141" type="button" class="Button RelatedCommodities-item FEfUrdfMIKpQDJDqkjte Button--plain fEPKGkUK5jyc4fUuT0QP" data-za-detail-view-path-module="EBookItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;EBook&quot;,&quot;token&quot;:&quot;119974141&quot;}}}"><img class="RelatedCommodities-image" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/v2-b2588b6fd4b900fdff6abcb2df57fecc_250x0.jpg" alt="live"><div class="RelatedCommodities-content"><div class="RelatedCommodities-subject RelatedCommodities-subject-two" data-tooltip="AI 前线（2018 年 10 月刊）">AI 前线（2018 年 10 月刊）</div><div class="RelatedCommodities-meta"><div class="RelatedCommodities-bookMeta">10 人读过<span class="RelatedCommodities-bookRead"><span style="display: inline-flex; align-items: center;">​<svg width="13" height="14" viewBox="0 0 24 24" class="Zi Zi--Ebook" fill="currentColor"><path fill-rule="evenodd" d="M16 17.649V2.931a.92.92 0 0 0-.044-.283.943.943 0 0 0-1.183-.604L4.655 5.235A.932.932 0 0 0 4 6.122v14.947c0 .514.421.931.941.931H19.06c.52 0 .941-.417.941-.93V7.292a.936.936 0 0 0-.941-.931h-.773v12.834a.935.935 0 0 1-.83.924L6.464 21.416c-.02.002 2.94-.958 8.883-2.881a.932.932 0 0 0 .653-.886Z" clip-rule="evenodd"></path></svg></span>阅读</span></div></div></div></a><a target="_blank" href="https://www.zhihu.com/pub/book/119617490" type="button" class="Button RelatedCommodities-item FEfUrdfMIKpQDJDqkjte Button--plain fEPKGkUK5jyc4fUuT0QP" data-za-detail-view-path-module="EBookItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;EBook&quot;,&quot;token&quot;:&quot;119617490&quot;}}}"><img class="RelatedCommodities-image" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/v2-258252c22246099570b1ea19f758a13d_250x0.jpg" alt="live"><div class="RelatedCommodities-content"><div class="RelatedCommodities-subject RelatedCommodities-subject-two" data-tooltip="「一带一路」倡议与中国参与全球治理新突破 （谷臻小简·AI 导读版）">「一带一路」倡议与中国参与全球治理新突破 （谷臻小简·AI 导读版）</div><div class="RelatedCommodities-meta"><div class="RelatedCommodities-bookMeta">9 人读过<span class="RelatedCommodities-bookRead"><span style="display: inline-flex; align-items: center;">​<svg width="13" height="14" viewBox="0 0 24 24" class="Zi Zi--Ebook" fill="currentColor"><path fill-rule="evenodd" d="M16 17.649V2.931a.92.92 0 0 0-.044-.283.943.943 0 0 0-1.183-.604L4.655 5.235A.932.932 0 0 0 4 6.122v14.947c0 .514.421.931.941.931H19.06c.52 0 .941-.417.941-.93V7.292a.936.936 0 0 0-.941-.931h-.773v12.834a.935.935 0 0 1-.83.924L6.464 21.416c-.02.002 2.94-.958 8.883-2.881a.932.932 0 0 0 .653-.886Z" clip-rule="evenodd"></path></svg></span>阅读</span></div></div></div></a><a target="_blank" href="https://www.zhihu.com/pub/book/119617685" type="button" class="Button RelatedCommodities-item FEfUrdfMIKpQDJDqkjte Button--plain fEPKGkUK5jyc4fUuT0QP" data-za-detail-view-path-module="EBookItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;EBook&quot;,&quot;token&quot;:&quot;119617685&quot;}}}"><img class="RelatedCommodities-image" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/v2-cd8acf5ffc3958f0bc4270ba56018688_250x0.jpg" alt="live"><div class="RelatedCommodities-content"><div class="RelatedCommodities-subject RelatedCommodities-subject-two" data-tooltip="国际资本流动与金融稳定性研究——基于中东欧和独联体国家的比较（谷臻小简·AI 导读版）">国际资本流动与金融稳定性研究——基于中东欧和独联体国家的比较（谷臻小简·AI 导读版）</div><div class="RelatedCommodities-meta"><div class="RelatedCommodities-bookMeta">2 人读过<span class="RelatedCommodities-bookRead"><span style="display: inline-flex; align-items: center;">​<svg width="13" height="14" viewBox="0 0 24 24" class="Zi Zi--Ebook" fill="currentColor"><path fill-rule="evenodd" d="M16 17.649V2.931a.92.92 0 0 0-.044-.283.943.943 0 0 0-1.183-.604L4.655 5.235A.932.932 0 0 0 4 6.122v14.947c0 .514.421.931.941.931H19.06c.52 0 .941-.417.941-.93V7.292a.936.936 0 0 0-.941-.931h-.773v12.834a.935.935 0 0 1-.83.924L6.464 21.416c-.02.002 2.94-.958 8.883-2.881a.932.932 0 0 0 .653-.886Z" clip-rule="evenodd"></path></svg></span>阅读</span></div></div></div></a></div></div><div class="Card"><div class="Pc-card Card"><a class="Banner-link" href="https://zhuanlan.zhihu.com/p/638637962?" target="_blank"><div class="Banner-adTag">广告</div><div class="AdvertImg AdvertImg--isLoaded Banner-image"><img alt="广告" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/v2-91ed71c8b13708c8a359f1024b297187_720w.webp"></div></a><div class="Pc-card-button-close " data-tooltip="不再显示" data-tooltip-position="bottom"><svg viewBox="0 0 12 12" class="Icon Icon--close Pc-card-button-close-svg "><path fill-rule="evenodd" d="M3 2L2 3l3 3-3 3 1 1 3-3 3 3 1-1-3-3 3-3-1-1-3 3"></path></svg></div></div></div><footer class="Footer" role="contentinfo"><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://liukanshan.zhihu.com/">刘看山</a><span class="Footer-dot"></span><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://www.zhihu.com/question/19581624">知乎指南</a><span class="Footer-dot"></span><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://www.zhihu.com/term/zhihu-terms">知乎协议</a><span class="Footer-dot"></span><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://www.zhihu.com/term/privacy">知乎隐私保护指引</a><br><a class="Footer-item" target="_blank" href="https://www.zhihu.com/app">应用</a><span class="Footer-dot"></span><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://app.mokahr.com/apply/zhihu">工作</a><span class="Footer-dot"></span><button type="button" class="Button OrgCreateButton FEfUrdfMIKpQDJDqkjte">申请开通知乎机构号</button><br><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://zhuanlan.zhihu.com/p/28852607">侵权举报</a><span class="Footer-dot"></span><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="http://www.12377.cn/">网上有害信息举报专区</a><br><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://tsm.miit.gov.cn/dxxzsp/">京 ICP 证 110745 号</a><br><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://beian.miit.gov.cn/">京 ICP 备 13052560 号 - 1</a><br><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=11010802020088"><img src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/v2-d0289dc0a46fc5b15b3363ffa78cf6c7.png" alt="">京公网安备 11010802020088 号</a><br><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://www.zhihu.com/certificates">京网文[2022]2674-081 号</a><br><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://pic3.zhimg.com/v2-c280f8bce57f9b045b83185384d86027.png">药品医疗器械网络信息服务备案<br>（京）网药械信息备字（2022）第00334号</a><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://www.zhihu.com/certificates">广播电视节目制作经营许可证:（京）字第06591号</a><span class="Footer-item">服务热线：400-919-0001</span><span class="Footer-item">违法和不良信息举报：010-82716601</span><a class="Footer-item" href="mailto:jubao@zhihu.com">举报邮箱：jubao@zhihu.com</a><br><a class="Footer-item" target="_blank" href="https://www.zhihu.com/term/child-jubao">儿童色情信息举报专区</a><br><a class="Footer-item" target="_blank" href="https://www.zhihu.com/term/algorithm-recommend-report">互联网算法推荐举报专区</a><br><a class="Footer-item" target="_blank" href="https://www.zhihu.com/term/counterfeit-swindle-report">仿冒诈骗专区</a><br><a class="Footer-item" target="_blank" href="https://www.zhihu.com/term/mcn/report">MCN 举报专区</a><br><a class="Footer-item" target="_blank" href="https://www.zhihu.com/term/info-sec">信息安全漏洞反馈专区</a><br><a class="Footer-item" target="_blank" href="https://www.zhihu.com/term/report-content-idustry">内容从业人员违法违规行为举报</a><br><a class="Footer-item" target="_blank" href="https://www.zhihu.com/term/net-report">网络谣言信息举报入口</a><br><a class="Footer-item" target="_blank" href="https://www.zhihu.com/term/cyber-communication-order">网络传播秩序举报专区</a><br><a class="Footer-item" target="_blank" href="https://www.zhihu.com/term/enterprise-fake-info">涉企虚假不实信息举报专区</a><br><a class="Footer-item" target="_blank" href="https://www.zhihu.com/certificates">证照中心</a><span class="Footer-dot"></span><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://ir.zhihu.com/">Investor Relations</a><br><a class="Footer-item" target="_blank" href="https://www.zhihu.com/contact">联系我们</a><span> © 2023 知乎</span><br><span class="Footer-item">北京智者天下科技有限公司版权所有</span><br><img loading="lazy" width="80" height="38" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/v2-ccdb7828c12afff31a27e51593d23260_720w.png" alt="本站提供适老化无障碍服务" class="css-1170n61"></footer></div></div></div></div></div></main><div data-zop-usertoken="{&quot;urlToken&quot;:&quot;62-96-78-5-56&quot;}"></div><div role="complementary"><div class="CornerButtons"><div class="CornerAnimayedFlex CornerAnimayedFlex--hidden"><button data-tooltip="回到顶部" data-tooltip-position="left" data-tooltip-will-hide-on-click="true" aria-label="回到顶部" type="button" class="Button CornerButton Button--plain css-gdd4kf"><svg width="24" height="24" viewBox="0 0 24 24" aria-hidden="true" class="Zi Zi--BackToTop" fill="currentColor"><path fill-rule="evenodd" d="M13.204 3.107a1.75 1.75 0 0 0-2.408 0L3.806 9.73c-1.148 1.088-.378 3.02 1.204 3.02h2.24V20c0 .966.784 1.75 1.75 1.75h6A1.75 1.75 0 0 0 16.75 20v-7.25h2.24c1.582 0 2.353-1.932 1.204-3.02l-6.99-6.623Z" clip-rule="evenodd"></path></svg></button></div></div></div></div></div><script id="js-clientConfig" type="text/json">{"fetchRoot":{"www":"https:\u002F\u002Fwww.zhihu.com","api":"https:\u002F\u002Fapi.zhihu.com","lens":"https:\u002F\u002Flens.zhihu.com","zhuanlan":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fapi\u002F","walletpay":"https:\u002F\u002Fwalletpay.zhihu.com","captcha":"https:\u002F\u002Fcaptcha.zhihu.com","vzuu":"https:\u002F\u002Fv.vzuu.com","openapi":"https:\u002F\u002Fopenapi.zhihu.com","svip":"https:\u002F\u002Fsvip.zhihu.com"},"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","videoHost":"video.zhihu.com","zhuanlanHost":"zhuanlan.zhihu.com","allowSignUp":true,"refreshValidityPeriod":"30","release":"840-a63ba4db","currentEntry":"main","isMobileEntry":false,"apollo":{"env":"prod","globalSilence":"","ncgModeSign":"3f8e56febda4fb3bbea72e379d76de1e","topstory_rec_adp":"1","test_canary":"member|0-100,1-0","use_new_player":"member|0-0,1-100","player_vendor":"member|0-0,1-100,2-0","use_hevc":"member|0-0,1-100","upload_use_signature":"member|0-0,1-100","use_backdrop_blur":"member|0-0,1-100","article_title_imagex":"member|0-0,1-100","play_station":"member|0-0,1-100"}}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"question\u002Fget\u002F":false,"question\u002FgetAnswers\u002F592844544":false}},"entities":{"users":{"e2df7218c1b1b43fb3163821b169d6de":{"uid":1634215762683547600,"userType":"people","id":"e2df7218c1b1b43fb3163821b169d6de"}},"questions":{"592844544":{"type":"question","id":592844544,"title":"为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？","questionType":"normal","created":1680179783,"updatedTime":1680179783,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F592844544","isMuted":false,"isVisible":true,"isNormal":true,"isEditable":true,"adminClosedComment":false,"hasPublishingDraft":false,"answerCount":6,"visitCount":70496,"commentCount":1,"followerCount":149,"collapsedAnswerCount":5,"excerpt":"即使是按使用量付费的api也同样有这个限制 具体到不同模型可能限制还不一样，少的只有几千","commentPermission":"all","detail":"\u003Cp\u003E即使是按使用量付费的api也同样有这个限制\u003C\u002Fp\u003E\u003Cp\u003E具体到不同模型可能限制还不一样，少的只有几千\u003C\u002Fp\u003E","editableDetail":"\u003Cp\u003E即使是按使用量付费的api也同样有这个限制\u003C\u002Fp\u003E\u003Cp\u003E具体到不同模型可能限制还不一样，少的只有几千\u003C\u002Fp\u003E","status":{"isLocked":false,"isClose":false,"isEvaluate":false,"isSuggest":false},"relationship":{"isAuthor":false,"isFollowing":false,"isAnonymous":false,"canLock":false,"canStickAnswers":false,"canCollapseAnswers":false,"voting":0},"topics":[{"id":"19550517","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19550517","name":"互联网","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c1ea3804dc369dbfad3de0c405c0a3d2_qhd.jpg","topicType":"NORMAL"},{"id":"19551275","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19551275","name":"人工智能","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-c41d10d22173d515740c43c70f885705_qhd.jpg?source=57bbeac9","topicType":"NORMAL"},{"id":"20106982","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20106982","name":"AI技术","avatarUrl":"","topicType":"NORMAL"},{"id":"26215901","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F26215901","name":"AIGC","avatarUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-17256bdee3ce72a338455853bb80eca1_qhd.jpg?source=57bbeac9","topicType":"NORMAL"},{"id":"26691895","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F26691895","name":"ChatGPT","avatarUrl":"","topicType":"NORMAL"}],"author":{"id":"ddac64e6b9b0e17d2e636880c2c005ff","urlToken":"aaazzz-53","name":"aaazzz","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f1be5ed24936a7311e75da3884b2bd6d_l.jpg?source=1940ef5c","avatarUrlTemplate":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-f1be5ed24936a7311e75da3884b2bd6d.jpg?source=1940ef5c","isOrg":false,"type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002Fddac64e6b9b0e17d2e636880c2c005ff","userType":"people","headline":"","badge":[],"badgeV2":{"title":"","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""},"gender":0,"isAdvertiser":false,"isPrivacy":false},"canComment":{"status":true,"reason":""},"thumbnailInfo":{"count":0,"type":"thumbnail_info","thumbnails":[]},"reviewInfo":{"type":"","tips":"","editTips":"","isReviewing":false,"editIsReviewing":false},"relatedCards":[],"muteInfo":{"type":""},"showAuthor":false,"isLabeled":false,"isBannered":false,"showEncourageAuthor":false,"voteupCount":4,"canVote":true,"reactionInstruction":{}}},"answers":{"3026367037":{"adminClosedComment":false,"annotationAction":null,"answerType":"normal","attachedInfo":"ogEQCAQQAxi95IqjCyCArtiaApICJQoJNTgwMzM0MTk0EgozMDI2MzY3MDM3GAQiCklNQUdFX1RFWFQ=","author":{"avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-97ad403a44e44e66b784c604a40288c7_l.jpg?source=1940ef5c","avatarUrlTemplate":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-97ad403a44e44e66b784c604a40288c7.jpg?source=1940ef5c","badge":[],"badgeV2":{"detailBadges":[],"icon":"","mergedBadges":[],"nightIcon":"","title":""},"exposedMedal":{"avatarUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-3fc138243615ec84e052fa0f66c0e1a4_l.png?source=1940ef5c","description":"获得 300 个赞同","medalAvatarFrame":"","medalId":"972475023013875712","medalName":"有口皆碑","miniAvatarUrl":"https:\u002F\u002Fpica.zhimg.com\u002Fv2-3fc138243615ec84e052fa0f66c0e1a4_r.png?source=1940ef5c"},"followerCount":182,"gender":1,"headline":"","id":"3bbb23c6e8a40ced4db0b11342aeb257","isAdvertiser":false,"isFollowed":false,"isFollowing":false,"isOrg":false,"isPrivacy":false,"name":"无数据不智能","type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002F3bbb23c6e8a40ced4db0b11342aeb257","urlToken":"ha-ha-67-11-64","userType":"people","vipInfo":{"isVip":false}},"canComment":{"reason":"","status":true},"collapseReason":"","collapsedBy":"nobody","commentCount":0,"commentPermission":"all","content":"\u003Cblockquote data-pid=\"NbVR2iEO\"\u003E 本文着重于对各种的方法的思想总结，非严谨推导\u003C\u002Fblockquote\u003E\u003Ch2\u003E\u003Cb\u003E循环记忆输入\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cblockquote data-pid=\"wJjor0Re\"\u003E Recurrent Memory Transformer (RMT)\u003C\u002Fblockquote\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpicx.zhimg.com\u002F50\u002Fv2-9a9aced7c5b9bc4cd0b7b94bdd699cff_720w.jpg?source=1940ef5c\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1356\" data-rawheight=\"584\" data-original-token=\"v2-aa7dd794ecdf027b15bd94e52047de26\" class=\"origin_image zh-lightbox-thumb\" width=\"1356\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-9a9aced7c5b9bc4cd0b7b94bdd699cff_r.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1356&#39; height=&#39;584&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1356\" data-rawheight=\"584\" data-original-token=\"v2-aa7dd794ecdf027b15bd94e52047de26\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1356\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-9a9aced7c5b9bc4cd0b7b94bdd699cff_r.jpg?source=1940ef5c\" data-actualsrc=\"https:\u002F\u002Fpicx.zhimg.com\u002F50\u002Fv2-9a9aced7c5b9bc4cd0b7b94bdd699cff_720w.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"wOHKO9i-\"\u003E总体思想：将长文本分段之后得到嵌入向量与记忆向量拼接，得到新的记忆向量之后与下一段再循环输入transformer。\u003C\u002Fp\u003E\u003Cp data-pid=\"yHUwzWbB\"\u003E注意：此论文实验结果在bert-base-cased（encoder-only上进行实验）\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpicx.zhimg.com\u002F50\u002Fv2-40c7dfcffee6506a0e422e5050d681a3_720w.jpg?source=1940ef5c\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1030\" data-rawheight=\"328\" data-original-token=\"v2-ce874ec5804ee94ca78cbb292a655c1b\" class=\"origin_image zh-lightbox-thumb\" width=\"1030\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-40c7dfcffee6506a0e422e5050d681a3_r.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1030&#39; height=&#39;328&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1030\" data-rawheight=\"328\" data-original-token=\"v2-ce874ec5804ee94ca78cbb292a655c1b\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1030\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-40c7dfcffee6506a0e422e5050d681a3_r.jpg?source=1940ef5c\" data-actualsrc=\"https:\u002F\u002Fpicx.zhimg.com\u002F50\u002Fv2-40c7dfcffee6506a0e422e5050d681a3_720w.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"h23f0ro4\"\u003ECoLT5达到64K，GPT-4达到32K长度，而RMT在实验结果中长度加到4096个分段2048000词汇，效果依然强劲。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E用提示词\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cblockquote data-pid=\"-qoUtuZR\"\u003E Self-Controlled Memory (SCM)\u003C\u002Fblockquote\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpica.zhimg.com\u002F50\u002Fv2-90b073ca331935a81fd3fe9e2e8afb0f_720w.jpg?source=1940ef5c\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1459\" data-rawheight=\"679\" data-original-token=\"v2-38a1ad8e80c3d00f020152f7ae597cc7\" class=\"origin_image zh-lightbox-thumb\" width=\"1459\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-90b073ca331935a81fd3fe9e2e8afb0f_r.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1459&#39; height=&#39;679&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1459\" data-rawheight=\"679\" data-original-token=\"v2-38a1ad8e80c3d00f020152f7ae597cc7\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1459\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-90b073ca331935a81fd3fe9e2e8afb0f_r.jpg?source=1940ef5c\" data-actualsrc=\"https:\u002F\u002Fpica.zhimg.com\u002F50\u002Fv2-90b073ca331935a81fd3fe9e2e8afb0f_720w.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"l1E7ZSqn\"\u003E如上图所示，此方法号称可以将输入延申至无限长，具体流程为：\u003C\u002Fp\u003E\u003Col\u003E\u003Cli data-pid=\"YdD6EmAC\"\u003E用户输入\u003Cbr\u002F\u003E \u003C\u002Fli\u003E\u003Cli data-pid=\"8fRBei0O\"\u003E判断是否需要从历史会话中获得记忆，提示词如下：\u003Cbr\u002F\u003E 给定一个用户指令，判断执行该指令是否需要历史信\u003Cbr\u002F\u003E息或者上文的信息，或者需要回忆对话内容，只需要\u003Cbr\u002F\u003E回答是(A)或者否(B)，不需要解释信息：\u003Cbr\u002F\u003E指令：[用户输入]\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E \u003C\u002Fli\u003E\u003Cli data-pid=\"-3O6Kow-\"\u003E如果需要获取记忆，通过相关性（余弦相似度）、近期性分数相加对历史记忆进行排序\u003Cbr\u002F\u003E \u003C\u002Fli\u003E\u003Cli data-pid=\"1d7dfcHw\"\u003E将记忆摘要\u003Cbr\u002F\u003E 以下是用户和人工智能助手的一段对话，请分\u003Cbr\u002F\u003E别用一句话写出用户摘要、助手摘要，分段列\u003Cbr\u002F\u003E出，要求尽可能保留用户问题和助手回答的关\u003Cbr\u002F\u003E键信息。\u003Cbr\u002F\u003E对话内容：\u003Cbr\u002F\u003E用户：[用户输入]\u003Cbr\u002F\u003E助手：[系统回复]\u003Cbr\u002F\u003E摘要：\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E \u003C\u002Fli\u003E\u003Cli data-pid=\"hiA76f49\"\u003E将记忆和输入拼接输入模型\u003Cbr\u002F\u003E 以下是用户和人工智能助手的对话，请根据历史\u003Cbr\u002F\u003E对话内容，回答用户当前问题：\u003Cbr\u002F\u003E相关历史对话：\u003Cbr\u002F\u003E[历史轮对话内容]\u003Cbr\u002F\u003E上一轮对话：\u003Cbr\u002F\u003E[上一轮对话内容]\u003Cbr\u002F\u003E\u003Ci\u003E###\u003C\u002Fi\u003E\u003Cbr\u002F\u003E用户：[用户问题]\u003Cbr\u002F\u003E助手：\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E \u003C\u002Fli\u003E\u003Cli data-pid=\"QfkTcuwx\"\u003E回复\u003Cbr\u002F\u003E \u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cp data-pid=\"WEtcAQ7j\"\u003E注意：此论文中只进行了定性分析，没有定量实验。以下是效果图：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpicx.zhimg.com\u002F50\u002Fv2-c23b706d65efa0d94bd065da95f08b99_720w.jpg?source=1940ef5c\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"726\" data-rawheight=\"602\" data-original-token=\"v2-8cbb05dac851be081593a8e99d7a6303\" class=\"origin_image zh-lightbox-thumb\" width=\"726\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-c23b706d65efa0d94bd065da95f08b99_r.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;726&#39; height=&#39;602&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"726\" data-rawheight=\"602\" data-original-token=\"v2-8cbb05dac851be081593a8e99d7a6303\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"726\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-c23b706d65efa0d94bd065da95f08b99_r.jpg?source=1940ef5c\" data-actualsrc=\"https:\u002F\u002Fpicx.zhimg.com\u002F50\u002Fv2-c23b706d65efa0d94bd065da95f08b99_720w.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch2\u003E\u003Cb\u003E词汇压缩\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cblockquote data-pid=\"uLiUsnyL\"\u003E VIP-token centric compression (Vcc)\u003C\u002Fblockquote\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpicx.zhimg.com\u002F50\u002Fv2-3d9db54d4962c0e9d61213f067e52e86_720w.jpg?source=1940ef5c\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1021\" data-rawheight=\"792\" data-original-token=\"v2-55d599c3eae34148f84e84b2882326a6\" class=\"origin_image zh-lightbox-thumb\" width=\"1021\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-3d9db54d4962c0e9d61213f067e52e86_r.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1021&#39; height=&#39;792&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1021\" data-rawheight=\"792\" data-original-token=\"v2-55d599c3eae34148f84e84b2882326a6\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1021\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-3d9db54d4962c0e9d61213f067e52e86_r.jpg?source=1940ef5c\" data-actualsrc=\"https:\u002F\u002Fpicx.zhimg.com\u002F50\u002Fv2-3d9db54d4962c0e9d61213f067e52e86_720w.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"nzNiAlVZ\"\u003E该方法使得模型输入延申至128K，并在Encoder-Only、Encoder-Decoder两种模型架构上都进行了实验。\u003C\u002Fp\u003E\u003Cp data-pid=\"MR1ZOX8A\"\u003E一句话描述思想：使模型输入长度独立于文本长度。\u003C\u002Fp\u003E\u003Cp data-pid=\"HcyRoBfC\"\u003E具体一点：\u003C\u002Fp\u003E\u003Col\u003E\u003Cli data-pid=\"-mTY9XX-\"\u003E将当前问句视为vip-token\u003C\u002Fli\u003E\u003Cli data-pid=\"QUfaVCPO\"\u003E利用当前问句与历史记忆的关系，压缩历史记忆到模型输入长度，无论历史记忆有多长\u003C\u002Fli\u003E\u003Cli data-pid=\"3CBgN-Do\"\u003Etransformer层输出之后再进行解压缩\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cp data-pid=\"sU1R1jMJ\"\u003EEncoder-Only架构表现：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpicx.zhimg.com\u002F50\u002Fv2-5b27ff419149fc1a0a3ce33911dfb657_720w.jpg?source=1940ef5c\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"668\" data-rawheight=\"288\" data-original-token=\"v2-7592fc65184d68d29e93b9a3f43ac708\" class=\"origin_image zh-lightbox-thumb\" width=\"668\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-5b27ff419149fc1a0a3ce33911dfb657_r.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;668&#39; height=&#39;288&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"668\" data-rawheight=\"288\" data-original-token=\"v2-7592fc65184d68d29e93b9a3f43ac708\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"668\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-5b27ff419149fc1a0a3ce33911dfb657_r.jpg?source=1940ef5c\" data-actualsrc=\"https:\u002F\u002Fpicx.zhimg.com\u002F50\u002Fv2-5b27ff419149fc1a0a3ce33911dfb657_720w.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"2v0xozWG\"\u003EEncoder-Decoder表现：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-deb7498d320aa9dc209409bf09718ff9_720w.jpg?source=1940ef5c\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1033\" data-rawheight=\"769\" data-original-token=\"v2-e935866876c479163317d26477ad1d1d\" class=\"origin_image zh-lightbox-thumb\" width=\"1033\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-deb7498d320aa9dc209409bf09718ff9_r.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1033&#39; height=&#39;769&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1033\" data-rawheight=\"769\" data-original-token=\"v2-e935866876c479163317d26477ad1d1d\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1033\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-deb7498d320aa9dc209409bf09718ff9_r.jpg?source=1940ef5c\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-deb7498d320aa9dc209409bf09718ff9_720w.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch2\u003E\u003Cb\u003E检索+交叉注意力\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cblockquote data-pid=\"W1Gk0cDd\"\u003E Unlimited Length Input Transformers (Unlimiformer)\u003C\u002Fblockquote\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpica.zhimg.com\u002F50\u002Fv2-917b060abda753ad352f98bcce863477_720w.jpg?source=1940ef5c\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1260\" data-rawheight=\"499\" data-original-token=\"v2-bd2d39e0ce9ac2946ed55f92531144aa\" class=\"origin_image zh-lightbox-thumb\" width=\"1260\" data-original=\"https:\u002F\u002Fpica.zhimg.com\u002Fv2-917b060abda753ad352f98bcce863477_r.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1260&#39; height=&#39;499&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1260\" data-rawheight=\"499\" data-original-token=\"v2-bd2d39e0ce9ac2946ed55f92531144aa\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1260\" data-original=\"https:\u002F\u002Fpica.zhimg.com\u002Fv2-917b060abda753ad352f98bcce863477_r.jpg?source=1940ef5c\" data-actualsrc=\"https:\u002F\u002Fpica.zhimg.com\u002F50\u002Fv2-917b060abda753ad352f98bcce863477_720w.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"12MRVfrL\"\u003E此方法只试用于Encoder-Decoder架构，其也称可以将输入长度延申至无限长。\u003C\u002Fp\u003E\u003Cp data-pid=\"YAbtthMf\"\u003E思路如下：\u003C\u002Fp\u003E\u003Col\u003E\u003Cli data-pid=\"dK1Y7UOu\"\u003E将长文本分成多个部分，将每一段进行编码\u003C\u002Fli\u003E\u003Cli data-pid=\"j4JAWeab\"\u003E利用query KNN检索长文本topN\u003C\u002Fli\u003E\u003Cli data-pid=\"9MXan1XI\"\u003E解码器对相关段落编码后的隐藏状态进行交叉注意力\u003C\u002Fli\u003E\u003Cli data-pid=\"FOXkU74x\"\u003E得到输出\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cp data-pid=\"oAaAfNX-\"\u003E可以看到此方法在长文本摘要任务上都取得了优异的结果\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpica.zhimg.com\u002F50\u002Fv2-d67e99e2c7ff3fa38d5a6b6ad1606509_720w.jpg?source=1940ef5c\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"986\" data-rawheight=\"311\" data-original-token=\"v2-7d3c6a5d15d36f0ac56989c46f6d5cd3\" class=\"origin_image zh-lightbox-thumb\" width=\"986\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d67e99e2c7ff3fa38d5a6b6ad1606509_r.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;986&#39; height=&#39;311&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"986\" data-rawheight=\"311\" data-original-token=\"v2-7d3c6a5d15d36f0ac56989c46f6d5cd3\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"986\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d67e99e2c7ff3fa38d5a6b6ad1606509_r.jpg?source=1940ef5c\" data-actualsrc=\"https:\u002F\u002Fpica.zhimg.com\u002F50\u002Fv2-d67e99e2c7ff3fa38d5a6b6ad1606509_720w.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch2\u003E\u003Cb\u003E累加\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cblockquote data-pid=\"vmwcy2Eh\"\u003E ALiBi（attention with linear biases），输出累加\u003C\u002Fblockquote\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpicx.zhimg.com\u002F50\u002Fv2-c009872174bc00e8bf1e6e53c02ed4b7_720w.jpg?source=1940ef5c\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"514\" data-rawheight=\"247\" data-original-token=\"v2-d7926e6a550d571eac942d9d370ed0c3\" class=\"origin_image zh-lightbox-thumb\" width=\"514\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-c009872174bc00e8bf1e6e53c02ed4b7_r.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;514&#39; height=&#39;247&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"514\" data-rawheight=\"247\" data-original-token=\"v2-d7926e6a550d571eac942d9d370ed0c3\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"514\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-c009872174bc00e8bf1e6e53c02ed4b7_r.jpg?source=1940ef5c\" data-actualsrc=\"https:\u002F\u002Fpicx.zhimg.com\u002F50\u002Fv2-c009872174bc00e8bf1e6e53c02ed4b7_720w.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"iY6jIkvx\"\u003E简单介绍一下ALiBi:\u003C\u002Fp\u003E\u003Col\u003E\u003Cli data-pid=\"1FT93yyi\"\u003E不再输入层保留位置向量\u003C\u002Fli\u003E\u003Cli data-pid=\"aDRLBC7D\"\u003E而在每层注入线性的偏移量，注意力分数从：\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cp data-pid=\"-B99BsuD\"\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Coperatorname+%7B+softmax%7D++%28+q_iK+%5E+%7B+T+%7D+%29+%5C%5C\" alt=\"\\operatorname { softmax}  ( q_iK ^ { T } ) \\\\\" eeimg=\"1\"\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"JVF5ZRyO\"\u003E变成了：\u003C\u002Fp\u003E\u003Cp data-pid=\"-AUml9rD\"\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=softmax%28q_%7Bi%7DK%5E%7BT%7D%2Bm+%5Ccdot+%5Cleft%5B+-%28i-1%29%2C+%5Cldots+%2C-2%2C-1%2C0+%5Cright%5D%29+%5C%5C\" alt=\"softmax(q_{i}K^{T}+m \\cdot \\left[ -(i-1), \\ldots ,-2,-1,0 \\right]) \\\\\" eeimg=\"1\"\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"euwTHgB6\"\u003E可以看到ALiBi比Sinusoidal、Rotary、T5 Bias在长距离输入上效果都要好得多。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpicx.zhimg.com\u002F50\u002Fv2-02ef9c04064fd44fe7b9b5e101c02e9f_720w.jpg?source=1940ef5c\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"834\" data-rawheight=\"339\" data-original-token=\"v2-b93c26f800a298e8af934395b50d5fee\" class=\"origin_image zh-lightbox-thumb\" width=\"834\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-02ef9c04064fd44fe7b9b5e101c02e9f_r.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;834&#39; height=&#39;339&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"834\" data-rawheight=\"339\" data-original-token=\"v2-b93c26f800a298e8af934395b50d5fee\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"834\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-02ef9c04064fd44fe7b9b5e101c02e9f_r.jpg?source=1940ef5c\" data-actualsrc=\"https:\u002F\u002Fpicx.zhimg.com\u002F50\u002Fv2-02ef9c04064fd44fe7b9b5e101c02e9f_720w.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"REwr0jAQ\"\u003Emosaicml\u002Fmpt-7b模型利用ALiBi将输入长度扩展至了84k，核心的思想为一下几行代码：\u003C\u002Fp\u003E\u003Cdiv class=\"highlight\"\u003E\u003Cpre\u003E\u003Ccode class=\"language-text\"\u003Eall_hidden_states = () if output_hidden_states else None\nfor (b_idx, block) in enumerate(self.blocks):\n    if output_hidden_states:\n        assert all_hidden_states is not None\n        all_hidden_states = all_hidden_states + (x,)\n    past_key_value = past_key_values[b_idx] if past_key_values is not None else None\n    (x, past_key_value) = block(x, past_key_value=past_key_value, attn_bias=attn_bias, attention_mask=attention_mask, is_causal=self.is_causal)\n    if past_key_values is not None:\n        past_key_values[b_idx] = past_key_value\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003C\u002Fdiv\u003E\u003Cp data-pid=\"FraY-rXa\"\u003E即MPT会对上次得到隐藏状态与本次的输入进行相加。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E引用\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Col\u003E\u003Cli data-pid=\"uOmolzDn\"\u003EVcc: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens\u003C\u002Fli\u003E\u003Cli data-pid=\"tk6PlAF7\"\u003EScaling Transformer to 1M tokens and beyond with RMT\u003C\u002Fli\u003E\u003Cli data-pid=\"--4wkXNq\"\u003EUnleashing Infinite-Length Input Capacity for Large-scale Language Models with Self-Controlled Memory System\u003C\u002Fli\u003E\u003Cli data-pid=\"rz4jAp53\"\u003EUnlimiformer: Long-Range Transformers with Unlimited Length Input\u003C\u002Fli\u003E\u003Cli data-pid=\"DR0cgxZF\"\u003E\u003Cb\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fhuggingface.co\u002Fmosaicml\u002Fmpt-7b\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttps:\u002F\u002F\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Ehuggingface.co\u002Fmosaicml\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003E\u002Fmpt-7b\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fb\u003E\u003C\u002Fli\u003E\u003C\u002Fol\u003E","contentMark":{},"createdTime":1683977234,"decorativeLabels":[],"editableContent":"","excerpt":"本文着重于对各种的方法的思想总结，非严谨推导循环记忆输入 Recurrent Memory Transformer (RMT) [图片] 总体思想：将长文本分段之后得到嵌入向量与记忆向量拼接，得到新的记忆向量之后与下一段再循环输入transformer。 注意：此论文实验结果在bert-base-cased（encoder-only上进行实验） [图片] CoLT5达到64K，GPT-4达到32K长度，而RMT在实验结果中长度加到4096个分段2048000词汇，效果依然强劲。 用提示词 Self-Controlled Memory (SCM) [图片] 如上…","extras":"","id":3026367037,"isCollapsed":false,"isCopyable":true,"isLabeled":false,"isMine":false,"isNormal":true,"isSticky":false,"isVisible":true,"markInfos":[],"question":{"created":1680179783,"id":592844544,"questionType":"normal","relationship":{},"title":"为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？","type":"question","updatedTime":1680179783,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F592844544"},"reactionInstruction":{},"relationship":{"isAuthor":false,"isAuthorized":false,"isNothelp":false,"isThanked":false,"upvotedFollowees":[],"voting":0},"relevantInfo":{"isRelevant":false,"relevantText":"","relevantType":""},"reshipmentSettings":"allowed","rewardInfo":{"canOpenReward":false,"isRewardable":true,"rewardMemberCount":0,"rewardTotalMoney":0,"tagline":"真诚赞赏，手留余香"},"settings":{"tableOfContents":{"enabled":false}},"stickyInfo":"","suggestEdit":{"reason":"","status":false,"tip":"","title":"","unnormalDetails":{"description":"","note":"","reason":"","reasonId":0,"status":""},"url":""},"thanksCount":26,"thumbnailInfo":{"count":11,"thumbnails":[{"height":584,"token":"v2-aa7dd794ecdf027b15bd94e52047de26","type":"image","url":"https:\u002F\u002Fpic1.zhimg.com\u002F80\u002Fv2-aa7dd794ecdf027b15bd94e52047de26_720w.jpg?source=1940ef5c","width":1356},{"height":328,"token":"v2-ce874ec5804ee94ca78cbb292a655c1b","type":"image","url":"https:\u002F\u002Fpica.zhimg.com\u002F80\u002Fv2-ce874ec5804ee94ca78cbb292a655c1b_720w.jpg?source=1940ef5c","width":1030},{"height":679,"token":"v2-38a1ad8e80c3d00f020152f7ae597cc7","type":"image","url":"https:\u002F\u002Fpica.zhimg.com\u002F80\u002Fv2-38a1ad8e80c3d00f020152f7ae597cc7_720w.jpg?source=1940ef5c","width":1459}],"type":"thumbnail_info"},"type":"answer","updatedTime":1683987204,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fanswers\u002F3026367037","visibleOnlyToAuthor":false,"voteupCount":38,"zhiPlusExtraInfo":""},"3027439372":{"adminClosedComment":false,"annotationAction":null,"answerType":"normal","attachedInfo":"ogEQCAQQAxiMnsyjCyCArtiaApICJQoJNTgwNTI5MDA3EgozMDI3NDM5MzcyGAQiCklNQUdFX1RFWFQ=","author":{"avatarUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-66f6fe0483a9e7ac39392659c121a0db_l.jpg?source=1940ef5c","avatarUrlTemplate":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-66f6fe0483a9e7ac39392659c121a0db.jpg?source=1940ef5c","badge":[],"badgeV2":{"detailBadges":[],"icon":"","mergedBadges":[],"nightIcon":"","title":""},"exposedMedal":{"avatarUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-b42f593c6ccee918d956a183eed2ffd7_l.png?source=1940ef5c","description":"回答收到「专业认可」即可获得","medalAvatarFrame":"","medalId":"1055464808517865473","medalName":"专业","miniAvatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-b42f593c6ccee918d956a183eed2ffd7_r.png?source=1940ef5c"},"followerCount":39618,"gender":-1,"headline":"数学、python、数据挖掘、天文","id":"ba2e8e5e459263c8074c6a50cd2763a4","isAdvertiser":false,"isFollowed":false,"isFollowing":false,"isOrg":false,"isPrivacy":false,"name":"苏剑林","type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002Fba2e8e5e459263c8074c6a50cd2763a4","urlToken":"su-jian-lin-22","userType":"people","vipInfo":{"isVip":true}},"canComment":{"reason":"","status":true},"collapseReason":"","collapsedBy":"nobody","commentCount":32,"commentPermission":"all","content":"\u003Cp data-pid=\"Kze_JE-0\"\u003E既是技术问题，也是算力问题。\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Cp data-pid=\"8sPcJO03\"\u003E算力问题很容易理解，GPT4大概率还是用Transformer模型。Transformer如果经过各种Linear技术的优化（如Sparse），那么Scaling Law难以保证；如果保持原样，那么复杂度是二次的，序列一长，训练成本和推理成本都比较难顶。\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Cp data-pid=\"0hEppDtz\"\u003E技术问题则主要是Transformer的长度外推性并不好。如果你想处理1000长度的文本，那么预训练阶段就拿1000长度的文本去训，那自然没有问题，但如果你只拿500长度的文本去预训练，那么得到的模型通常无法很好地处理1000长度的文本，尤其是生成模型场景。\u003C\u002Fp\u003E\u003Cp data-pid=\"ntKbwGcP\"\u003E也就是说，短文本训练的模型，通常无法直接处理长文本，这就是长度外推问题，这个问题不只是Transformer有，RNN甚至CNN都会有。注意这里的长短是相对的，如果你想处理10000长度的文本，那么5000长度都算短文本了。\u003C\u002Fp\u003E\u003Cp data-pid=\"evmtO9PU\"\u003E已经有一些工作试图解决这个问题，比如ALIBI、KERPLE、XPOS等，可以参考 \u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fkexue.fm\u002Farchives\u002F9431\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003ETransformer升级之路：7、长度外推性与局部注意力 - 科学空间|Scientific Spaces\u003C\u002Fa\u003E ，但是这类工作都是基于局部化Attention思想强行赋予平移不变性，无法做到全局依赖，在LLM场景下意义不大。最近笔者也进行了一些尝试，初步看下述方案能保留全局依赖：\u003C\u002Fp\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fkexue.fm\u002Farchives\u002F9603\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003ETransformer升级之路：9、一种全局长度外推的新思路 - 科学空间|Scientific Spaces\u003C\u002Fa\u003E\u003Cp data-pid=\"dSulfxbC\"\u003E此外，还有一个名为Parallel Context Window的方法值得一提，它是一种事后修改方案，能够增强训练好的模型的处理长度，并且理论上能保持全局依赖：\u003C\u002Fp\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fabs\u002F2212.10947\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4baaae2386ede0213c693947a141a747_180x120.jpg\" data-image-width=\"1200\" data-image-height=\"700\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EParallel Context Windows Improve In-Context Learning of Large Language Models\u003C\u002Fa\u003E\u003Chr\u002F\u003E\u003Cp data-pid=\"tLqmZ05h\"\u003E至于Claude的100k token或者GPT4的32k token是怎么做的，这个没有任何技术细节披露，没法猜。个人感觉100k其实还在能想象的范围内，硬训也是也可能的。说到这里，顺便提一件事：OpenAI在2019年的论文 \u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fabs\u002F1904.10509\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EGenerating Long Sequences with Sparse Transformers\u003C\u002Fa\u003E 就已经做到了一万多token的自回归生成，并且它的Sparsity Pattern跟Parallel Context Learning很相似。\u003C\u002Fp\u003E\u003Cp data-pid=\"1x6WRvia\"\u003E也就是说，人家19年的自回归生成长度就能够突破10k（并且还预见了接近Parallel Context Learning的方案），现在32k就是“洒洒水”了。\u003C\u002Fp\u003E","contentMark":{},"createdTime":1684051250,"decorativeLabels":[],"editableContent":"","excerpt":"既是技术问题，也是算力问题。 算力问题很容易理解，GPT4大概率还是用Transformer模型。Transformer如果经过各种Linear技术的优化（如Sparse），那么Scaling Law难以保证；如果保持原样，那么复杂度是二次的，序列一长，训练成本和推理成本都比较难顶。 技术问题则主要是Transformer的长度外推性并不好。如果你想处理1000长度的文本，那么预训练阶段就拿1000长度的文本去训，那自然没有问题，但如果你只拿500长度的文本去预训练…","extras":"","id":3027439372,"isCollapsed":false,"isCopyable":false,"isLabeled":false,"isMine":false,"isNormal":true,"isSticky":false,"isVisible":true,"markInfos":[],"question":{"created":1680179783,"id":592844544,"questionType":"normal","relationship":{},"title":"为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？","type":"question","updatedTime":1680179783,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F592844544"},"reactionInstruction":{},"relationship":{"isAuthor":false,"isAuthorized":false,"isNothelp":false,"isThanked":false,"upvotedFollowees":[],"voting":0},"relevantInfo":{"isRelevant":false,"relevantText":"","relevantType":""},"reshipmentSettings":"disallowed","rewardInfo":{"canOpenReward":false,"isRewardable":true,"rewardMemberCount":0,"rewardTotalMoney":0,"tagline":"真诚赞赏，手留余香"},"settings":{"tableOfContents":{"enabled":false}},"stickyInfo":"","suggestEdit":{"reason":"","status":false,"tip":"","title":"","unnormalDetails":{"description":"","note":"","reason":"","reasonId":0,"status":""},"url":""},"thanksCount":62,"thumbnailInfo":{"count":0,"thumbnails":[],"type":"thumbnail_info"},"type":"answer","updatedTime":1684121634,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fanswers\u002F3027439372","visibleOnlyToAuthor":false,"voteupCount":319,"zhiPlusExtraInfo":""},"3031186320":{"adminClosedComment":false,"annotationAction":null,"answerType":"normal","attachedInfo":"ogEQCAQQAxiQ97ClCyCArtiaApICJQoJNTgxMjEwMDc1EgozMDMxMTg2MzIwGAQiCklNQUdFX1RFWFQ=","author":{"avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d2570783a2ad9d096539faf0f2beea83_l.jpg?source=1940ef5c","avatarUrlTemplate":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-d2570783a2ad9d096539faf0f2beea83.jpg?source=1940ef5c","badge":[{"description":"合肥工业大学 管理科学与工程博士","topics":[],"type":"identity"}],"badgeV2":{"detailBadges":[{"description":"合肥工业大学 管理科学与工程博士","detailType":"identity_people","icon":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-2ddc5cc683982648f6f123616fb4ec09_l.png?source=32738c0c","nightIcon":"https:\u002F\u002Fpica.zhimg.com\u002Fv2-2ddc5cc683982648f6f123616fb4ec09_l.png?source=32738c0c","sources":[],"title":"已认证的个人","type":"identity","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F96956163"}],"icon":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-2ddc5cc683982648f6f123616fb4ec09_l.png?source=32738c0c","mergedBadges":[{"description":"合肥工业大学 管理科学与工程博士","detailType":"identity","icon":"","nightIcon":"","sources":[],"title":"认证","type":"identity","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F96956163"}],"nightIcon":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-2ddc5cc683982648f6f123616fb4ec09_l.png?source=32738c0c","title":"合肥工业大学 管理科学与工程博士"},"exposedMedal":{"avatarUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-6da117205088242db9cd82e48b28640d_l.png?source=1940ef5c","description":"被 1000 个人关注","medalAvatarFrame":"","medalId":"972477022068568064","medalName":"备受瞩目","miniAvatarUrl":"https:\u002F\u002Fpica.zhimg.com\u002Fv2-6da117205088242db9cd82e48b28640d_r.png?source=1940ef5c"},"followerCount":1473,"gender":0,"headline":"","id":"0b41335b2041b4a052f123e492fbf45a","isAdvertiser":false,"isFollowed":false,"isFollowing":false,"isOrg":false,"isPrivacy":false,"name":"数据学习","type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002F0b41335b2041b4a052f123e492fbf45a","urlToken":"datalearner","userType":"people","vipInfo":{"isVip":true,"vipIcon":{"nightModeUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-c9686ff064ea3579730756ac6c289978.jpg?source=88ceefae","url":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-4812630bc27d642f7cafcd6cdeca3d7a.jpg?source=88ceefae"}}},"canComment":{"reason":"","status":true},"collapseReason":"","collapsedBy":"nobody","commentCount":1,"commentPermission":"all","content":"\u003Cp data-pid=\"4rRDdzsN\"\u003E\u003Cb\u003E本文原文转载自DataLearner官方博客：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fwww.datalearner.com\u002Fblog\u002F1051684253569005\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-6630f86a46d914bf5c0e7874a480e1ca_ipico.jpg\" data-image-width=\"372\" data-image-height=\"372\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E解决大语言模型的长输入限制：MetaAI发布MegaByte最高支持几百万上下文输入！ | 数据学习者官方网站(Datalearner)\u003C\u002Fa\u003E\u003Cp data-pid=\"F690fSK9\"\u003E尽管OpenAI的ChatGPT很火爆，但是这类大语言模型有一个非常严重的问题就是对输入的内容长度有着很大的限制。例如，ChatGPT-3.5的输入限制是4096个tokens。MetaAI在前几天提交了一个论文，提出了MegaByte方法，几乎可以让模型接受任意长度的限制！\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-5cfc5eb464c458cf71d0ef3f145cb802_720w.jpg?source=1940ef5c\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"721\" data-rawheight=\"725\" data-original-token=\"v2-a3c1c55d51401d6a2621940443493513\" class=\"origin_image zh-lightbox-thumb\" width=\"721\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-5cfc5eb464c458cf71d0ef3f145cb802_r.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;721&#39; height=&#39;725&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"721\" data-rawheight=\"725\" data-original-token=\"v2-a3c1c55d51401d6a2621940443493513\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"721\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-5cfc5eb464c458cf71d0ef3f145cb802_r.jpg?source=1940ef5c\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-5cfc5eb464c458cf71d0ef3f145cb802_720w.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"uI5ND4ck\"\u003E本文将简单介绍这个方法！论文名称：《MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers》。\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"-8eN7a60\"\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fwww.datalearner.com\u002Fblog\u002F1051684253569005%23transformer%25E6%25A8%25A1%25E5%259E%258B%25E7%259A%2584%25E8%25BE%2593%25E5%2585%25A5%25E9%2599%2590%25E5%2588%25B6%25E9%2597%25AE%25E9%25A2%2598\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003Etransformer模型的输入限制问题\u003C\u002Fa\u003E\u003C\u002Fli\u003E\u003Cli data-pid=\"xBZ016RW\"\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fwww.datalearner.com\u002Fblog\u002F1051684253569005%23MetaByte%25E6%25A0%25B8%25E5%25BF%2583%25E6%2580%259D%25E6%2583%25B3%25E7%25AE%2580%25E4%25BB%258B\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EMetaByte核心思想简介\u003C\u002Fa\u003E\u003C\u002Fli\u003E\u003Cli data-pid=\"AVTZ3_hR\"\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fwww.datalearner.com\u002Fblog\u002F1051684253569005%23MetaByte%25E7%259A%2584%25E6%25B5%258B%25E8%25AF%2595%25E7%25BB%2593%25E6%259E%259C\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EMetaByte的测试结果\u003C\u002Fa\u003E\u003C\u002Fli\u003E\u003Cli data-pid=\"4RI6ucCn\"\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fwww.datalearner.com\u002Fblog\u002F1051684253569005%23MetaByte%25E7%259A%2584%25E6%2580%25BB%25E7%25BB%2593\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EMetaByte的总结\u003C\u002Fa\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch3\u003E\u003Cb\u003Etransformer模型的输入限制问题\u003C\u002Fb\u003E\u003C\u002Fh3\u003E\u003Cp data-pid=\"aNoH64LV\"\u003E尽管目前的大语言模型在较短序列（几千个tokens以内）的应用场景下有着惊人的效果，但是数百万序列的输入依然是一种刚需，包括音乐、图片、视频、小说、代码等，它们的输入长度通常都是远远超过当前大语言模型的输入限制。主要原因是自注意力机制的“成本”随着输入成二次方增长，以及每个位置都需要计算大型的前馈网络。\u003C\u002Fp\u003E\u003Ch3\u003E\u003Cb\u003EMetaByte核心思想简介\u003C\u002Fb\u003E\u003C\u002Fh3\u003E\u003Cp data-pid=\"m25c0dNB\"\u003E为了解决当前transformer模型对输入的限制，MetaAI提出的MetaByte引入了一个概念，称为patch，将模型的输入序列分割成固定大小的patches，这是一个类似于token的概念，但是显然比token覆盖的范围要宽。然后通过一个全局的模块，建立一个大的自回归transformer，把输入和输出从tokens变成patches。同时，引入了一个本地的模块，用于每个patch内部的字节的预测，其输入是从全局模块来的上下文patches表示结果，输出是预测下一个patch，这是一个小的自回归模型。\u003C\u002Fp\u003E\u003Cp data-pid=\"l6ULrXOC\"\u003E这个方法与当前transformer模型还有一个最重要的区别是\u003Cb\u003E不需要做tokenization\u003C\u002Fb\u003E！这是一个非常重要的特性。tokenization是当前大语言模型的一种通用做法，尽管tokens相比较单词具有更好的一致性，处理起来更加容易。但是，Tokenization意味着LLMs实际上不是完全的端到端。还有一个完全不同的阶段，有自己的训练和推理，并且需要额外的库。它使得引入其他模态变得复杂。\u003C\u002Fp\u003E\u003Cp data-pid=\"Y6GRPNJ9\"\u003E总结一下，MetaByte相比现有的transformer模型的差异为：\u003C\u002Fp\u003E\u003Col\u003E\u003Cli data-pid=\"JZbUmDpV\"\u003E大多数优化长序列输入的模型都是关注减少自注意力机制的二次方成本上，而MetaByte将长序列分解为两个较短的序列，并通过优化patch大小将自注意力成本降至O(n^{4\u002F3})，这仍然适用于长序列。\u003C\u002Fli\u003E\u003Cli data-pid=\"OlA0KPca\"\u003E在GPT-3这样大小的模型上，98%的计算都是在做基于位置的前馈网络，而MetaByte计算的是基于patch的前馈网络。Patch的大小如果是P，那么它的计算效率就是同等规模模型的P倍。\u003C\u002Fli\u003E\u003Cli data-pid=\"9mQsxqoh\"\u003ETransformers在生成过程中必须按照顺序生成。而MetaByte可以针对patches做并行的生成。例如，15亿参数的MetaByte生成速度比3.5亿参数规模的transformer模型还要快40%！\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Ch3\u003E\u003Cb\u003EMetaByte的总结\u003C\u002Fb\u003E\u003C\u002Fh3\u003E\u003Cp data-pid=\"EYn2-jq3\"\u003EMetaAI这项工作引起了很多人的注意。个人觉得MetaByte的最大的价值有2个：一个是将大语言模型的上下文限制能力大大拓展，按照论文的描述，可以支持到数百万的字节输入，这几乎是目前常规大语言模型的几百倍，也远超过前几天ClaudeAI-100k和GPT-4-32k，几乎可以解决当下大部分输入限制问题。而另一个最大的优点是没有tokenization，几乎可以完成一个端到端的transformer模型。\u003C\u002Fp\u003E\u003Cp data-pid=\"sEyN6CbZ\"\u003E\u003Cb\u003E这个模型的开源实现和其它信息参考原文：\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fwww.datalearner.com\u002Fblog\u002F1051684253569005\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E解决大语言模型的长输入限制：MetaAI发布MegaByte最高支持几百万上下文输入！ | 数据学习者官方网站(Datalearner)\u003C\u002Fa\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E","contentMark":{},"createdTime":1684254337,"decorativeLabels":[],"editableContent":"","excerpt":"本文原文转载自DataLearner官方博客：解决大语言模型的长输入限制：MetaAI发布MegaByte最高支持几百万上下文输入！ | 数据学习者官方网站(Datalearner) 尽管OpenAI的ChatGPT很火爆，但是这类大语言模型有一个非常严重的问题就是对输入的内容长度有着很大的限制。例如，ChatGPT-3.5的输入限制是4096个tokens。MetaAI在前几天提交了一个论文，提出了MegaByte方法，几乎可以让模型接受任意长度的限制！ [图片] 本文将简单介绍这个方法！论…","extras":"","id":3031186320,"isCollapsed":false,"isCopyable":true,"isLabeled":false,"isMine":false,"isNormal":true,"isSticky":false,"isVisible":true,"markInfos":[],"question":{"created":1680179783,"id":592844544,"questionType":"normal","relationship":{},"title":"为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？","type":"question","updatedTime":1680179783,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F592844544"},"reactionInstruction":{},"relationship":{"isAuthor":false,"isAuthorized":false,"isNothelp":false,"isThanked":false,"upvotedFollowees":[],"voting":0},"relevantInfo":{"isRelevant":false,"relevantText":"","relevantType":""},"reshipmentSettings":"allowed","rewardInfo":{"canOpenReward":false,"isRewardable":false,"rewardMemberCount":0,"rewardTotalMoney":0,"tagline":""},"settings":{"tableOfContents":{"enabled":false}},"stickyInfo":"","suggestEdit":{"reason":"","status":false,"tip":"","title":"","unnormalDetails":{"description":"","note":"","reason":"","reasonId":0,"status":""},"url":""},"thanksCount":7,"thumbnailInfo":{"count":1,"thumbnails":[{"height":725,"token":"v2-a3c1c55d51401d6a2621940443493513","type":"image","url":"https:\u002F\u002Fpica.zhimg.com\u002F80\u002Fv2-a3c1c55d51401d6a2621940443493513_720w.jpg?source=1940ef5c","width":721}],"type":"thumbnail_info"},"type":"answer","updatedTime":1684254337,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fanswers\u002F3031186320","visibleOnlyToAuthor":false,"voteupCount":6,"zhiPlusExtraInfo":""},"3050301434":{"adminClosedComment":false,"annotationAction":null,"answerType":"normal","attachedInfo":"ogEQCAQQAxj6z7+uCyCArtiaApICJQoJNTg0Njg1NzI1EgozMDUwMzAxNDM0GAQiCklNQUdFX1RFWFQ=","author":{"avatarUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-c60e031a7866520434165a26e8eaade3_l.jpg?source=1940ef5c","avatarUrlTemplate":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-c60e031a7866520434165a26e8eaade3.jpg?source=1940ef5c","badge":[{"description":"监事","topics":[],"type":"identity"},{"description":"北京邮电大学 工学硕士","topics":[],"type":"identity"}],"badgeV2":{"detailBadges":[{"description":"信息技术行业 监事","detailType":"identity_people","icon":"https:\u002F\u002Fpica.zhimg.com\u002Fv2-2ddc5cc683982648f6f123616fb4ec09_l.png?source=32738c0c","nightIcon":"https:\u002F\u002Fpica.zhimg.com\u002Fv2-2ddc5cc683982648f6f123616fb4ec09_l.png?source=32738c0c","sources":[],"title":"已认证的个人","type":"identity","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F96956163"}],"icon":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-2ddc5cc683982648f6f123616fb4ec09_l.png?source=32738c0c","mergedBadges":[{"description":"信息技术行业 监事","detailType":"identity","icon":"","nightIcon":"","sources":[],"title":"认证","type":"identity","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F96956163"}],"nightIcon":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-2ddc5cc683982648f6f123616fb4ec09_l.png?source=32738c0c","title":"信息技术行业 监事"},"exposedMedal":{"avatarUrl":"","description":"","medalId":"0","medalName":""},"followerCount":13,"gender":1,"headline":"教你使用人工智能工具，提升财富水平，实现财务自由。","id":"1137751d68e7d494da93bca89d275bbf","isAdvertiser":false,"isFollowed":false,"isFollowing":false,"isOrg":false,"isPrivacy":true,"name":"AI财富学院大熊喵","type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002F1137751d68e7d494da93bca89d275bbf","urlToken":"heavenskybird","userType":"people"},"canComment":{"reason":"","status":true},"collapseReason":"","collapsedBy":"nobody","commentCount":0,"commentPermission":"all","content":"\u003Cp data-pid=\"7KT-vlfg\"\u003EGPT模型输入的token数量被限制在较小的范围（几千到几万）有以下技术上的原因：\u003C\u002Fp\u003E\u003Cp data-pid=\"QEZHcidV\"\u003E1. **计算资源限制**：GPT模型是一个非常庞大的神经网络，具有数以亿计的参数。对于处理大量的token输入，需要大量的计算资源和内存来执行模型的前向计算和反向传播。限制输入token数量可以控制模型的计算量，以适应计算资源的限制。\u003C\u002Fp\u003E\u003Cp data-pid=\"tleBm6f_\"\u003E2. **序列长度对齐**：在使用批量计算时，输入序列的长度需要对齐。如果不限制输入token的数量，那么在一个批次中，不同样本的token长度可能会有很大的差异，这会导致需要填充或截断不同长度的序列，增加计算复杂度和内存需求。\u003C\u002Fp\u003E\u003Cp data-pid=\"SDk0_NvH\"\u003E3. **模型训练的限制**：GPT模型通常是通过将输入序列分为固定长度的小块进行训练的。如果输入序列太长，会导致训练过程中的内存和计算需求剧增，从而增加训练时间和成本。限制输入token数量可以降低训练的复杂性和资源消耗。\u003C\u002Fp\u003E\u003Cp data-pid=\"Usady0Pq\"\u003E4. **推理效率考量**：在实际应用中，高效的模型推理是非常重要的。限制输入token数量可以降低推理过程中的计算和内存需求，从而提高模型的运行效率和响应速度。\u003C\u002Fp\u003E\u003Cp data-pid=\"6x1owaxY\"\u003E不同的GPT模型可能会有不同的输入token数量限制，这取决于模型的架构和设计。此外，即使通过按使用量付费的API，也需要限制输入token数量，以确保公平的资源分配和稳定的服务质量。\u003C\u002Fp\u003E\u003Cp data-pid=\"JR7zQnV5\"\u003E总之，限制GPT模型输入token数量的主要原因是为了控制计算资源的消耗、提高模型的效率，并确保稳定的服务提供。这是为了平衡模型的性能和可用性之间的技术考量。\u003C\u002Fp\u003E","contentMark":{},"createdTime":1685368962,"decorativeLabels":[],"editableContent":"","excerpt":"GPT模型输入的token数量被限制在较小的范围（几千到几万）有以下技术上的原因： 1. **计算资源限制**：GPT模型是一个非常庞大的神经网络，具有数以亿计的参数。对于处理大量的token输入，需要大量的计算资源和内存来执行模型的前向计算和反向传播。限制输入token数量可以控制模型的计算量，以适应计算资源的限制。 2. **序列长度对齐**：在使用批量计算时，输入序列的长度需要对齐。如果不限制输入token的数量，那么在一个批次中…","extras":"","id":3050301434,"isCollapsed":false,"isCopyable":false,"isLabeled":true,"isMine":false,"isNormal":true,"isSticky":false,"isVisible":true,"markInfos":[],"question":{"created":1680179783,"id":592844544,"questionType":"normal","relationship":{},"title":"为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？","type":"question","updatedTime":1680179783,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F592844544"},"reactionInstruction":{},"relationship":{"isAuthor":false,"isAuthorized":false,"isNothelp":false,"isThanked":false,"upvotedFollowees":[],"voting":0},"relevantInfo":{"isRelevant":false,"relevantText":"","relevantType":""},"reshipmentSettings":"need_payment","rewardInfo":{"canOpenReward":false,"isRewardable":false,"rewardMemberCount":0,"rewardTotalMoney":0,"tagline":""},"settings":{"tableOfContents":{"enabled":false}},"stickyInfo":"","suggestEdit":{"reason":"","status":false,"tip":"","title":"","unnormalDetails":{"description":"","note":"","reason":"","reasonId":0,"status":""},"url":""},"thanksCount":3,"thumbnailInfo":{"count":0,"thumbnails":[],"type":"thumbnail_info"},"type":"answer","updatedTime":1685368962,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fanswers\u002F3050301434","visibleOnlyToAuthor":false,"voteupCount":0,"zhiPlusExtraInfo":""},"3094076650":{"adminClosedComment":false,"annotationAction":null,"answerType":"normal","attachedInfo":"ogEhCAQQAxjqua\u002FDCyCArtiaAjIICgQ2MDAyEAEyBQoBMBABkgIlCgk1OTI2NDQzNzkSCjMwOTQwNzY2NTAYBCIKSU1BR0VfVEVYVA==","author":{"avatarUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-9e1c1a0ab7e4f6e5a98490e8a1a6bb87_l.jpg?source=1940ef5c","avatarUrlTemplate":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-9e1c1a0ab7e4f6e5a98490e8a1a6bb87.jpg?source=1940ef5c","badge":[],"badgeV2":{"detailBadges":[],"icon":"","mergedBadges":[],"nightIcon":"","title":""},"exposedMedal":{"avatarUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-ef2b573cb63e7775da93fbed57c78496_l.png?source=1940ef5c","description":"评论 50 次","medalAvatarFrame":"","medalId":"972462538273792000","medalName":"乐于交流","miniAvatarUrl":"https:\u002F\u002Fpica.zhimg.com\u002Fv2-ef2b573cb63e7775da93fbed57c78496_r.png?source=1940ef5c"},"followerCount":3227,"gender":1,"headline":"生活不易，请坚持走下去！","id":"ae26aafcdd3defe7e4c32d0fc3897e54","isAdvertiser":false,"isFollowed":false,"isFollowing":false,"isOrg":false,"isPrivacy":false,"name":"互联网知识的力量","type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002Fae26aafcdd3defe7e4c32d0fc3897e54","urlToken":"wjhfsc","userType":"people"},"canComment":{"reason":"你的评论将会由作者筛选后显示","status":true},"collapseReason":"","collapsedBy":"nobody","commentCount":0,"commentPermission":"censor","content":"\u003Cp data-pid=\"HlEAtF57\"\u003EGPT模型输入token数量的限制并不完全是技术问题，而是由于多个因素综合考虑所做出的权衡决策。下面我会解释一些原因：\u003C\u002Fp\u003E\u003Col\u003E\u003Cli data-pid=\"Ob70lfZT\"\u003E内存和计算资源限制：较长的输入序列需要更多的内存和计算资源进行处理。大模型相对较大的参数量和计算需求意味着我们需要在处理上做一些妥协，以平衡性能和可扩展性。\u003C\u002Fli\u003E\u003Cli data-pid=\"OrJj1OMk\"\u003E模型复杂性：增加输入序列的长度会增加模型的复杂性。长序列可能导致梯度消失或梯度爆炸等问题，从而降低模型的训练效果和性能。\u003C\u002Fli\u003E\u003Cli data-pid=\"gyGXmSq6\"\u003E训练数据：通常情况下，训练数据中的大多数样本包含的信息主要集中在较短的上下文中。长序列的训练数据相对较少，因此将其作为输入可能会导致信息冗余和效率低下。\u003C\u002Fli\u003E\u003Cli data-pid=\"4LJq_obf\"\u003E推理时间：处理更长的输入序列会增加推理时间。对于在线应用和实时交互，较长的推理时间可能无法满足用户的响应需求。\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cp data-pid=\"ZX8bWkVZ\"\u003E综上所述，将GPT模型输入token数量限制在几万的范围内是考虑到了内存、计算资源限制、模型复杂性、训练数据和推理时间等多个因素所做的权衡决策。这样的限制可以帮助平衡性能、效率和模型的应用范围。然而，对于特定任务或需求，也可以通过拆分较长的输入序列或使用更大的模型来尝试克服这些限制。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpica.zhimg.com\u002F50\u002Fv2-c4c548848b91d08a499a272b58166e44_720w.jpg?source=1940ef5c\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"848\" data-rawheight=\"848\" data-original-token=\"v2-c4c548848b91d08a499a272b58166e44\" class=\"origin_image zh-lightbox-thumb\" width=\"848\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-c4c548848b91d08a499a272b58166e44_r.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;848&#39; height=&#39;848&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"848\" data-rawheight=\"848\" data-original-token=\"v2-c4c548848b91d08a499a272b58166e44\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"848\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-c4c548848b91d08a499a272b58166e44_r.jpg?source=1940ef5c\" data-actualsrc=\"https:\u002F\u002Fpica.zhimg.com\u002F50\u002Fv2-c4c548848b91d08a499a272b58166e44_720w.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"c5vNncl9\"\u003E有大佬撰写了一个原创\u003Cb\u003E有所有权版‬认证以在及‬北国京‬信\u003Cspan class=\"nolink\"\u003E公证处\u003C\u002Fspan\u003E进行公证\u003C\u002Fb\u003E的\u003Cb\u003E26万字\u003C\u002Fb\u003E的实时在线文档\u003Cb\u003E《玩赚：108种chatgpt创业变现和创业思维手册》\u003C\u002Fb\u003E，目前\u003Cb\u003E国内有很多行业大佬就是靠这个手册启蒙的，所以它很适合刚接触chatgpt的朋友！\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpica.zhimg.com\u002F50\u002Fv2-b63bfc11603aa53b5f11d750c4cbf90a_720w.jpg?source=1940ef5c\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"4698\" data-original-token=\"v2-b63bfc11603aa53b5f11d750c4cbf90a\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-b63bfc11603aa53b5f11d750c4cbf90a_r.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1200&#39; height=&#39;4698&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"4698\" data-original-token=\"v2-b63bfc11603aa53b5f11d750c4cbf90a\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-b63bfc11603aa53b5f11d750c4cbf90a_r.jpg?source=1940ef5c\" data-actualsrc=\"https:\u002F\u002Fpica.zhimg.com\u002F50\u002Fv2-b63bfc11603aa53b5f11d750c4cbf90a_720w.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"raX5Gfjo\"\u003E哪怕你是小白，\u003Cb\u003E你也可以不用注册、不用登录、不用科学上网、不限时长、纯免费无限制畅玩chatgpt\u003C\u002Fb\u003E，更有大量的\u003Cb\u003E精准搜索指令\u003C\u002Fb\u003E供你在短时间内学会\u003Cb\u003E让chatgpt来提升你的工作技能\u003C\u002Fb\u003E，\u003Cb\u003E让你一个人轻松干10个人的活！\u003C\u002Fb\u003E更有不少利用chatgpt\u003Cb\u003E创业和变现的小项目\u003C\u002Fb\u003E供你参考。\u003C\u002Fp\u003E\u003Cp data-pid=\"13zZ5tfF\"\u003E如果你已经是\u003Cb\u003E精通chatgpt使用的大佬\u003C\u002Fb\u003E，或者你更\u003Cb\u003E侧重于利用chatgpt来创业和变现\u003C\u002Fb\u003E，那么这个\u003Cb\u003E26万字\u003C\u002Fb\u003E的\u003Cb\u003E《玩赚：108种chatgpt创业变现和创业思维手册》\u003C\u002Fb\u003E更适合你！它包含了《\u003Cspan class=\"nolink\"\u003Echatgpt无障碍使用手册\u003C\u002Fspan\u003E》的内容，有108种chatgpt变现和创业的项目，每个项目都包含了\u003Cb\u003E\u003Cspan class=\"nolink\"\u003E项目名称\u003C\u002Fspan\u003E、项目概述、适合人群、项目变现方式、操作步骤提示、\u003Cspan class=\"nolink\"\u003E网络宣传\u003C\u002Fspan\u003E渠道、网络宣传文案参考、扩展思路、注意事项、\u003Cspan class=\"nolink\"\u003Echatgpt指令\u003C\u002Fspan\u003E参考（截图）等十个方面进行了阐述。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"sNM-n2FE\"\u003E\u003Cb\u003E更有不用科学上网、不用注册、不用账号和密码，更不限时长就能纯免费畅玩\u003Cspan class=\"nolink\"\u003Echatgpt4.0\u003C\u002Fspan\u003E的\u003Cspan class=\"nolink\"\u003E镜像站\u003C\u002Fspan\u003E推荐，而且还是联网的！（稀缺资源）。\u003C\u002Fb\u003E具体的完整介绍可直接查看下面的内容：\u003C\u002Fp\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fmuyin.360.cn\u002Fpc\u002Fshop.html%3Fshop_id%3DvyzixHVqWq\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E《玩赚：108种chatgpt创业变现和创业思维手册》\u003C\u002Fa\u003E\u003Cp\u003E\u003C\u002Fp\u003E","contentMark":{},"createdTime":1687932112,"decorativeLabels":[],"editableContent":"","excerpt":"GPT模型输入token数量的限制并不完全是技术问题，而是由于多个因素综合考虑所做出的权衡决策。下面我会解释一些原因： 内存和计算资源限制：较长的输入序列需要更多的内存和计算资源进行处理。大模型相对较大的参数量和计算需求意味着我们需要在处理上做一些妥协，以平衡性能和可扩展性。模型复杂性：增加输入序列的长度会增加模型的复杂性。长序列可能导致梯度消失或梯度爆炸等问题，从而降低模型的训练效果和性能。训练数据：…","extras":"","id":3094076650,"isCollapsed":false,"isCopyable":true,"isLabeled":false,"isMine":false,"isNormal":true,"isSticky":false,"isVisible":true,"markInfos":[],"question":{"created":1680179783,"id":592844544,"questionType":"normal","relationship":{},"title":"为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？","type":"question","updatedTime":1680179783,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F592844544"},"reactionInstruction":{},"relationship":{"isAuthor":false,"isAuthorized":false,"isNothelp":false,"isThanked":false,"upvotedFollowees":[],"voting":0},"relevantInfo":{"isRelevant":false,"relevantText":"","relevantType":""},"reshipmentSettings":"allowed","rewardInfo":{"canOpenReward":false,"isRewardable":true,"rewardMemberCount":0,"rewardTotalMoney":0,"tagline":"真诚赞赏，手留余香"},"settings":{"tableOfContents":{"enabled":false}},"stickyInfo":"","suggestEdit":{"reason":"","status":false,"tip":"","title":"","unnormalDetails":{"description":"","note":"","reason":"","reasonId":0,"status":""},"url":""},"thanksCount":0,"thumbnailInfo":{"count":2,"thumbnails":[{"height":848,"token":"v2-c4c548848b91d08a499a272b58166e44","type":"image","url":"https:\u002F\u002Fpicx.zhimg.com\u002F80\u002Fv2-c4c548848b91d08a499a272b58166e44_720w.jpg?source=1940ef5c","width":848}],"type":"thumbnail_info"},"type":"answer","updatedTime":1687932112,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fanswers\u002F3094076650","visibleOnlyToAuthor":false,"voteupCount":0,"zhiPlusExtraInfo":"{\"id\":1687952163030727042,\"impression_tracks\":[\"https:\u002F\u002Fsugar.zhihu.com\u002Fad-track\u002Fcommercial_content?dts=1687952163\\u0026ed=GCoEO00mKTQnSjxBUzlxBlt1C2xaM2dxaEY9WgB2L1lWcAB4Dnduc3kcdARFKXFRBTZOL0plOTRzFGJBQjlxBFhyXS4IdDttekRiXxxkegIKaFsuWiV3di0QawMCM3kBX3MPbEoqZ3MrQTZWVWJ1HQ12DCgVd29wLAhqBgMzYVEIcl0sXHY-JXwXYkFdJXEBWnIMegF0Y3R_AycODGF9CUV0DX0WcmpufBVkQVA5cQldfQ5_CHBoZihVO1oDYH8FXnQObE4qKn1_FWpXAWl_3qMjA7Gtld0%3D\\u0026nt=0\\u0026pf=3\\u0026pi=94024913\\u0026at=CjEEI1UzKCU9VjsIXwyIzIulhGWC\"],\"view_tracks\":[\"https:\u002F\u002Fsugar.zhihu.com\u002Fad-track\u002Fcommercial_content?dts=1687952163\\u0026ed=GCoEO00mKTQnSjxBUzlxBlt1C2xaM2dxaEY9WgB2L1lWcAB4Dnduc3kcdARFKXFRBTZOL0plOTRzFGJBQjlxBFhyXS4IdDttekRiXxxkegIKaFsuWiV3di0QawMCM3kBX3MPbEoqZ3MrQTZWVWJ1HQ12DCgVd29wLAhqBgMzYVEIcl0sXHY-JXwXYkFdJXEBWnIMegF0Y3R_AycODGF9CUV0DX0WcmpufBVkQVA5cQldfQ5_CHBoZihVO1oDYH8FXnQObE4qKn1_FWpXAWl_3qMjA7Gtld0%3D\\u0026nt=0\\u0026pf=3\\u0026pi=94024913\\u0026at=CjEEPFEmLWm8-o_znDYB\"],\"view_x_tracks\":[\"https:\u002F\u002Fsugar.zhihu.com\u002Fad-track\u002Fcommercial_content?dts=1687952163\\u0026ed=GCoEO00mKTQnSjxBUzlxBlt1C2xaM2dxaEY9WgB2L1lWcAB4Dnduc3kcdARFKXFRBTZOL0plOTRzFGJBQjlxBFhyXS4IdDttekRiXxxkegIKaFsuWiV3di0QawMCM3kBX3MPbEoqZ3MrQTZWVWJ1HQ12DCgVd29wLAhqBgMzYVEIcl0sXHY-JXwXYkFdJXEBWnIMegF0Y3R_AycODGF9CUV0DX0WcmpufBVkQVA5cQldfQ5_CHBoZihVO1oDYH8FXnQObE4qKn1_FWpXAWl_3qMjA7Gtld0%3D\\u0026nt=0\\u0026pf=3\\u0026pi=94024913\\u0026at=CjEEPFEmLR820zpS4V8L1jE%3D\"],\"click_tracks\":[\"https:\u002F\u002Fsugar.zhihu.com\u002Fad-track\u002Fcommercial_content?dts=1687952163\\u0026ed=GCoEO00mKTQnSjxBUzlxBlt1C2xaM2dxaEY9WgB2L1lWcAB4Dnduc3kcdARFKXFRBTZOL0plOTRzFGJBQjlxBFhyXS4IdDttekRiXxxkegIKaFsuWiV3di0QawMCM3kBX3MPbEoqZ3MrQTZWVWJ1HQ12DCgVd29wLAhqBgMzYVEIcl0sXHY-JXwXYkFdJXEBWnIMegF0Y3R_AycODGF9CUV0DX0WcmpufBVkQVA5cQldfQ5_CHBoZihVO1oDYH8FXnQObE4qKn1_FWpXAWl_3qMjA7Gtld0%3D\\u0026nt=0\\u0026pf=3\\u0026pi=94024913\\u0026at=CjEEKVQqOSuIYIdBvEo7Wg%3D%3D\"],\"conversion_tracks\":[\"https:\u002F\u002Fsugar.zhihu.com\u002Fad-track\u002Fcommercial_content?dts=1687952163\\u0026ed=GCoEO00mKTQnSjxBUzlxBlt1C2xaM2dxaEY9WgB2L1lWcAB4Dnduc3kcdARFKXFRBTZOL0plOTRzFGJBQjlxBFhyXS4IdDttekRiXxxkegIKaFsuWiV3di0QawMCM3kBX3MPbEoqZ3MrQTZWVWJ1HQ12DCgVd29wLAhqBgMzYVEIcl0sXHY-JXwXYkFdJXEBWnIMegF0Y3R_AycODGF9CUV0DX0WcmpufBVkQVA5cQldfQ5_CHBoZihVO1oDYH8FXnQObE4qKn1_FWpXAWl_3qMjA7Gtld0%3D\\u0026nt=0\\u0026pf=3\\u0026pi=94024913\\u0026at=CjEEKVctLCU8VjsIX1u2KBLeP1SB\"],\"sign\":\"437dd07a-4a08-462a-bdbf-6c59d3c51466\"}"}},"articles":{},"columns":{},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"zvideos":{},"zvideoContributions":{},"briefs":{},"eduCourses":{}},"currentUser":"e2df7218c1b1b43fb3163821b169d6de","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false},"cardUserInfo":{"vipInfo":{}},"handleWidget":{},"widgetList":[],"userWidgetId":""},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"zvideosByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{},"creationsFeed":{},"infinity":{},"batchUsers":{},"profileInfinity":null},"env":{"ab":{"config":{"params":[],"experiments":[],"chains":[],"encodedParams":"Cgo7ArcDiwUnB\u002FgMEgUBAAAAAA=="},"triggers":{}},"abV2":{"config":{"paramMap":{"ws_sug_fix":{"value":"0","abId":"sugfix-0"},"pm_new_task":{"value":"1","abId":"rl-personal_new_task-1"},"ws_platform_new":{"value":"1","abId":"author_platform-1"},"in_editor_title":{"value":"1","abId":"rl-pineditor_title-1"}},"abMap":{"sugfix-0":{"abId":"sugfix-0","layerId":"web_standard_domain_layer11","diversionType":2},"rl-personal_new_task-1":{"abId":"rl-personal_new_task-1","layerId":"rl-personal_new_task","diversionType":2},"author_platform-1":{"abId":"author_platform-1","layerId":"author_platform_layer","diversionType":2},"rl-pineditor_title-1":{"abId":"rl-pineditor_title-1","layerId":"rl-pineditor_title","diversionType":2}}},"triggers":{}},"userAgent":{"Edge":false,"IE":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":true,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Quark":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"AndroidDaily":false,"iOSDaily":false,"WxMiniProgram":false,"BaiduMiniProgram":false,"QQMiniProgram":false,"JDMiniProgram":false,"isWebView":false,"isMiniProgram":false,"origin":"Mozilla\u002F5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F114.0.0.0 Safari\u002F537.36"},"appViewConfig":{},"ctx":{"path":"\u002Fquestion\u002F592844544","query":{},"href":"http:\u002F\u002Fwww.zhihu.com\u002Fquestion\u002F592844544","host":"www.zhihu.com"},"trafficSource":"production","edition":{"beijing":false,"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false,"oppoSearch":false,"baiduSearch":false,"googleSearch":true,"shenma":false,"miniProgram":false,"xiaomi":false,"huaweiSearch":false},"theme":"light","appHeaderTheme":{"current":"normal","disable":true,"normal":{"bgColor":"GBK99A"},"custom":{"bgColor":"GBK99A"}},"enableShortcut":true,"referer":"https:\u002F\u002Fwww.google.com\u002F","xUDId":"ANDYgdoXphaPTmmx5RUWYCYBsHaR30hHWLY=","mode":"ssr","conf":{},"xTrafficFreeOrigin":"","ipInfo":{},"logged":true,"vars":{"passThroughHeaders":{}}},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0}},"recommend":{"recommendTimes":{}}},"explore":{},"levelUpperLimit":10,"mcn":{},"mcnManage":{},"tasks":{},"announcement":{},"creatorsRecommendInfo":{}},"creators":{"common":{"applyStatus":{},"rightsStatus":{}},"bayesDomains":{"status":{},"options":{"topDomains":null,"allDomains":null,"editable":0},"contents":null},"school":{"tabs":[],"contents":[],"banner":null,"entities":{}},"faq":{"tabs":[],"article":{}},"knowledgeIncome":{},"safeguardRights":{},"analytics":{"all":{},"answer":{},"zvideo":{},"article":{},"pin":{},"singleContent":{}},"account":{"growthLevel":{}},"KMResource":{},"training":{},"ToolsQuestion":{"goodatTopics":[]},"ToolsHotspot":{"domains":[]},"ToolsRecommend":{},"ToolsCustomPromotion":{"itemLists":{},"baseInfo":{}},"ToolsSearchQuestion":{},"editorSetting":{},"MCNManage":{},"knowledgeTasks":{},"incomeAnalysis":{"income":{"aggregation":{}}},"creationManage":{"editModal":{"status":false}},"activity":{},"announcement":{},"home":{"currentCreatorUrlToken":null,"rights":[],"newRights":[],"scoreInfo":{},"menusShowControlByServer":{"bVipRecomend":false,"creationRelationship":false},"newTasks":{"creatorTask":{"tasks":[],"des":[]}},"bannerList":[],"recentlyCreated":[],"homecard":{}},"videoSupport":{"textBenefit":{}},"videoDistribution":{}},"question":{"followers":{},"concernedFollowers":{},"answers":{"592844544":{"isFetching":false,"isDrained":false,"ids":[{"type":"question_feed_card","targetType":"answer","target":3027439372,"skipCount":false,"position":0,"cursor":"f62a6511d2e6a1c4a496a9692aede6d8"},{"type":"question_feed_card","targetType":"answer","target":3031186320,"skipCount":false,"position":0,"cursor":"f62a6511d2e6a1c4103297692aede6d8"},{"type":"question_feed_card","targetType":"answer","target":3026367037,"skipCount":false,"position":0,"cursor":"f62a6511d2e6a1c4b98fa4692aede6d8"},{"type":"question_feed_card","targetType":"answer","target":3094076650,"skipCount":false,"position":0,"cursor":"f62a6511d2e6a1c4d0b460682aede6d8"},{"type":"question_feed_card","targetType":"answer","target":3050301434,"skipCount":false,"position":0,"cursor":"f62a6511d2e6a1c45625ea692aede6d8"}],"newIds":[{"type":"question_feed_card","targetType":"answer","target":3027439372,"skipCount":false,"position":0,"cursor":"f62a6511d2e6a1c4a496a9692aede6d8"},{"type":"question_feed_card","targetType":"answer","target":3031186320,"skipCount":false,"position":0,"cursor":"f62a6511d2e6a1c4103297692aede6d8"},{"type":"question_feed_card","targetType":"answer","target":3026367037,"skipCount":false,"position":0,"cursor":"f62a6511d2e6a1c4b98fa4692aede6d8"},{"type":"question_feed_card","targetType":"answer","target":3094076650,"skipCount":false,"position":0,"cursor":"f62a6511d2e6a1c4d0b460682aede6d8"},{"type":"question_feed_card","targetType":"answer","target":3050301434,"skipCount":false,"position":0,"cursor":"f62a6511d2e6a1c45625ea692aede6d8"}],"antiSpider":false,"error":false,"next":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F592844544\u002Ffeeds?cursor=f62a6511d2e6a1c45625ea692aede6d8&include=data%5B%2A%5D.is_normal%2Cadmin_closed_comment%2Creward_info%2Cis_collapsed%2Cannotation_action%2Cannotation_detail%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cattachment%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Ccreated_time%2Cupdated_time%2Creview_info%2Crelevant_info%2Cquestion%2Cexcerpt%2Cis_labeled%2Cpaid_info%2Cpaid_info_content%2Creaction_instruction%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%3Bdata%5B%2A%5D.mark_infos%5B%2A%5D.url%3Bdata%5B%2A%5D.author.follower_count%2Cvip_info%2Cbadge%5B%2A%5D.topics%3Bdata%5B%2A%5D.settings.table_of_content.enabled&limit=5&offset=0&order=default&platform=desktop&session_id=1687952162953866507","sessionId":"1687952162953866507"}},"hiddenAnswers":{},"updatedAnswers":{},"ariaAnswers":{},"collapsedAnswers":{},"notificationAnswers":{},"invitedQuestions":{"total":{"count":null,"isEnd":false,"isLoading":false,"questions":[]},"followees":{"count":null,"isEnd":false,"isLoading":false,"questions":[]}},"laterQuestions":{"count":null,"isEnd":false,"isLoading":false,"questions":[]},"waitingQuestions":{"recommend":{"isEnd":false,"isLoading":false,"questions":[]},"invite":{"isEnd":false,"isLoading":false,"questions":[]},"newest":{"isEnd":false,"isLoading":false,"questions":[]},"hot":{"isEnd":false,"isLoading":false,"questions":[]}},"invitationCandidates":{},"inviters":{},"invitees":{},"similarQuestions":{},"questionBanners":{},"relatedCommodities":{},"bio":{},"brand":{},"permission":{},"adverts":{},"advancedStyle":{},"commonAnswerCount":0,"hiddenAnswerCount":0,"topicMeta":{},"bluestarRanklist":{},"relatedSearch":{},"autoInvitation":{},"simpleConcernedFollowers":{},"draftStatus":{},"disclaimers":{},"isShowMobileSignInModal":false},"shareTexts":{},"answers":{"voters":{},"copyrightApplicants":{},"favlists":{},"newAnswer":{},"entityWords":{},"concernedUpvoters":{},"simpleConcernedUpvoters":{},"paidContent":{},"settings":{}},"banner":{},"topic":{"bios":{},"hot":{},"newest":{},"top":{},"sticky":{},"pin":{},"unanswered":{},"questions":{},"zivdeo":{},"zvideo-new":{},"followers":{},"contributors":{},"parent":{},"children":{},"bestAnswerers":{},"wikiMeta":{},"index":{},"intro":{},"meta":{},"schema":{},"creatorWall":{},"wikiEditInfo":{},"committedWiki":{},"landingBasicData":{},"landingExcellentItems":[],"landingExcellentEditors":[],"landingCatalog":[],"landingEntries":{}},"explore":{"recommendations":{},"specials":{"entities":{},"order":[]},"roundtables":{"entities":{},"order":[]},"collections":{},"columns":{},"square":{"hotQuestionList":[],"potentialList":[]}},"articles":{"voters":{},"concernedUpvoters":{}},"favlists":{"relations":{}},"pins":{"reviewing":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]},"hotDaily":{"data":[],"paging":{}},"hotHighlight":{"isFetching":false,"isDrained":false,"data":[],"paging":{}},"banner":{},"commercialBanner":{"show":false,"banner":{},"trackData":{}},"video":{"items":[],"next":null,"isLoading":false,"isDrained":false}},"upload":{},"video":{"data":{},"shareVideoDetail":{},"last":{}},"zvideos":{"campaignVideoList":{},"campaigns":{},"tagoreCategory":[],"recommendations":{},"insertable":{},"recruit":{"form":{"platform":"","nickname":"","followerCount":"","domain":"","contact":""},"submited":false,"ranking":[]},"qyActivityData":{},"talkActivityData":{},"party2022ActivityData":{},"batchVideos":{},"contribution":{"selectedContribution":null,"campaign":null,"configs":{},"contributionLists":{},"recommendQuestions":{"isLoading":true,"paging":{"isEnd":false,"isStart":true,"totals":0},"data":[]},"questionSearchResults":{"isLoading":true,"paging":{"isEnd":false,"isStart":true,"totals":0},"data":[]}},"creationReferences":{},"zvideoCollection":{},"zvideoGrant":{},"collectData":{"isFetching":false,"list":[]},"videoSource":{"isLoaded":false}},"guide":{"guide":{"isFetching":false,"isShowGuide":false}},"reward":{"answer":{},"article":{},"question":{}},"search":{"recommendSearch":[],"topSearch":{},"searchValue":{},"suggestSearch":{},"attachedInfo":{"generalByQuery":{}},"nextOffset":{"generalByQuery":{}},"topicReview":{},"sidebar":{},"calendar":{},"scores":null,"majors":{},"university":{},"generalByQuery":{},"generalByQueryInADay":{},"generalByQueryInAWeek":{},"generalByQueryInThreeMonths":{},"peopleByQuery":{},"topicByQuery":{},"zvideoByQuery":{},"scholarByQuery":{},"columnByQuery":{},"liveByQuery":{},"albumByQuery":{},"eBookByQuery":{},"kmGeneralByQuery":{},"kmCourseByQuery":{},"customFilter":{"requestFinished":false,"keys":[],"tags":[]}},"creatorSalt":{"recommendQuestionList":[],"bannerList":[],"claimBannerList":[],"sites":[],"domains":{},"hasRecored":false,"hasClaim":false,"hasContributedList":[],"notContributedList":[],"contributesTotal":null,"previewPageTitle":"","previewPageContent":"","restContributionNumber":"-"},"publicEditPermission":{},"readStatus":{},"draftHistory":{"history":{},"drafts":{}},"notifications":{"recent":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"history":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"notificationActors":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"recentNotificationEntry":"all"},"specials":{"entities":{},"all":{"data":[],"paging":{},"isLoading":false}},"collections":{"hot":{"data":[],"paging":{},"isLoading":false},"collectionFeeds":{}},"userProfit":{"permission":{"permissionStatus":{"zhiZixuan":0,"recommend":-1,"task":0,"plugin":0,"infinity":0},"visible":false},"linkCardLimit":0},"mcn":{"bindInfo":{},"memberCategoryList":[],"producerList":[],"categoryList":[],"lists":{},"banners":{},"protocolStatus":{"isAgreedNew":true,"isAgreedOld":true},"probationCountdownDays":0},"mcnActivity":{"household":{"products":{},"rankList":{"total":{},"yesterday":{}}}},"brand":{"contentPlugin":{}},"host":{"roundtable":{"subjects":{},"applications":{"total":0},"online":{"total":0},"applies":{},"details":{},"includedResource":{},"hotQuestions":{},"warmupContents":{},"batchInclude":{}},"special":{"applications":{"total":0,"pages":{},"entities":{}},"censorHistory":{},"drafts":{}}},"campaign":{"single":{},"list":{},"videoMakerAcq":{},"vote":{},"cardCollecting":{"message":null,"profile":{"balance":"0","chance":0,"coinNum":0,"gatherClose":false,"isGotMagicCard":false,"isPay":false,"partitionStart":false,"totalDone":0,"withdrawStart":false},"sharePoster":{"share":"","sendCard":"","invite":""},"shareLink":null,"shareIntention":"share","shareKey":null,"shareCardId":null,"inviterInfo":null,"giverInfo":null,"prize":null,"receivedCard":null,"newCoinCount":null,"newCardList":[],"newUserCardCount":1,"taskList":[],"prizeList":null,"cardList":null,"panel":{"showTaskPanel":false,"showRewardPanel":false},"modal":{"showWelcomeModal":false,"showFusionModal":false,"showFusionPromptModal":false,"showShareModal":false,"showBackModal":false}},"zhiboPandian2020":null,"boarding":{},"searchGaokaoSubPage":{},"searchHealth":{}},"knowledgePlan":{"lists":{},"allCreationRankList":{},"featuredQuestions":{}},"wallE":{"protectHistory":{"total":0,"pages":{},"entities":{}}},"roundtables":{"hotQuestions":{},"warmupContents":{},"hotDiscussions":{},"selectedContents":{},"roundtables":{}},"helpCenter":{"entities":{"question":{},"category":{}},"categories":[],"commonQuestions":[],"relatedQuestions":{},"faqTypes":[]},"republish":{},"commercialReport":{"commercialTypes":[]},"creatorMCN":{"mcn":{},"mcnStatistics":{},"isNoAuth":false,"creatorManageData":[],"creatorManageDataTotal":1,"mcnDomains":[],"bill":{"list":{},"detail":{}}},"commentManage":{"commentList":{"ids":[],"entities":{},"nextOffset":0,"urlToken":""},"subCommentList":{"ids":[],"entities":{},"paging":{"next":"","isEnd":false}}},"commentPermission":{},"creatorRightStatus":{"list":[]},"zhiPlus":{"permissionStatus":9999},"streaming":{},"creationRanking":{},"eduSections":{"eduSectionState":{}},"adPromotion":{"answer":{},"article":{}},"editVideo":{"editVideoEnabled":false}},"subAppName":"main","spanName":"QuestionPage","canaryConfig":{"test_canary":"0","use_new_player":"1","player_vendor":"1","use_hevc":"1","upload_use_signature":"1","use_backdrop_blur":"1","article_title_imagex":"1","play_station":"1"}}</script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/vendor.5f3e51e68d56265eb628.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/react.production.min.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/react-dom.production.min.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/react-dom-server.browser.production.min.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/runtime.app.1d968dc82a2145a2d3f3.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/6116.app.1dc8e5399aaea81ea5cc.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/main.app.1b13efff3ca9e91fd3c9.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/lib-0e5ce61e.e4f4e744ae1151f4720b.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/lib-79b5cf47.f16b5bf4c3cff85007a0.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/lib-330004dc.1d8c6cf5d213e2258b49.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/lib-29107295.d95bc1e8eb9ab5aec415.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/lib-2ec050f6.f25933021425f59f8df9.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/lib-83b0f42f.44dc6ae9bf70728f077f.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/shared-e5fb4baf7f81913234c8ae38d77981ef34c5b741.8836e010fb06f5f943db.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/shared-0b43bf3e67dbb6b623fe8ec6c5d091d1b549b2dc.a20a7bdb0167643bb059.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/shared-c1b26e28f9af848665b4dda36429ffbbc02ba722.f9699cd38b546bc886c5.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/shared-a03539a0bcd1a09accc148479ff7b81e93db1ac3.7f7443bc7d6d2229f7bb.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/shared-11cdd05708e8231a679e46442ff0ae122532f1bc.4d7535e8b5c2846184c0.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/shared-707a11ebc868d394defdec5e3c9c3bd627194a5c.9a235b2bc9f0b85654f4.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/shared-344960c9bb3f9e501026d17224a6974d3281f1a3.5795903ab1d3d81303d6.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/shared-30b2a91d27f48fa9c977462bb1d69791a88a1110.cee14cbb50bc20e7c813.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/shared-ae9b75872c9a83d5429962a0fa1dbe92db2f9066.f383caafe67e5ae0f0fe.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/shared-f65d696a86e1d5d9cbd56fc51b73898ffa8043de.5e62c87c8299211a3fdd.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/shared-7df56d9846d5f71fc0428c60463f36496d768b20.43e63299a511e5b49a84.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/shared-ff6488b53b31e2f26005da423c1542f5a34ce2b9.897d67bd40ae08744ee2.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/shared-d0bb0dc86392a7e972798467f9dd20ba179b044b.9ca4872b1ee6740b2f8f.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/shared-a3708c7e8c84cce0a3b8da43db0c3cd735be2320.ffaab8af9e59a455b00b.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/shared-c28a9bf3464dd32af4306520d44ac7bcef62e866.872657963e22852e2265.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/4633.de351d4baff09b5ba6dd.js"></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/main-question-routes.a1d57b742ad21d5b64e5.js"></script><script defer="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/aria.js" id="ariascripts" wapforceoldfixed="false" loaddata="false"></script><script src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/hm.js" async=""></script><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/zap.js"></script><div><div style="display: none;"><i>想来知乎工作？请发送邮件到 jobs@zhihu.com</i></div></div><script src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/push.js"></script><div><div><div id="_scroll-padding-polyfill_" style="position: fixed; top: 0px; left: 0px; right: 0px; pointer-events: none; background: transparent; height: calc(108px + 2em);"></div></div></div><div><div><div class="css-8pdeid"></div></div></div><script crossorigin="" src="./(1 封私信) 为什么gpt模型输入的token最大数量被限制在几万，是有技术问题吗？ - 知乎_files/js"></script></body></html>